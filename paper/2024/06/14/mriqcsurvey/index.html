<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      Summaries of papers on MRI Quality Assessment and Control &middot; Coffee Chat
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/paper/2024/06/14/mriqcsurvey/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/paper/2024/06/14/mriqcsurvey/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Summaries of papers on MRI Quality Assessment and Control</h1>
  <p class="post-date">14 Jun 2024&nbsp;&nbsp;&nbsp;&nbsp;
    <!--<span class="post-date">14 Jun 2024</span>-->
    
      
        
          <span class="tag" data-tag="brainImaging">
            <a href="https://alatteaday.github.io/tags/?tag=brainImaging">
              #brainImaging
            </a>
          </span>
        
      
        
          <span class="tag" data-tag="mri">
            <a href="https://alatteaday.github.io/tags/?tag=mri">
              #mri
            </a>
          </span>
        
      
    
  </p>
  <p>I summarized four papers related to MRI quality assessment and control. Below are the summaries:</p>

<h1 id="paper-list">Paper list</h1>

<ul>
  <li>Liao, Lufan, et al. “Joint image quality assessment and brain extraction of fetal MRI using deep learning.” <em>Medical Image Computing and Computer Assisted Intervention–MICCAI</em> <em>2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VI 23</em>. Springer International Publishing, 2020.</li>
  <li>Giganti, Francesco, et al. “Prostate Imaging Quality (PI-QUAL): a new quality control scoring system for multiparametric magnetic resonance imaging of the prostate from the PRECISION trial.” European urology oncology 3.5 (2020): 615-619.</li>
  <li>Esses, Steven J., et al. “Automated image quality evaluation of T2‐weighted liver MRI utilizing deep learning architecture.” <em>Journal</em> <em>of</em> <em>Magnetic</em> <em>Resonance</em> <em>Imaging</em> 47.3 (2018): 723-728.</li>
  <li>Monereo-Sánchez, Jennifer, et al. “Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations-insights from the Maastricht study.” <em>Neuroimage</em> 237 (2021): 118174.</li>
</ul>

<p><br /></p>

<h1 id="joint-image-quality-assessment-and-brain-extraction-of-fetal-mri-using-deep-learning-2020">Joint Image Quality Assessment and Brain Extraction of Fetal MRI Using Deep Learning (2020)</h1>

<h2 id="background">Background</h2>

<ul>
  <li>Quality Assessment (QA): Evaluates MRI image quality for analysis suitability.</li>
  <li>Brain Extraction (BE): Identifies and isolates the brain region from the MRI image.</li>
</ul>

<p>Traditionally handled separately, this paper proposes a joint deep learning model for simultaneous QA and BE, enhancing performance and efficiency, since both tasks focus on the brain region. Besides, dealing with fetal brain images are difficult, in that they can appear in different positions and angles within the MRI scans. And their shapes and appearances change as fetuses grow. To solve this difficulty, the study leverages deformable convolution method.</p>

<h2 id="main-contributions">Main Contributions</h2>

<ol>
  <li>
    <p>Joint Optimization: Combining QA and BE, allowing the network to learn shared features and reducing the risk of overfitting.</p>
  </li>
  <li>
    <p>Multi-Stage Deep Learning Model:</p>

    <ul>
      <li>Brain Detector: Locates the brain region within the MRI scan. This helps in focusing the subsequent analysis on the relevant part of the image.</li>
      <li>Deformable Convolution: Adapts the receptive field to the varying shapes and sizes of fetal brains. This is crucial because fetal brain shapes change significantly across different gestational ages.</li>
      <li>Task-Specific Module: Simultaneously performs QA and BE.</li>
    </ul>
  </li>
  <li>
    <p>Multi-Step Training Strategy: Progressive training enhances model learning.</p>
  </li>
</ol>

<h2 id="evaluation">Evaluation</h2>

<ul>
  <li>Datasets: Fetal MRI images, focusing on 2D slice quality.</li>
  <li>Metrics:
    <ul>
      <li>Dice Similarity Coefficient (DSC): The primary metric for evaluating the accuracy of brain extraction, measuring the overlap between the predicted and true brain regions.</li>
      <li>Quality Scores: For image quality assessment, the model was trained to classify images into different quality levels.</li>
    </ul>
  </li>
  <li>Results:
    <ul>
      <li>The model achieved a DSC score of 0.89, which is comparable to or better than existing methods, indicating high accuracy in brain extraction.</li>
      <li>The image quality assessment module successfully classified image slices into quality categories, with 85% accuracy in distinguishing between high and low-quality images.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>The study introduces a DL model for simultaneous QA and BE in fetal MRI scans, using deformable convolutions to handle variability in brain images. The multi-step training and diverse dataset validation demonstrate its effectiveness, making it a promising tool for fetal MRI analysis.</p>

<p><br /></p>

<h1 id="prostate-imaging-quality-pi-qual-a-new-quality-control-scoring-system-for-multiparametric-magnetic-resonance-imaging-of-the-prostate-from-the-precision-trial-2020">Prostate Imaging Quality (PI-QUAL): A New Quality Control Scoring System for Multiparametric Magnetic Resonance Imaging of the Prostate from the PRECISION trial (2020)</h1>

<h2 id="background-1">Background</h2>

<p>The PRECISION trial was a multicenter randomized study that demonstrated multiparametric magnetic resonance imaging (mpMRI)-targeted biopsy is superior to standard transrectal ultrasound-guided biopsy for detecting prostate cancer. The success of mpMRI-targeted biopsies relies heavily on the quality of the mpMRI scans, but there was no existing scoring system to evaluate this quality.</p>

<h2 id="prostate-imaging-quality-pi-qual">Prostate Imaging Quality (PI-QUAL)</h2>

<p>To address this gap, the researchers introduced a novel scoring system called the Prostate Imaging Quality (PI-QUAL) score. PI-QUAL is a Likert scale from 1 to 5:</p>

<ul>
  <li>1: No mpMRI sequences are of diagnostic quality.</li>
  <li>5: Each sequence is independently of optimal diagnostic quality.</li>
</ul>

<h2 id="method">Method</h2>

<ol>
  <li>Selection of MRI scans: From the PRECISION trial, 58 out of 252 mpMRI scans (23%) were randomly selected for evaluation. These scans were taken from 22 different centers involved in the trial.</li>
  <li>Radiologist assessment: Two experienced radiologists from the coordinating trial center independently assessed the selected MRI scans. The radiologists were blinded to the pathology results to ensure unbiased evaluation.</li>
  <li>Scoring system: The scans were scored using the newly developed PI-QUAL system.</li>
  <li>Quality Metrics
    <ul>
      <li>Overall quality: The overall diagnostic quality of the scans was evaluated.</li>
      <li>Specific sequence quality: The quality of individual sequences such as T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and dynamic contrast-enhanced imaging (DCE) was assessed separately.</li>
    </ul>
  </li>
  <li>Statistical Analysis
    <ul>
      <li>The percentage of scans with sufficient diagnostic quality (PI-QUAL score ≥3) was calculated.</li>
      <li>The percentage of scans with good or optimal diagnostic quality (PI-QUAL score ≥4) was determined.</li>
      <li>The diagnostic quality of the specific imaging sequences (T2WI, DWI, DCE) was also analyzed.</li>
    </ul>
  </li>
</ol>

<h2 id="results">Results</h2>

<ul>
  <li>Overall Diagnostic Quality:
    <ul>
      <li>55 out of 58 scans (95%) had sufficient diagnostic quality (PI-QUAL score ≥3).</li>
      <li>35 out of 58 scans (60%) had good or optimal diagnostic quality (PI-QUAL score ≥4).</li>
    </ul>
  </li>
  <li>Sequence-Specific Quality: 95% of T2WI scans, 79% of DWI scans, and 66% of DCE scans were of diagnostic quality.</li>
</ul>

<h2 id="conclusion-1">Conclusion</h2>

<p>The introduction of the PI-QUAL score provides a standardized method to assess the quality of mpMRI scans. Further validation of this scoring system is recommended to ensure its effectiveness in various clinical settings.</p>

<p><br /></p>

<h1 id="automated-image-quality-evaluation-of-t2-weighted-liver-mri-utilizing-deep-learning-architecture-2018">Automated image quality evaluation of T2-weighted liver MRI utilizing deep learning architecture (2018)</h1>

<h2 id="background-2">Background</h2>

<p>Accurate screening of T2-weighted (T2WI) liver MRI scans is essential for effective diagnosis, but manual evaluation by radiologists is time-consuming and subject to variability. Automated methods, specifically using deep learning (DL) approaches like Convolutional Neural Networks (CNNs), offer a promising solution for consistent and efficient image quality assessment. This study aimed to develop and evaluate a CNN for automated screening to identify non-diagnostic images and compare its performance to radiologists’ evaluations.</p>

<h2 id="method-1">Method</h2>

<ul>
  <li>Data Collection: The study utilized 522 liver MRI exams performed at 1.5T and 3T between November 2014 and May 2016 for training and validation of the CNN.</li>
  <li>CNN Architecture: The CNN consisted of several layers, including an input layer, convolutional layer, fully connected layer, and output layer.</li>
  <li>Training Data: 351 T2WI images were anonymized and labeled as diagnostic or non-diagnostic based on their ability to detect lesions and assess liver morphology. These were used to train CNN.</li>
  <li>Validation Data: An independent set of 171 T2WI images was used for blind testing. Two radiologists independently evaluated these images, labeling them as diagnostic or non-diagnostic.</li>
  <li>Comparison: The image quality (IQ) output from the CNN was compared to the evaluations made by the two radiologists.</li>
</ul>

<h2 id="results-1">Results</h2>

<ul>
  <li>The agreement between the CNN and the radiologists was: 79% with Reader 1, and 73% with Reader 2</li>
  <li>Sensitivity and Specificity of the CNN in identifying non-diagnostic images:
    <ul>
      <li>Sensitivity: 67% with respect to Reader 1 and 47% with respect to Reader 2</li>
      <li>Specificity: 81% with respect to Reader 1 and 80% with respect to Reader 2</li>
    </ul>
  </li>
  <li>Negative predictive value: 94% with respect to Reader 1 and 86% with respect to Reader 2</li>
</ul>

<h2 id="conclusion-2">Conclusion</h2>

<p>This research shows the potential of using deep learning, specifically a CNN, for automated quality evaluation of T2-weighted liver MRI images. The CNN’s performance was compared to radiologists’ assessments, showing a high negative predictive value, which indicates its reliability in identifying diagnostic images. This automated approach could be assist radiologists in clinical settings by quickly and accurately determining the quality of MRI scans.</p>

<p><br /></p>

<h1 id="quality-control-strategies-for-brain-mri-segmentation-and-parcellation-practical-approaches-and-recommendations---insights-from-the-maastricht-study-2021">Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations - insights from the Maastricht study (2021)</h1>

<h2 id="background-3">Background</h2>

<p>Quality control (QC) in brain MRI segmentation is crucial for ensuring accurate data. Manual QC, although considered the gold standard, is impractical for large datasets due to its time-consuming nature. Automated methods offer faster and reproducible alternatives but lack a consensus on the best approach. This study aims to highlight the impact of manual edits on brain segmentation accuracy and compare various QC strategies to reduce measurement errors effectively.</p>

<h2 id="method-2">Method</h2>

<ul>
  <li>Data: Structural brain MRI from 259 participants of The Maastricht Study.</li>
  <li>Segmentation Tool: FreeSurfer 6.0 was used to automatically extract morphological estimates.</li>
  <li>Manual Editing: Segmentations with inaccuracies were manually edited, and the differences in morphological estimates before and after editing were compared.</li>
  <li>Quality Control Strategies:
    <ul>
      <li>Manual Strategies: Visual inspection to exclude or manually edit images.</li>
      <li>Automated strategies: Exclusion of outliers using MRIQC and Qoala-T, and metrics such as morphological global measures, Euler numbers, and Contrast-to-Noise ratio.</li>
      <li>Semi-Automated Strategies: Visual inspection and manual editing of outliers detected by tools and metrics without excluding them.</li>
    </ul>
  </li>
  <li>Evaluation: Measuring the proportion of unexplained variance relative to the total variance after applying each strategy.</li>
</ul>

<h2 id="results-2">Results</h2>

<ul>
  <li>Manual Editing: Significant changes in subcortical brain volumes and moderate changes in cortical surface area, thickness, and hippocampal volumes.</li>
  <li>Strategy Performance: Depended on the morphological measure of interest.
    <ul>
      <li>Manual Strategies: Provided the largest reduction in unexplained variance.</li>
      <li>Automated Alternatives: Based on Euler numbers and MRIQC scores.</li>
      <li>Global Morphological Measures: Excluding outliers increased unexplained variance.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion-3">Conclusion</h2>

<p>The study underscores the importance of QC in brain MRI segmentation, advocating for manual methods as the most reliable, though impractical for large datasets. Automated methods, especially those using Euler numbers and MRIQC, provide effective alternatives. Excluding outliers based on global measures may increase errors, guiding practical QC recommendations for neuroimaging research to ensure data accuracy and reliability.</p>

<p><br /></p>

</div>


<div class="related">
  <h2 class="related-title">Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/study/2024/05/28/mriqc_report/">
            [MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)&nbsp;
            <small>28 May 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/">
            [MRIQC 3-1] Opening an HTML file using Flask&nbsp;
            <small>21 May 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/2024/05/20/mriqc_run/">
            [MRIQC 3] Running MRIQC: A Step-by-Step Guide using nii2dcm, Heudiconv, and MRIQC&nbsp;
            <small>20 May 2024</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


        
          <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
