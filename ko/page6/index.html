<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      Coffee Chat &middot; Brewing AI Knowledge
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/ko/page6/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/ko/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/ko/about">About</a>
    <a class="sidebar-nav-item active" href="https://alatteaday.github.io/ko/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/ko/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/ko/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/ko/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/ko/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/ko/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/ko/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/ko/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        
            <a href=" /page6/">eng</a>
        
    

    
    
        kor
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/papers/2020/12/22/srsurvey/">
        [Paper] Deep learning for image super-resolution: A survey (2020)
      </a>
    </h1>
    <!--<span class="post-date">22 Dec 2020</span>-->
    <p class="post-date">22 Dec 2020&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="cv">
              <a href="https://alatteaday.github.io/ko/tags/?tag=cv">
                #cv
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Wang, Zhihao, Jian Chen, and Steven CH Hoi. “Deep learning for image super-resolution: A survey.” <em>IEEE transactions on pattern analysis and machine intelligence</em> 43.10 (2020): 3365-3387.</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/9044873?casa_token=ZvibT-s3inQAAAAA:7z3uDjyf2cDsJhnY-NLadsaG1exlVS3qQAPck6JXaj6awV7I5Gcc8XXbjjw5uugCWXfE6tXJNB4">Paper Link</a></p>

<h1 id="introduction">Introduction</h1>

<ul>
  <li>Super-resolution (SR)은 저화질(low-resolution; LR) 이미지를 고해상도(high-resolution; HR) 이미지로 변환하는 과정이다.</li>
  <li>LR 이미지에는 여러 HR 이미지가 있을 수 있다는 점에서 SR은 불완전한 문제이다.</li>
  <li>Deep learning (DL)은 SR의 발전에 크게 기여했는데, CNN (SRCNN) 및 GAN (SRGAN)과 같은 방법이 사용되었다.</li>
</ul>

<p><br /></p>

<h1 id="problem-setting-and-terminology">Problem Setting and Terminology</h1>

<ul>
  <li>Problem Definition: LR input에서 HR 이미지를 근사하는 SR 모델 개발</li>
  <li>
    <p>Image Quality Assessment (IQA): 인간의 주관적 판단 및 객관적 계산을 포함하여, full-reference, reduced-reference, no-reference 방법으로 분류된다.</p>

    <p><br /></p>
  </li>
</ul>

<h1 id="supervised-super-resolution">Supervised Super-Resolution</h1>

<h2 id="sr-framework">SR Framework</h2>

<ul>
  <li>Pre-Upsampling Framework: 전통적인 upsampling 방식으로 LR 이미지를 확대한 후 DL network로 화질을 정제한다. (e.g., SRCNN).</li>
  <li>Post-Upsampling Framework: End-to-end DL 모델을 사용해 upsampling 한다.</li>
  <li>Progressive Upsampling Framework: CNNs을 cascade로 사용해 단계적으로 이미지를 정제한다.</li>
  <li>Iterative Up-and-Down Sampling: DBPN이나 SRFBN 모델과 같이 LR-HR 간 dependency를 더 잘 포착하는 방법.</li>
</ul>

<p><br /></p>

<h2 id="upsampling-methods">Upsampling Methods</h2>

<h3 id="interpolation-based">Interpolation-Based</h3>
<p>Nearest-neighbor, bilinear, bicubic interpolation을 포함한다. DL 기반 방법이 등장하기 전까지 이미지 크기를 조정하는 데 사용되었다.</p>

<h3 id="learning-based">Learning-Based</h3>
<p>Transposed convolution layer나 sub-pixel layer를 사용해 end-to-end로 모델을 학습한다</p>
<ol>
  <li>
    <p><strong>Transposed Convolution Layer (Deconvolution Layer)</strong>: convolution output과 같은 크기의 feature map을 기반으로 input을 예측하여, 0을 삽입한 후 convolution을 수행해 이미지 크기를 키운다.</p>

    <p align="center">
     <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/deconv.jpg?raw=true" alt="deconv" style="zoom: 70%;" />
 </p>

    <ul>
      <li>이미지 크기를 키우면서 패턴의 연결성을 유지하지만, 각 축에서 불균일하게 겹치는 부분이 생기면서 checkerboard 같은 결함을 초래할 수 있다.</li>
    </ul>
  </li>
  <li>
    <p><strong>Sub-Pixel Layer (Pixelshuffle)</strong>: Convolution을 통해 다수의 채널을 생성하고 이것을 재구성한다.</p>

    <p align="center">
     <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/subpixel.png?raw=true" alt="subpixel" style="zoom: 70%;" />
 </p>

    <ul>
      <li>Input size가 $(h \times w \times c)$일 때, $s^2$배의 채널이 만들어진다. $s$는 scaling factor이다. Output size는 $(h \times w \times s^2c)$가 되고, 이것을 $(sh \times sw \times c)$로 재구성(shuffle)한다.</li>
      <li>Transposed convolution layer보다 receptive field가 커, 맥락과 현실적인 세부 사항을 더 반영할 수 있다. 하지만 receptive field의 분포가 균일하지 않아 블록 사이 경계 부근에서 결함이 발생할 수 있다.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h2 id="network-design">Network Design</h2>

<p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/networks.png?raw=true" alt="networks" style="zoom: 100%;" />
</p>

<h3 id="residual-learning">Residual Learning</h3>

<ul>
  <li>LR과 HR를 직접 매핑하는 것 대신 둘 간의 잔차(residual)에 집중하여 학습을 단순하게 한다. 변환 작업의 복잡도를 줄여준다.</li>
  <li>Input 이미지와 target 이미지 간 residual만 학습함으로써 모델이 세부적인 사항에 집중할 수 있고, 이것으로 성능이 향상될 뿐 아니라 수렴 속도도 빨라진다.</li>
  <li>Example: ResNet architecture는 residual block을 사용해 네트워크가 아주 깊어도 효과적으로 학습시킬 수 있다.</li>
</ul>

<h3 id="recursive-learning">Recursive Learning</h3>

<ul>
  <li>동일한 모듈을 반복적으로 적용하여 high-level feature를 캡쳐한다.</li>
  <li>feature를 반복적으로 정제하여 더 디테일하고 정확한 이미지를 reconstruct 하게끔 한다.</li>
  <li>Example: Deep Recursive Convolutional Network (DRCN)은 단일 convolutional layer를 여러 번 사용해 receptive field를 확장하면서도 파라미터 수를 크게 증가시키지 않는다.</li>
</ul>

<h3 id="multi-path-learning">Multi-Path Learning</h3>

<ol>
  <li><strong>Local Multi-Path Learning</strong>
    <ul>
      <li>병렬적인 경로를 통해 feature를 추출하고, 이를 융함하여 더 나은 모델링을 가능하게 한다. 이미지의 다양한 측면을 동시에 캡쳐하는 데 도움이 된다.</li>
      <li>각 경로는 각각 다른 규모나 유형의 feature에 집중할 수 있고, 이것을 결합해 전체적인 representation을 개선한다.</li>
      <li>Example: Multi-scale Residual Network (MSRN)은 다양한 커널 크기를 가진 multiple convolutional layer를 사용해 multi-scale feature를 캡쳐한다.</li>
    </ul>
  </li>
  <li><strong>Scale-Specific Multi-Path Learning</strong>
    <ul>
      <li>단일 네트워크 내에서 다양한 scaling factor에 대해 별도의 경로를 갖으면서도, 네트워크가 여러 scale을 더 효과적으로 처리할 수 있게 한다.</li>
      <li>Example: MDSR (Multi-Scale Deep Super-Resolution)은 대부분의 파라미터를 공유하지만, 다른 upscaling factor를 다루기 위해 scale-specific layer를 사용한다.</li>
    </ul>
  </li>
</ol>

<h3 id="dense-connections">Dense Connections</h3>

<ul>
  <li>각 레이어를 서로 feed forward 방식으로 연결해, gradient flow와 feature 재사용을 촉진한다. 이것으로 gradient가 이전 레이어로 직접 흐를 수 있게 하여 학습 효율을 높인다.</li>
  <li>feature를 재사용하게끔 하여 효율적이고 컴팩트한 네트워크를 만든다.</li>
  <li>Example: DenseNet은 각 레이어를 모든 레이어에 연결해 feature의 propagation을 촉진하여 gradient vanishing의 위험을 줄인다.</li>
</ul>

<h3 id="group-convolution">Group Convolution</h3>

<ul>
  <li>Input 채널을 그룹화하고, 각 그룹 내에서 convolution을 수행하여 계산 복잡도와 파라미터 수를 줄인다.</li>
  <li>경량 모델에서 성능과 효율성을 균형 있게 가져가기 위해 사용된다.</li>
  <li>Example: Xception과 MobileNet은 depthwise separable convolution을 사용하여 파라미터와 계산량을 줄인다.</li>
</ul>

<h3 id="pyramid-pooling">Pyramid Pooling</h3>

<ul>
  <li>여러 scale에서 pooling을 해 global 및 local 맥락 정보를 파악한다. 이미지를 다양한 해상도로 이해하는 데 도움이 된다.</li>
  <li>Example: PSPNet (Pyramid Scene Parsing Network)은 pyramid pooling을 사용해 다양한 scale에서의 얻은 정보를 결합하여 feature representation을 향상시킨다.</li>
</ul>

<h3 id="attention-mechanisms">Attention Mechanisms</h3>

<ol>
  <li><strong>Channel Attention</strong>
    <ul>
      <li>Feature 채널 간 상호의존성(interdependency)에 집중한다. 각 채널에 다른 weight를 주어 중요한 feature를 강조하고 덜 중요한 feature는 덜 반영한다.</li>
      <li>Example: Squeeze-and-Excitation Networks (SENet)은 spacial 차원에서 feature map을 squeeze하고 채널 별 feature를 재조정하는 excitation 연산을 한다.</li>
    </ul>
  </li>
  <li><strong>Spatial Attention</strong>
    <ul>
      <li>feature의 공간적 위치에 집중한다. 다른 위치에 weight를 할당하여, 모델이 이미지에 관련된 영역에 집중할 수 있게 한다.</li>
      <li>Example: Convolutional Block Attention Module (CBAM)은 채널 및 공간 정보 관련 attention을 결합해 의미 있는 부분에 집중해 representation을 개선한다.</li>
    </ul>
  </li>
  <li><strong>Non-Local Attention</strong>
    <ul>
      <li>멀리 떨어진 픽셀 간 dependency를 파악한다. 이는 global context가 중요한 SR 작업에 특히 유용하다.</li>
      <li>Example: Non-local Neural Networks는 self-attention mechanism을 사용해 feature map 내 모든 위치 간 관계를 계산해 global context와 dependency를 캡쳐한다.</li>
    </ul>
  </li>
  <li><strong>Combined Attention</strong>
    <ul>
      <li>여러 가지 attention mechanism 방식을 결합해 각 유형의 강점을 활용할 수 있다. 예를 들어 channel attention과 spatial attention을 결합해 더욱 포괄적으로 해석 가능한 attention mechanism을 구현할 수 있다.</li>
      <li>Example: The Residual Channel Attention Network (RCAN) residual 네트워크 내에서 channel attention 모듈을 사용해 중요한 feature를 캡쳐하는 능력을 향상시켰다.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h2 id="learning-strategies">Learning Strategies</h2>

<ul>
  <li>Loss Functions: 초기에는 pixel-wise L2 loss를 사용했는데, 최근에는 content loss, adversarial loss, perceptual loss와 같은 더 복잡한 loss를 통합 사용하여 이미지의 품질을 향상시킨다.</li>
  <li>Training Techniques: curriculum learning, multi-supervision, progressive learning과 같은 기법을 사용해 학습 과정 및 모델 성능을 개선한다.</li>
</ul>

<p><br /></p>

<h1 id="unsupervised-super-resolution">Unsupervised Super-Resolution</h1>

<p>Unsupervised method는 페어링된 LR-HR dataset에 의존하지 않는다. 대신 adversarial training을 사용해 LR를 HR로 매핑하는 것을 생성 모델에 학습시킨다. 예를 들어 CycleGAN은 LR을 HR로, 또 그 역으로도 매핑함으로써 이미지 변환을 학습한다.</p>

<h1 id="domain-specific-super-resolution">Domain-Specific Super-Resolution</h1>

<p>Domain-specific method는 face SR, text SR, medical image SR과 같이 특정 응용 분야에 중점을 둔다. 도메인 지식을 활용해 특정 context에서의 SR 품질을 향상시킨다.</p>

<h1 id="benchmark-datasets-and-performance-evaluation">Benchmark Datasets and Performance Evaluation</h1>

<p>Set5, Set14, BSD100, Urban100 등 여러 benchmark dataset이 SR 모델 평가에 사용된다. 일반적인 metric으로는 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index (SSIM)가 있다.</p>

<h3 id="metrics">Metrics</h3>

<p>PSNR은 널리 사용되나, 품질에 대한 사람의 인식과 잘 일치하지는 않는다. 한편 SSIM은 밝기, 대비, 구조 등을 고려해 이 점을 보완한다.</p>

<ul>
  <li>
    <p>PSNR: 제일 보편적인 reconstruction quality 측정 metric이다. SR의 경우 최대 픽셀 값($L$)과 이미지 간 평균 제곱 오차(mean squared error; MSE)를 통해 PSNR을 정의한다.</p>

\[PSNR=10\cdot\log_{10}\big({L^2\over{1\over N}\sum_{i=1}^N(I(i)-\hat{I}(i))^2}\big)\]

    <ul>
      <li>$I(i)$와 $\hat{I}(i)$는 각각 원본 이미지와 생성된 이미지의 픽셀 값을 나타낸다. $N$은 총 픽셀 수이다.</li>
    </ul>
  </li>
  <li>
    <p>SSIM: 밝기, 대비, 구조의 측면에서 이미지를 각각 비교하여 구조적 유사성을 측정한다. 인간의 시각 시스템(human visual system; HVS)이 자연스럽고 익숙하게 이미지 구조를 파악한다고 가정한다.</p>

\[SSIM(I,\hat{I})={(2\mu_I\mu_{\hat{I}}+C_1)(2\sigma_{I\hat{I}}+C_2)\over(\mu_I^2+\mu_{\hat{I}}^2+C_1)(\sigma_I^2+\sigma_\hat{I}^2+C_2)}\]

    <ul>
      <li>$\mu_I$와 $\mu_\hat{I}$은 각각 원본 이미지와 생성된 이미지의 평균 픽셀 값이다. $\sigma_I^2$와 $\sigma_\hat{I}^2$는 분산,  $\sigma_{I\hat{I}}$는 $I$와 $\hat{I}$의 공분산이다. $C_1$와 $C_2$는 분모가 작은 경우를 대비해 계산의 안정성을 위해 사용되는 상수이다.</li>
    </ul>
  </li>
</ul>

<h1 id="challenges-and-future-directions">Challenges and Future Directions</h1>

<ul>
  <li>Scalability: 다양한 scale과 resolution을 효율적으로 처리할 수 있는 SR 모델 개발하기.</li>
  <li>Real-World Applications: 다양한 원인에 의해 해상도가 낮은 실제 이미지에서 잘 작동하도록 SR 모델 개선하기.</li>
  <li>Efficiency: 높은 성능을 유지하며 계산 복잡도와 메모리 샤용량 줄이기.</li>
  <li>Generality: 다양한 유형과 도메인의 이미지에서 일반화될 수 있는 SR 모델 개발하기.</li>
  <li>Perceptual Quality: 시각적으로 깔끔하고 결함이 없는 이미지를 생성하도록 모델 발전시키기.</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>이 survey paper는 supervised, unsupervised, domain-specific method 등으로 유형화하여 DL 기반의 SR 기술에 대해 심층적으로 검토한다. Benchmark dataset과 성능 평가 metric을 설명하고, SR 연구의 현재 상태에 대해 포괄적인 개요를 제공한다. 나아가 다양한 네트워크, upsampling 및 학습 기술을 살펴보고, 이 분야의 발전 방향을 제시한다.</p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/papers/2020/12/11/singan/">
        [Paper] SinGAN: Learning a Generative Model from a Single Natural Image (2024)
      </a>
    </h1>
    <!--<span class="post-date">11 Dec 2020</span>-->
    <p class="post-date">11 Dec 2020&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="cv">
              <a href="https://alatteaday.github.io/ko/tags/?tag=cv">
                #cv
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Shaham, Tamar Rott, Tali Dekel, and Tomer Michaeli. “Singan: Learning a generative model from a single natural image.” <em>Proceedings of the IEEE/CVF international conference on computer vision</em>. 2019.</p>

<p><a href="https://openaccess.thecvf.com/content_ICCV_2019/html/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.html">Paper Link</a></p>

<p><br /></p>

<h1 id="background">Background</h1>

<p>GAN은 현실감 있고 질 좋은 이미지를 생성하는 성공적인 결과를 보여주었다. 그러나 이 장점은 모델이 특정한 클래스 내 데이터를 학습하여 해당 클래스에 속하는 이미지를 생성하는 것에 국한된다는 한계를 동반한다. 여러 개의 클래스를 갖는 다양한 데이터에서 어떠한 분포를 찾아내는 것은 여전히 어려운 문제이다. 이를 해결하기 위해 생성할 때 다른 input signal을 추가하여 조절하거나 모델을 특정한 task에 맞게 학습시키는 것이 요구된다.</p>

<p>이 논문에서는 기존의 한계점을 벗어나기 위해 “Unconditional generation learned from a single natural image” 를 제안한다. 단일 이미지(single image) 내부 패턴의 통계량만을 가지고도 충분히 생성 모델을 학습시킬 수 있다는 것이 이 논문의 아이디어이다. 이미지 하나에서 충분히 복잡한 구조와 질감을 얻어낼 수 있기 때문이다. 이런 방식으로 GAN 모델을 학습할 수 있다면 같은 클래스에 속하는 여러 개의 이미지 데이터에 의존할 필요가 없다.</p>

<p>Single image를 다루는 기존의 모델들은 이미지를 생성할 때 저마다의 한계를 갖는다. InGAN은 최초로 Single image를 다룬 GAN 기반의 모델이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/ingan.png?raw=true" alt="dddd" style="zoom: 40%;" />
</p>

<p>핵심적인 아이디어는 Generator가 입력 이미지인 $x$를 가지고 생성된 이미지인 $y$를 “retarget”하게 한다는 것이다. 그러나 InGAN은 입력 이미지에 conditional하다는 한계를 갖는다. 입력 이미지가 있어야 이미지를 생성할 수 있다. unconditional한 single image GAN 모델의 경우 texture generation에 관하여만 존재했다[3, 4, 5]. 이 모델들은 texture에 국한된 이미지만 생성할 수 있을 뿐, 보통 이미지라고 인식될 만큼의 자연스러운 결과물의 생성이 불가하다.</p>

<p>Image manipulation task를 다루는 최근의 생성 모델들은 대부분 GAN을 기반으로 한다. 관련 task는 teractive image editing, sketch2image, image-to-image translation, super resolution 등이다. 그런데 기존 모델들은 특정한 클래스의 데이터를 학습하게 되어 있어 다양한 task를 유연하게 해결하지 못하는 등의 한계가 있다.</p>

<p>이 논문에서 제안하는 SinGAN 모델은 기존의 한계점을 해결할 수 있다. SinGAN은 unconditional하게 입력 이미지 없이 noise만으로 이미지를 생성한다. 그러면서도 기존의 unconditional texture generation 모델과 달리 자연스러운 이미지를 생성할 수 있다. 나아가 특정한 클래스의 데이터의 공통적인 특성을 학습하는 것에 주력하지 않고, 하나의 이미지를 가지고 scale을 변화시키며 내부적인 특성을 학습한다. 이는 모델이 한정되지 않은 다양한 task를 수행하면서도 좋은 성능을 가지게끔 한다.</p>

<p><br /></p>

<h1 id="method">Method</h1>

<p>SinGAN은 하나의 입력 이미지의 내재된 통계량을 가지고 unconditional하게 이미지를 생성하도록 만들어진 모델이다. 학습 방식의 핵심적인 부분은 이미지의 scale을 여러 단계를 거칠 때마다 변화시키며 이미지의 특성을 파악하게끔 한다는 것이다. 아래는 모델의 전체적 구조를 나타낸 것이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig4.png?raw=true" style="zoom: 50%;" />
</p>

<p>모델의 구조는 전반적으로 데이터 scale이 위로 올라갈수록 정교해지는 pyramid 형태를 갖는다. Generator $G: {G_0, …, G_N}$는 입력 이미지인 $x:{x_0, …, x_N}$로 학습된다. $x_n$은 값 $r_n (r&gt;1)$에 따라 원본 이미지를 downsampling한 것으로, $x_N$은 제일 coarse한 scale을 갖는다. 각 $G$는 이전 단계에서 생성된 이미지 $\tilde{x}$와 해당 단계의 scale에 맞는 noise $z$를 입력 받는다. 그리고 대응되는 Discriminator $D :{D_0, …, D_N}$를 속이는 방향으로 이미지를 생성하며 학습한다. D는 원본 이미지 $x_n$를 $G$가 생성한 $\tilde{x}_n$와 비교하여 판별해내는 방향으로 학습된다. 이 때 $x_n$과 $\tilde{x}_n$ 이미지 전체를 기준으로 비교하는 것이 아니라 이미지의 일부분을 두고 비교하는데, 마치 이미지 위에 겹쳐져 비교할 부분을 가리키는 것을 patch라고 한다. 이 patch size는 pyramid 단계에서 올라갈수록 작아진다.</p>

<p>Scale이 제일 coarse한 단계의 $G_N$은 white gaussian noise $z_N$만을 입력으로 받아 이미지를 생성한다.</p>

<p>\[
\tilde{x}_N=G_N(z_N)
\]</p>

<p>맨 처음 단계의 patch size는 보통 원본 이미지 높이의 절반 정도가 된다. 따라서 $\tilde{x}_N$은 이미지의 대략적인 배치와 구조를 나타내게 된다. 이후 단계가 올라갈수록 이전 단계에서 표현되지 못한 디테일들을 가지는 이미지가 생성된다. 이것을 위해 $G_n$의 입력으로 $z_n$과 함께 이전 단계에서 생성된 이미지를 upsampling한 이미지가 주어진다.</p>

<p>\[
\tilde{x}_n=G_n(z_n, (\tilde{x}_{n+1})\uparrow^r), n&lt;N
\]</p>

<p>각 단계에서 $G_n$의 내부적 구조는 5개의 Conv-block으로 이뤄져 있다. 아래의 그림과 같다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig5.png?raw=true" style="zoom: 40%;" />
</p>

<p>\[
\tilde{x}_n=(\tilde{x}_{n+1})\uparrow^r+\psi_n(z_n, (\tilde{x}_{n+1})\uparrow^r)
\]</p>

<p>$z_n$을 $(\tilde{x}_{n+1})\uparrow^r$ 에 더하는 데, 이것을 Convolutional layer 이전에 하여 $G$가 noise를 누락하지 못하게 한다. 종종 randomness를 조건으로 다루는 GAN 연구에서 발생하는 문제점을 이렇게 해결하였다. 또한 데이터를 5개의 Conv-block을 통과시킨 후 $(\tilde{x}_{n+1})↑^r$을 한 번 더 더해주는 residual learning 방식을 사용한다. 각 Conv layer는 Conv(3X3), BatchNorm, LeakyReLU로 구성되어 있다. 오로지 Conv layer만 사용한 점은 test 시 noise의 차원을 변경하여 다양한 크기의 이미지를 생성할 수 있다는 이점을 주기도 한다. 한편 $D_n$의 구조는 $G_n$의 5-Conv net과 동일하다.</p>

<p>각 단계의 Loss function은 Adversarial Loss와 Reconstruction Loss으로 이뤄져 있다.</p>

<p>\[
min_{G_n}max_{D_n}L_{adv}(G_n, D_n)+{\alpha}L_{rec}(G_n)
\]</p>

<p>Adversarial loss는 xn과 $\tilde{x}_n$의 분포 차이를 작게 하기 위해 사용된다. Reconstruction loss는 이미지를 생성할 때 필요한 원본 이미지의 중요한 특징 정보들을 보존하게끔 하기 위해 사용된다.</p>

<p>Adversarial loss로는 WGAN-GP Loss[6]를 사용했다. WGAN-GP는 Wasserstein GAN(WGAN)의 weight clipping의 문제점을 해결하기 위한 방법으로 gradient penalty를 도입한 모델이다. GAN은 원본 이미지와 $G$에 의해 생성된 이미지 분포의 차이로써 Jensen-Shannon divergence를 사용한다. $G$는 이것을 최소화 하도록 학습된다. 그런데 이 과정이 반복되다보면 $D$가 포화되면서 gradient vanishing 문제가 발생한다. WGAN은 이 문제를 해결하기 위해 분포 간 차이의 척도로서 Wasserstein-1 distance를 사용한 것이다. 또한 $D$-해당 논문에서는 ‘the critic’-는 1-Lipschitz Function으로, 미분 계수가 거의 모든 곳에서 1을 넘지 않는다. 이 Lipschitz constraint를 강화하기 위해 $D$의 weight를 콤팩트 공간에 가둬두는 weight clipping이 더불어 제시되었다. 이것으로 gradient vanishing 문제가 발생하는 것을 방지한다.</p>

<p>WGAN-GP는 weight clipping이 모델 최적화를 어렵게 한다는 것을 보이면서, 이 문제를 해결하기 위해 만들어졌다. Gradient penalty는 weight clipping 대신 $D$의 Lipschitz constraint를 강화하기 위한 장치로서 제시되었다. 1-Lipschitz function인 D의 미분 계수를 입력값에 따라 직접적으로 제한하는 방식이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/loss.png?raw=true" style="zoom: 43%;" />
</p>

<p>SinGAN에서는 학습 안정성이 높은 WGAN-GP를 사용하였다. 또한 loss function을 patch 몇 개가 아닌 이미지 전체에 걸쳐 적용하였는데, 이것으로 모델이 boundary condition을 학습하게끔 했다.</p>

<p>Reconstruction loss는 원본 이미지를 생성하는 noise map의 존재를 가정하기 위함이다. $\tilde{x}_n^{rec}$은 $n$번째 단계에서 noise map ${z_N^{rec}, z_{N-1}^{rec}, …, z_0^{rec}}={z^*, 0, …, 0}$을 가지고 생성된 이미지이다. $z^*$은 학습 내내 고정되는 noise map이다.</p>

\[\begin{aligned}
L_{rec} &amp;= \lVert G_n(0, (\tilde{x}\_{n+1}^{rec}\uparrow^r))-x_n \rVert^2, \ n&lt;N \\
L_{rec} &amp;= \lVert G_N(z^*)-x_N \rVert^2, \ n&lt;N
\end{aligned}\]

<p><br /></p>

<h1 id="result">Result</h1>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig6.png?raw=true" style="zoom: 50%;" />
</p>

<p>SinGAN은 Single image를 학습하여 이미지를 자연스럽고도 다양하게 생성하였다. 원본 이미지의 전반적인 배치와 패턴 구조를 보존하여 현실감 있는 결과물을 만들어 냈다. 그림자, 물에 반사되는 모습 등이 자연스럽게 표현되었다. 그러면서도 patch의 새로운 조합을 생성하여 원본 이미지와 완전히 다른 이미지를 만들거나, 학습한 이미지보다 더 높은 화질을 갖는 이미지를 생성하기도 했다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig8.png?raw=true" style="zoom: 45%;" />
</p>

<p>Scale이 pyramid 형태인 모델 구조는 좋은 결과를 보였다. 학습 시 설정하는 scale 수에 따라 모델이 이미지를 생성하는 데에 이미지의 특성을 반영하는 정도가 결정되는 것을 볼 수 있었다. Scale 수가 작으면 coarse한 단계의 patch가 작아지므로 집약적인 디테일을 학습한다. Scale 수가 증가할 수록 patch가 커져 이미지의 전반적인 배치나 특성을 보존하도록 학습한다. 이것을 시험하기 위해 scale을 지정하여 원본 이미지를 얼마나 변화시켜 생성할 것인지 결정하였다. n=N일 때 얼룩말의 모습은 n=N-1, n=N-2일 때보다 부자연스럽다.</p>

<p>모델이 생성한 이미지가 얼마나 자연스러운지를 평가하기 위해 두 가지 metric이 사용되었다. 첫 번째 방법은 user study이다. 고용된 사람들에게 두 가지 질문을 했다. 하나는 원본 이미지와 생성된 이미지를 한 번에 1초 간 보여준 뒤 어떤 것이 가짜인지 묻는 paired case이다. 다른 하나는 하나의 이미지를 1초 간 보여준 뒤 그것이 가짜인지 아닌지를 묻는 unpaired case이다. 50개의 가짜 이미지를 랜덤하게 제공했다. 한편 coarsest scale을 N과 N-1로 달리하여 이미지를 준비하기도 했다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table1.png?raw=true" style="zoom: 30%;" />
</p>

<p>결과적으로 비교 기준이 없는 unpaired case에서, coarsest scale에 관해서는 전반적 구조가 더 보존되는 N-1일 때 사람들이 더 헛갈려 했다. 그러나 N일 때에도 40% 이상의 혼동율로, 분간할 수 없는 수준을 가리키는 50%에 근접하였다. SinGAN이 생성한 이미지가 사람이 보기에 꽤 자연스럽다는 것을 알 수 있다.</p>

<p>두 번째 방법은 Single Image Frechet Inception Distance(SIFID) metric이다. Frechet Inception Distance(FID)는 원본 이미지와 생성된 이미지 feature 분포의 편차를 측정하는 것이다. SIFID는 원본 이미지의 통계량을 얼마나 보존하였는지를 측정하기 위해 이 논문에서 제안한 것이다. SIFID는 FID가 이미지 당 하나의 vector를 사용하는 것과 달리, 이미지 내 위치 당 하나의 vector를 사용하여 원본 이미지와 그것으로부터 생성된 이미지 feature 간의 통계량 차이를 비교하는 기준이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table2.png?raw=true" style="zoom: 30%;" />
</p>

<p>Coarsest scale에 관해서는 user study의 경우와 같이 N-1일 때가 결과가 더 좋았다. 한편 user study와 달리 paired case의 결과가 더 좋았는데, SIFID는 원본과 생성 이미지를 비교하여 계산되기 때문이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table3.png?raw=true" style="zoom: 30%;" />
</p>

<p>SinGAN은 이미지 생성 뿐만아니라 다양한 image manipulation task에 사용될 수 있다. 모델 구조를 바꾸거나 fine-tuning을 적용하지 않고도 super resolution, editing, paint-to-image 등의 여러 task를 수행한다. 단 입력 이미지와 같은 특징 분포를 갖는 이미지만을 생성할 수 있다. n번째 scale 단계에서 down sampled된 이미지를 넣어 해당 이미지의 patch들의 분포에 맞게 학습해 나가는 방식이다. 성능 또한 각 task를 목적으로 만들어진 모델들에 비해 성능 또한 떨어지지 않는다. 예를 들어 super resolution의 경우 SRGAN, EDSR, DIP, ZSSR과 SinGAN을 비교했는데, single data 기반 SOTA 모델인 DIP, ZSSR 및 dataset 기반 모델인 EDSR보다 성능이 좋았고, SRGAN과는 거의 근접한 성능을 보였다.</p>

<h1 id="discussion">Discussion</h1>

<p>SinGAN은 Single image로 GAN을 학습하여 자연스러운 이미지를 생성하고, 다양한 이미지의 변용이 가능한 장점이 있다. 우선 SinGAN은 입력 데이터 하나만으로도 학습이 가능하다. 기존의 모델들은 특정 클래스에 속하는 이미지를 생성하는 모델을 학습시키기 위해 해당 클래스의 많은 이미지 데이터가 필요했다. SinGAN은 이 점에서 많은 데이터를 구할 필요가 없어 학습이 용이하다. 그럼에도 SinGAN은 사람이 보기에도 자연스러운 이미지를 생성해낼 수 있다. 기존의 Single image 기반 GAN 모델은 texture 이미지 생성에만 국한되었다. 나아가 이미지의 전반적인 패턴과 특징을 유지하면서도 물체의 배치 등을 변화시켜 입력 이미지와 다른 다양한 이미지를 생성할 수 있다.</p>

<p>많은 image manipulation task를 다루는 모델들은 특정한 task를 수행하기 위한 목적으로 만들어졌다. 그러나 SinGAN은 모델 구조 수정이나 tuning 등의 추가 작업을 하지 않으면서도 다양한 task를 해결할 수 있다. Scale을 변화시켜 학습하고, 테스트 시 적절한 단계에 이미지를 입력함으로써 super resolution, editing, paint-to-image, single image animation 등을 수행한다.</p>

<p>다만 user study에서 볼 수 있듯 결과물이 실제 이미지와 비교하였을 때 가짜인지 구분이 되는 정도이다. 더 자연스러운 이미지를 생성해내는 성능을 위해 발전할 여지가 있다.</p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  
  
    
      <a class="pagination-item newer" href="https://alatteaday.github.io/ko/page5">Newer</a>
    
  
</div>

        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
