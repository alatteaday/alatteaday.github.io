<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<!--site.title -->
 <title>Coffee Chat</title>
 <link href="https://alatteaday.github.io/ko/atom.xml" rel="self"/>
 <link href="https://alatteaday.github.io/ko/"/>
 <updated>2024-06-27T02:14:44-05:00</updated>
 <id>https://alatteaday.github.io</id>
 <author>
   <name>Jiyun</name>
   <email>jyuun.k@gmail.com</email>
 </author>

 
 <entry>
   <title>MRI Quality Assessment 및 Control 관련 네 개 논문 요약</title>
   <link href="https://alatteaday.github.io/ko/paper/2024/06/14/mriqcsurvey/"/>
   <updated>2024-06-14T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/paper/2024/06/14/mriqcsurvey</id>
   <content type="html">&lt;p&gt;다음은 MRI 품질 평가(quality assessment) 및 관리(quality control)와 관련된 네 편의 논문 요약입니다:&lt;/p&gt;

&lt;h1 id=&quot;paper-list&quot;&gt;Paper list&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Liao, Lufan, et al. “Joint image quality assessment and brain extraction of fetal MRI using deep learning.” &lt;em&gt;Medical Image Computing and Computer Assisted Intervention–MICCAI&lt;/em&gt; &lt;em&gt;2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VI 23&lt;/em&gt;. Springer International Publishing, 2020.&lt;/li&gt;
  &lt;li&gt;Giganti, Francesco, et al. “Prostate Imaging Quality (PI-QUAL): a new quality control scoring system for multiparametric magnetic resonance imaging of the prostate from the PRECISION trial.” European urology oncology 3.5 (2020): 615-619.&lt;/li&gt;
  &lt;li&gt;Esses, Steven J., et al. “Automated image quality evaluation of T2‐weighted liver MRI utilizing deep learning architecture.” &lt;em&gt;Journal&lt;/em&gt; &lt;em&gt;of&lt;/em&gt; &lt;em&gt;Magnetic&lt;/em&gt; &lt;em&gt;Resonance&lt;/em&gt; &lt;em&gt;Imaging&lt;/em&gt; 47.3 (2018): 723-728.&lt;/li&gt;
  &lt;li&gt;Monereo-Sánchez, Jennifer, et al. “Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations-insights from the Maastricht study.” &lt;em&gt;Neuroimage&lt;/em&gt; 237 (2021): 118174.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;joint-image-quality-assessment-and-brain-extraction-of-fetal-mri-using-deep-learning-2020&quot;&gt;Joint Image Quality Assessment and Brain Extraction of Fetal MRI Using Deep Learning (2020)&lt;/h1&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Quality Assessment (QA): MRI 이미지의 분석 적합성을 평가한다.&lt;/li&gt;
  &lt;li&gt;Brain Extraction (BE): MRI 이미지에서 뇌 영역을 식별하고 분리한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;지금까지 QA와 BE는 독립적으로 수행되어 왔으나, 이 연구에서는 두 작업 모두 이미지 내 뇌 영역에 집중하므로 동시에 최적화하면 성능을 향상시킬 수 있다고 주장한다. QA와 BE를 결합한 deep learning (DL) 모델을 제안한다. 또한 태아의 뇌는 영상 내 다양한 위치와 각도로 나타나고, 태아가 성장함에 따라 그 형태가 변하므로, 태아 뇌 영상을 다루는 것은 난이도가 높다. 이것을 해결하기 위해 deformable convolution method를 도입한다.&lt;/p&gt;

&lt;h2 id=&quot;contributions&quot;&gt;Contributions&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Joint optimization: QA와 BE를 결합하여, 모델에 shared feature를 학습시키고, overfitting 위험을 줄인다.&lt;/li&gt;
  &lt;li&gt;Multi-stage deep learning (DL) model:
    &lt;ul&gt;
      &lt;li&gt;Brain detector: MRI 스캔 내에서 뇌 영역을 찾는 detector를 사용한다. 이것으로 후속 작업에서 관련한 이미지 영역에 집중하도록 돕는다.&lt;/li&gt;
      &lt;li&gt;Deformable convolution: 태아 뇌는 크기와 형태가 다양하므로 이에 맞게 receptive field를 조정한다.&lt;/li&gt;
      &lt;li&gt;Task-specific module: 앞의 두 단계를 거친 후 모델이 QA와 BE를 동시에 수행하도록 한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multi-step training strategy: 모델을 점진적으로 학습시켜 모델 학습을 강화한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset: 태아 MRI 이미지, 2D 슬라이스 품질 평가.&lt;/li&gt;
  &lt;li&gt;Metrics:
    &lt;ul&gt;
      &lt;li&gt;Dice Similarity Coefficient (DSC): BE 정확도를 평가하는 주요 지표.&lt;/li&gt;
      &lt;li&gt;Quality Scores: 이미지 품질 분류 정확도.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Results:
    &lt;ul&gt;
      &lt;li&gt;0.89의 DSC score를 달성하여 높은 BE 정확도를 보였다.&lt;/li&gt;
      &lt;li&gt;85% accuracy의 이미지 품질 분류 성능을 보였다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;이 연구는 태아 MRI 스캔에서 QA와 BE를 동시에 처리하는 DL 모델을 제안했다. Deformable convolution을 사용해 뇌 이미지의 변동성을 처리하고, multi-step training과 다양한 dataset을 통한 검증으로 모델의 성능을 입증했다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;prostate-imaging-quality-pi-qual-a-new-quality-control-scoring-system-for-multiparametric-magnetic-resonance-imaging-of-the-prostate-from-the-precision-trial-2020&quot;&gt;Prostate Imaging Quality (PI-QUAL): A New Quality Control Scoring System for Multiparametric Magnetic Resonance Imaging of the Prostate from the PRECISION trial (2020)&lt;/h1&gt;

&lt;h2 id=&quot;background-1&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;PRECISION trial은 다기관 무작위 연구로, 다매개자기공명영상(multiparametric magnetic resonance imaging; mpMRI)을 타겟으로 하는 생검(biopsy)이 표준 경직장 초음파 유도(transrectal ultrasound-guided) biopsy보다 전립선암 진단에 우수하다는 것을 입증했다. 한편 mpMRI-targeted biopsy의 성공은 mpMRI 스캔 품질에 크게 의존하는데, 이 품질을 평가할 시스템이 기존에 존재하지 않았다.&lt;/p&gt;

&lt;h2 id=&quot;prostate-imaging-quality-pi-qual&quot;&gt;Prostate Imaging Quality (PI-QUAL)&lt;/h2&gt;

&lt;p&gt;이 문제를 해결하기 위해 Prostate Imaging Quality (PI-QUAL)이라는 새로운 평가 시스템을 도입했다. PI-QUAL은 1에서 5까지의 Likert scale이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1: mpMRI 시퀀스 품질이 진단에 적합하지 않음&lt;/li&gt;
  &lt;li&gt;5: 각각의 시퀀스가 독립적으로 진단에 최적화된 품질을 가짐&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;MRI 스캔 선택: PRECISION trial에서 252개의 mpMRI 스캔 중 58개(23%)가 랜덤으로 선택된다. 이 스캔은 trial에 참여한 22개 센터에서 가져왔다.&lt;/li&gt;
  &lt;li&gt;Radiologist의 평가: 숙련된 방사선 전문의가 각자 독립적으로, pathology를 모르는 상태로 MRI 스캔을 평가했다.&lt;/li&gt;
  &lt;li&gt;Metrics
    &lt;ul&gt;
      &lt;li&gt;Overall quality:  스캔의 전체 진단 품질 평가&lt;/li&gt;
      &lt;li&gt;특정 시퀀스 quality: T2WI, DWI, DCE와 같은 개별 시퀀스의 품질 별도 평가&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Statistical Analysis
    &lt;ul&gt;
      &lt;li&gt;충분한 진단 품질을 가진 스캔의 비율(PI-QUAL 점수 ≥3)을 계산했다.&lt;/li&gt;
      &lt;li&gt;좋은 또는 최적의 진단 품질을 가진 스캔의 비율(PI-QUAL 점수 ≥4)을 결정했다.&lt;/li&gt;
      &lt;li&gt;특정 영상 시퀀스의 진단 품질을 분석했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;전체 진단 품질: 58개 스캔 중 55개(95%)가 충분한 진단 품질(PI-QUAL 점수 ≥3)을, 35개(60%)가 좋은 또는 최적의 진단 품질(PI-QUAL 점수 ≥4)을 보였다.&lt;/li&gt;
  &lt;li&gt;시퀀스 별 품질: T2WI 스캔의 95%, DWI 스캔의 79%, DCE 스캔의 66%가 진단 품질을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-1&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;PI-QUAL 점수의 도입은 mpMRI 스캔의 품질을 평가하는 표준화된 방법을 제공한다. 다만 다양한 임상 환경에서 이 점수 시스템의 효과를 보장하기 위해 추가 검증이 권장된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;automated-image-quality-evaluation-of-t2-weighted-liver-mri-utilizing-deep-learning-architecture-2018&quot;&gt;Automated image quality evaluation of T2-weighted liver MRI utilizing deep learning architecture (2018)&lt;/h1&gt;

&lt;h2 id=&quot;background-2&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;간 T2WI MRI 스캔 검토는 진단을 효과적으로 하기 위해 정확해야 하는데, 방사선 전문의가 manual하게 평가하게 되면 시간이 많이 걸리고 의사마다 진단의 차이가 있다. DL, 특히 convolutional neural network (CNN)를 사용하는 자동화된 방법은 일관적이고 효율적인 이미지 품질 평가를 위한 솔루션을 제공한다. 이 연구는 CNN 기반 모델을 개발해 non-diagnostic 이미지를 식별하고, 모델의 결과를 전문의의 평가와 비교하고자 한다.&lt;/p&gt;

&lt;h2 id=&quot;method-1&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data collection: 2024.11 ~ 2016.05 간 1.5T 및 3T에서 수행된 522개 간 MRI 검사를 사용했다.&lt;/li&gt;
  &lt;li&gt;CNN architecture: CNN 모델은 input layer, convolutional layer, fully connected layer 및 output layer 등 여러 층으로 구성된다.&lt;/li&gt;
  &lt;li&gt;Training data: 351개 T2WI 이미지를 익명화하고, 병변(lesion)이 탐지되는지, 간 형태(morphology)가 보이는지 등에 따라 diagnostic/non-diagnostic으로 레이블링했다.&lt;/li&gt;
  &lt;li&gt;Validation data: 172개 T2WI 이미지를 테스트에 사용했다. 두 명의 방사선 전문의가 이미지를 평가해 위 두 개로 레이블링했다.&lt;/li&gt;
  &lt;li&gt;Comparison: 모델의 이미지 품질 관련 출력을 두 전문의의 판단과 비교했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-1&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;모델의 예측은 전문의 1과 79%, 전문의 2와 73% 일치했다.&lt;/li&gt;
  &lt;li&gt;Non-diagnostic 이미지를 식별하는 데 있어 CNN의 sensitivity와 specificity는
    &lt;ul&gt;
      &lt;li&gt;Sensitivity: 전문의 1과 67%, 전문의 2와 47%,&lt;/li&gt;
      &lt;li&gt;Specificity: 전문의 1과 81%, 전문의 2와 80% 일치했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Negative 예측값은 전문의 1과 94%, 전문의 2와 86% 일치했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-2&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;이 연구는 T2WI 이미지 품질 평가 자동화를 위해 DL, 특히 CNN 모델을 사용하는 가능성을 보여주었다. 모델의 성능을 방사선 전문의와 비교하였고, 결과적으로 모델이 높은 음성 예측값을 보여 diagnostic 이미지 식별에 있어 신뢰할 수 있음을 입증했다. 자동화된 품질 평가는 임상 시 전문의가 MRI 스캔의 품질을 신속 정확하게 결정하는 데 도움을 줄 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;quality-control-strategies-for-brain-mri-segmentation-and-parcellation-practical-approaches-and-recommendations---insights-from-the-maastricht-study-2021&quot;&gt;Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations - insights from the Maastricht study (2021)&lt;/h1&gt;

&lt;h2 id=&quot;background-3&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;뇌 MRI segmentation에서 품질 관리(quality control; QC)는 데이터의 정확성을 보장하는 데 중요하다. Manual QC는 gold standard로 간주되지만, dataset 규모가 클 경우 현실적이지 않다. 자동화된 방법은 빠르고 효율적인 대안이지만 이것이 최선의 방법인지에 대해 합의가 부족하다. 이 연구는 manual하게 segmentation을 편집하는 것(manual editing)이 갖는 영향을 밝히고, 다양한 QC 전략을 비교해 측정 오류를 효과적으로 줄이고자 한다.&lt;/p&gt;

&lt;h2 id=&quot;method-2&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data: Maastricht Study 참여자 259명의 structural brain MRI&lt;/li&gt;
  &lt;li&gt;Segmentation Tool: FreeSurfer 6.0을 사용해 형태학적(morphological) 추정치를 자동으로 추출&lt;/li&gt;
  &lt;li&gt;Manual Editing: 부정확한 segmentation을 manual하게 편집하고, 편집 전후의 morphological estimate를 비교&lt;/li&gt;
  &lt;li&gt;Quality Control Strategies:
    &lt;ul&gt;
      &lt;li&gt;Manual Strategy: 이미지를 제외하거나 편집하기 위해 일일이 눈으로 검사&lt;/li&gt;
      &lt;li&gt;Automated Strategy: MRIQC, Qoala-T 등의 도구를 사용해 이상치를 제외, morphological global measures, Euler numbers, Contrast-to-Noise ratio 등의 수치를 측정&lt;/li&gt;
      &lt;li&gt;Semi-automated Strategy: 도구와 지표로 감지된 이상치를 제외하지 않고 검사하여 manual editing&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation: 각 전략을 적용한 후 전체 분산에 비해 설명되지 않는 분산의 비율을 측정&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-2&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Manual QC: subcortical brain 용적에서 유의한 변화가 있었고, cortical surface, thickness 및 hippocampal 용적에서 어느 정도의 변화가 있었다.&lt;/li&gt;
  &lt;li&gt;Strategy performance: 관점이 된 morphological measure에 따라 달라진다.
    &lt;ul&gt;
      &lt;li&gt;Manual Strategy: 설명할 수 없는 분산이 제일 적었다.&lt;/li&gt;
      &lt;li&gt;Automated Alternative: Euler numbers와 MRIQC 점수 기반&lt;/li&gt;
      &lt;li&gt;Global Morphological Measure: 이상치를 제외하면 설명할 수 없는 분산이 증가한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-3&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;이 연구는 뇌 MRI segmentation에서 QC의 중요성을 강조한다. 대규모 dataset에서는 실질적으로 불가능한 manual method 대신 Euler 수 및 MRIQC를 사용하는 자동화 방법이 효과적이고, global estimate를 기반으로 이상치를 제외하는 방식은 오류가 증가한다는 것을 지적했다. 이로서 실질적인 QC strategy 구현에 관한 권장 사항을 제공한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 4] MRIQC Report와 Image Quality Metrics (IQMs)</title>
   <link href="https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/"/>
   <updated>2024-05-28T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/28/mriqc_report</id>
   <content type="html">&lt;style&gt;
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: -0.5em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
p.b{
   margin-top: 1em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
&lt;/style&gt;

&lt;h1 id=&quot;mriqc-results&quot;&gt;MRIQC Results&lt;/h1&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex1.png?raw=true&quot; alt=&quot;ex1&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex2.png?raw=true&quot; alt=&quot;ex2&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex3.png?raw=true&quot; alt=&quot;ex3&quot; style=&quot;width: 32%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;MRIQC를 사용하여 magnetic resonance imaging (MRI) 이미지를 분석하면 HTML 형식의 report를 얻을 수 있습니다. Report 결과는 크게 두 섹션으로 구분됩니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Basic visual report&lt;/strong&gt;: View of the background of the anatomical image, Zoomed-in mosaic view of the brain&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;About&lt;/strong&gt;: Errors, Reproducibility and provenance information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;view-of-the-background-of-the-anatomical-image&quot;&gt;View of the background of the anatomical image&lt;/h2&gt;

&lt;p&gt;MRI 상에서 뇌 영역을 둘러싸고 있는 배경(air) 부분의 결함(artifact)을 시각화하여 보여줍니다. 머리 주변의 배경에는 일반적으로 신호가 존재하지 않습니다. 이 air mask에서 감지되는 신호는 이미지 처리 과정에서 발생한 잡음이나 이상한 패턴, 즉 artifact라고 볼 수 있습니다. 잘 촬영된 &lt;a href=&quot;http://localhost:4000/ko/study/2023/12/26/mri2/&quot;&gt;T1 weighted-image (T1WI)&lt;/a&gt;와 괜찮은 영상에 인위적으로 noise를 추가한 T1WI의 MRIQC report를 비교해보겠습니다. noise는 torchio 라이브러리를 사용해 &lt;a href=&quot;https://mriquestions.com/ghosting.html&quot;&gt;ghosting&lt;/a&gt; 현상를 주었습니다.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_normal1.png?raw=true&quot; alt=&quot;mosaic_bg_normal1&quot; style=&quot;width: 49%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_normal2.png?raw=true&quot; alt=&quot;mosaic_bg_normal2&quot; style=&quot;width: 49%;&quot; /&gt;
&lt;/p&gt;

&lt;p class=&quot;a&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_abnormal1.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal1&quot; style=&quot;width: 49%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_abnormal2.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal2&quot; style=&quot;width: 49%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위쪽 결과가 잘 촬영된 영상(1번), 아래쪽이 noise가 추가된 영상(2번)의 report 결과 중 일부입니다. 슬라이스 별로 영상 내 신호 강도가 밝기로 표시 됩니다. 색이 진할수록 신호가 강하게 나타납니다. 1번 이미지에서는 전반적으로 head mask가 어둡고, air mask가 밝아 명확히 구분됩니다. 반면 2번 이미지에서는 head와 air의 밝기 차이가 상대적으로 크지 않고, 오히려 air보다 강도가 약한 head 부분도 존재합니다. 자세히 보면 인위적으로 생성한 ghosting 현상이 이미지에서 물결 무늬로 나타납니다. 이렇게 background artifact 검사를 통해 잡음이 개입되지 않고 배경이 배제되어 뇌 영역이 잘 촬영되었는지 정성적으로 판단할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;zoomed-in-mosaic-view-of-brain&quot;&gt;Zoomed-in mosaic view of brain&lt;/h2&gt;

&lt;p&gt;MRI를 슬라이스 순서대로 나열해(mosaic view) 보여 줍니다. 이미지 중 뇌 부분을 자세히 보기 위해 배경부는 거의 제외되고 head mask 크기에 맞게 확대되어(zoomed-in) 있습니다. Mosaic view를 통해 MRI 촬영 시 움직임이 있었는지(head-motion), 이미지의 밝기가 균일하게 나타나는지(intensity inhomogeneities), 이미지에 전반적 또는 국소적인 noise가 있는지(global/local noise) 등을 확인하여 품질을 평가할 수 있습니다. 위에서 사용된 두 이미지의 MRIQC report 결과를 다시 비교해보겠습니다.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_normal1.png?raw=true&quot; alt=&quot;mosaic_bg_normal1&quot; style=&quot;width: 48.5%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_normal2.png?raw=true&quot; alt=&quot;mosaic_bg_normal2&quot; style=&quot;width: 50.5%;&quot; /&gt;
&lt;/p&gt;

&lt;p class=&quot;a&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_abnormal1.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal1&quot; style=&quot;width: 48.5%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_abnormal2.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal2&quot; style=&quot;width: 50.5%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;위쪽이 1번, 아래쪽이 2번의 결과입니다. 전반적으로 화질이나 구조물의 영역 간 구분 등을 기준으로 1번 이미지가 더 선명한 것을 볼 수 있습니다. Head-motion의 경우 mosaic view의 모든 슬라이스 이미지를 놓고 판단했을 때, 두 이미지 모두 두드러진 관련 사항은 없었습니다. 다만 2번 이미지의 경우 인위적으로 추가한 ghosting noise가 슬라이스 내에서 관찰됩니다. Head mask 내 물결 무늬가 나타나 영상 화질이 떨어져 보입니다. 이렇게 mosaic view를 통해 직접적으로 이미지를 검토함으로써 문제사항에 대해 판단할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reproducibility-and-provenance-information&quot;&gt;Reproducibility and provenance information&lt;/h2&gt;

&lt;p&gt;MRIQC report 결과의 재현성 및 투명성을 보장하기 위해 품질 검사 관련 출처 사항을 알려줍니다.&lt;/p&gt;

&lt;h3 id=&quot;provenance-information&quot;&gt;Provenance Information&lt;/h3&gt;

&lt;p&gt;재현성과 출처 관련 메타데이터를 제공합니다. 여기에는 분석 환경(Execution environment), 사용한 데이터 경로(Input filename), 사용된 패키지의 버전(Versions), 파일 무결성 검증을 위한 MD5 checksum(MD5sum) 등의 정보가 포함됩니다.&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/prov_info.png?raw=true&quot; alt=&quot;prov_info&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Execution environment: 분석 환경. 여기서는 ‘singularity’ 컨테이너 환경에서 실행되었음을 의미합니다.&lt;/li&gt;
  &lt;li&gt;Input filename: 사용한 데이터의 경로.&lt;/li&gt;
  &lt;li&gt;Versions: MRIQC, NiPype, TemplateFlow 등 사용한 패키지의 버전.&lt;/li&gt;
  &lt;li&gt;md5sum: 입력한 파일과 같은 파일을 사용하였는지 확인하기 위한 MD5 checksum.&lt;/li&gt;
  &lt;li&gt;warnings: ‘large_rot_frame’은 이미지 내 큰 회전 프레임이 있었는지, ‘small_air_mask’는 작은 air mask가 있었는지를 나타냅니다. 두 요인 모두 이미지 분석 정확성에 영향을 미칠 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset-information&quot;&gt;Dataset Information&lt;/h3&gt;

&lt;p&gt;분석에 사용된 데이터 관련 메타데이터가 제공됩니다.&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/data_info.png?raw=true&quot; alt=&quot;data_info&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AcquisitionMatrixPE: matrix의 인코딩 방향 크기. 위 예시에서는 256 x 256을 나타냅니다.&lt;/li&gt;
  &lt;li&gt;AcquisitionTime: 이미지 스캔이 수행된 시점.&lt;/li&gt;
  &lt;li&gt;ConversionSoftware: DICOM을 NIfTI로 변환하는 데 사용된 소프트웨어. 여기서는 ‘dcm2niix’가 사용되었습니다.&lt;/li&gt;
  &lt;li&gt;ConversionSoftwareVersion: 위 변환 소프트웨어의 버전.&lt;/li&gt;
  &lt;li&gt;HeudiconvVersion: 파일을 BIDS 형식으로 만드는 데 사용한 Heudiconv의 버전.&lt;/li&gt;
  &lt;li&gt;ImageOrientationPatientDICOM: 환자의 몸의 방향 관련 벡터 정보&lt;/li&gt;
  &lt;li&gt;ImageType: 이미지의 유형으로, 여기서는 ‘2차적’으로 생성 유도된 이미지임을 의미합니다.&lt;/li&gt;
  &lt;li&gt;InstitutionName: 데이터의 출처가 되는 기관명.&lt;/li&gt;
  &lt;li&gt;Modality: 이미지의 촬영 방식. 여기서는 ‘Magnetic Resonance (MR)’ 이미지가 사용되었습니다.&lt;/li&gt;
  &lt;li&gt;ProtocolName: 사용한 프로토콜의 이름.&lt;/li&gt;
  &lt;li&gt;RawImage: raw image 인지 아닌지를 나타냅니다.&lt;/li&gt;
  &lt;li&gt;ReconMatrixPE: 재구성된 행렬의 인코딩 방향 크기. 여기서는 256 x 256을 나타냅니다.&lt;/li&gt;
  &lt;li&gt;ScanningSequence: 사용된 스캐닝 시퀀스.&lt;/li&gt;
  &lt;li&gt;SeriesNumber: 시리즈 번호로, dataset이 속한 시리즈를 식별하는 데 사용됩니다.&lt;/li&gt;
  &lt;li&gt;SliceThickness: 슬라이스의 두께.&lt;/li&gt;
  &lt;li&gt;SpacingBetweenSlice: 각 슬라이스 사이 간격.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;image-quality-metrics&quot;&gt;Image Quality Metrics&lt;/h3&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/iqm.png?raw=true&quot; alt=&quot;iqm&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;이미지 품질을 정량적으로 평가하는 다양한 Image quality metrics (IQMs) 점수가 보고됩니다. 이미지 모달리티(modality)에 따라 metric 항목이 달라집니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IQMs for structural images: T1WI, T2WI 등&lt;/li&gt;
  &lt;li&gt;IQMs for functional images: fMRI 관련 이미지 등&lt;/li&gt;
  &lt;li&gt;IQMs for diffusion images: DWI 등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IQM 점수 결과는 각 이미지의 MRIQC output directory에 생성되는 JSON 파일에서도 찾아볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;iqms-for-structural-images&quot;&gt;IQMs for Structural Images&lt;/h1&gt;

&lt;p&gt;이번 예시에서 T1WI를 사용함에 따라 IQMs for structural images에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;measures-based-on-noise-measurements&quot;&gt;Measures based on noise measurements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cjv&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Coefficient of joint variation (CJV; 계수 결합 변이)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;두 개 이상의 변수를 동시에 고려한 상대적 변이의 측도로, 여러 변수의 변이가 그들 간 평균에 비해 얼마나 큰지를 알려준다.&lt;/li&gt;
      &lt;li&gt;여러 변수를 포함한 데이터셋을 다룰 때 유용하며, 전체적인 변이를 이해하는 데에 도움을 준다.&lt;/li&gt;
      &lt;li&gt;여러 변수의 표준 편차를 변수들의 평균으로 나눈 비율로 계산한다:&lt;/li&gt;
    &lt;/ul&gt;

\[CJV={(Standard \ Deviation \ of \ Combined \ Variables)\over(Mean \ of \ Combined \ Variables)}\times100\%\]

    &lt;ul&gt;
      &lt;li&gt;MRIQC에서는 뇌의 회백질(gray matter)과 백질(white matter) 간 CJV를 구한다. GM과 WM의 CJV는 Intensity non-uniformity (INU) 보정 알고리즘 최적화의 object function으로서 &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fninf.2016.00010/full&quot;&gt;Granzetti 등&lt;/a&gt;이 제안했다.
        &lt;ul&gt;
          &lt;li&gt;INU은 MRI에서 서로 다른 부위에서 나타나는 밝기의 불균일성을 말한다. 자기장이 균질하지 않은 경우, 특히 라디오 주파수(radio frequency; RF) 전파 강도에 의해 발생한다.&lt;/li&gt;
          &lt;li&gt;INU는 이미지의 정확성을 저하시켜 해석을 어렵게 할 수 있으므로, MRI 품질 향상을 위해 INU를 보정하는 것이 좋다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;CJV가 높을 수록 머리가 강하게 움직였거나, INU 결함이 크다는 것을 의미한다. 따라서 CJV가 작을 수록 이미지 quality가 좋다고 평가할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snr&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Signal-to-noise ratio (SNR; 신호 대 잡음 비율)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;측정한 신호의 강도와 주변 잡음 수준의 관계를 나타내는 측도로, 측정된 신호의 품질과 정확성을 나타낸다. 신호(signal)는 관찰 대상인 조직에서 보이는 신호, 잡음(noise)은 환자의 움직임이나 전자기기의 간섭 등으로 나타나는 신호로, SNR은 둘을 구분하기 위해 사용된다.&lt;/li&gt;
      &lt;li&gt;SNR이 높을수록 측정하고 싶은 신호가 잡음에 비해 크다, 즉 데이터의 quality가 좋다.&lt;/li&gt;
    &lt;/ul&gt;

\[SNR={Signal \ Strength\over Stnadard \ Deviation \ of \ Noise}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snrd&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Dietrich’s SNR (SNRd)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;MRI에서 주변 대기 배경을 참조로 하여 SNR을 계산하는 것으로, MRI 품질을 평가하는 중요 지표 중 하나이다.&lt;/li&gt;
      &lt;li&gt;대기는 일반적으로 균일한 신호를 가지므로, 이를 참조하면 신호를 잡음과 더 정확하게 구별할 수 있다. 이로써 더 정확하게 이미지를 진단할 수 있다. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/jmri.20969&quot;&gt;Dietrich 등&lt;/a&gt;이 제안하였다.&lt;/li&gt;
    &lt;/ul&gt;

\[SNRd={Signal \ Strength\over Stnadard \ Deviation \ of \ Air Background}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnr&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Contrast-to-noise ratio (CNR; 대비 대 잡음 비율)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;이미지에서 대비와 잡음 수준의 관계를 나타내는 측도로, SNR을 확장한 개념이다. 대비(contrast)는 이미지 내 구조나 물체 간의 밝기 차이를, 잡음(noise)은 불규칙하거나 무작위하게 나타나는 신호를 말한다.&lt;/li&gt;
      &lt;li&gt;CNR이 높을수록 원하는 이미지 대비를 얻었을 때 잡음이 낮다. 즉 높은 CNR은 물체나 구조가 뚜렷이 표현되어 있으면서도 잡음이 낮아 이미지 해석이 쉽고 이미지 quality가 좋다는 것을 의미한다.&lt;/li&gt;
      &lt;li&gt;MRIQC에서는 CNR을 GM과 WM가 얼마나 잘 분리되어 나타나고 영상 해석이 쉬운지를 평가하기 위해 사용한다.&lt;/li&gt;
    &lt;/ul&gt;

\[CNR={|\mu_{GM}-\mu_{WM}|\over \sqrt{\sigma^2_{GM}+\sigma^2_{wM}}}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qi_2&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Mortamet’s Quality index 2 (QI2; 품질 지수 2)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;인위적 강도(artificial intensities)가 제거된 후 대기 마스크(air mask) 상의 데이터 분포가 적합한지를 평가하는 지표이다. 대기 마스크 영역 내 데이터 분포의 적합성은 이미지 처리 및 해석의 신뢰성에 영향을 미칠 수 있다.&lt;/li&gt;
      &lt;li&gt;낮은 값일수록 좋은 품질을 나타낸다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;measures-based-on-information-theory&quot;&gt;Measures based on information theory&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;efc&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Entropy-focus criterion (EFC)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;머리 움직임에 의해 발생한 ghosting과 blurring의 지표로 voxel 강도의 Shannon entropy를 사용하는 측정법이다. &lt;a href=&quot;https://ieeexplore.ieee.org/document/650886&quot;&gt;Atkinson 등&lt;/a&gt;이 제안했다.&lt;/li&gt;
      &lt;li&gt;ghosting과 blurring이 증가할수록 voxel은 정보량을 잃게 되어, 보클의 Shannon entropy가 증가한다. 즉, EFC는 ghosting 및 blurring이 많을수록 큰 값을 가지므로, 낮은 값일수록 이미지의 quality가 좋다.&lt;/li&gt;
      &lt;li&gt;계산식은 maximum entropy로 normalize되어 있어 이미지 차원이 달라도 비교할 수 있다. $p_i$는 각 voxel의 확률, $N$은 pixel 수를 의미한다.&lt;/li&gt;
    &lt;/ul&gt;

\[EFC={-\sum^N_i=1 p_i\log_2(p_i) \over \log_2(N)}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fber&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Fraction of brain explained by resting-state data (FBER)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;이미지 속 뇌 조직의 평균 에너지를 뇌 주변의 대기 값과 비교한다. 이것으로 이미지 상 뇌 조직이 얼마나 포함되어 있는지를 측정하여 이미지 품질을 평가한다. &lt;a href=&quot;&quot;&gt;Shehzad 등&lt;/a&gt;이 제안했다.&lt;/li&gt;
      &lt;li&gt;Quality assurance protocol (QAP) 측정 항목 중 하나이다.&lt;/li&gt;
    &lt;/ul&gt;

\[FBER ={Mean \ energy \ of image \ value \ within \ the \ head \over Mean \ energy \ of image \ value \ outside \ the \ head}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;measures-targeting-specific-artifacts&quot;&gt;Measures targeting specific artifacts&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inu&lt;/code&gt; : &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;N4ITK로 추출된 INU bias field에 대한 요약 통계(max, min, median)&amp;lt;/span
&lt;/span&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/5445030&quot;&gt;N4ITK&lt;/a&gt; 알고리즘은 MRI의 RF field 불균일성을 보정하여 영상의 품질을 향상시키는 고급 기법이다.&lt;/li&gt;
      &lt;li&gt;INU field 또는 bias field는 N4ITK를 통해 필터링 된 field를 말한다. INU field에 대한 통계를 통해 영상의 quality를 판단할 수 있다. 값이 0에 가까울수록 RF field 불균일성이 크다는 것을 의미하므로, 통계치가 1에 가까울수록 보정이 잘 된, quality가 높은 영상이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qi_1&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Mortamet’s Quality index 1 (QI1; 품질 지수 1)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;대기 마스크 상의 인위적 강도를 감지하는 데 사용되는 지수이다. 인위적인 강도를 제거하여 대기 마스크를 올바르게 분석하기 위한 목적으로 사용한다.&lt;/li&gt;
      &lt;li&gt;일반적으로 MRI 등 영상 데이터 전처리 단계에서 이미지의 품질을 향상시킴으로서 중요한 지표로 여겨진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wm2max&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;White-matter to maximum intensity ratio&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;WM 내 중간 intensity와 전체 intensity 분포의 95% 백분위수(percentile)의 비율이다. 이로써 WM 영역 내 중요하게 나타난 강도의 비율을 측정한다.&lt;/li&gt;
      &lt;li&gt;이 비율을 통해 intensity의 분포 상 꼬리가 어떤 경우에 길게 나타나는지를 알 수 있는데, 이 꼬리는 보통 성상 동맥 혈관이나 지방 조직의 intensity에 의해 발생할 수 있다.&lt;/li&gt;
      &lt;li&gt;비율이 0.6에서 0.8 사이를 벗어나는 경우 이미지의 WM 영역이 불균일하다고, 즉 quality가 떨어진다고 판단할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-measures&quot;&gt;Other measures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fwhm&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Full width ad half maximum (FWHM)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;이미지 intensity 값의 spatial distribution에서 전체 너비를 나타내는 값으로, 이미지의 해상도와 선명도를 측정하는 데 사용된다.&lt;/li&gt;
      &lt;li&gt;Spatial distribution의 최고점의 절반 값에서부터 얻을 수 있는 전체 너비 값으로 구해진다.&lt;/li&gt;
      &lt;li&gt;FWHM 값이 낮을수록 선명하고 고해상도의 이미지를 나타낸다.&lt;/li&gt;
      &lt;li&gt;MRIQC에서는 AFNI의 3dWHMx에 구현된 Gaussian width estimator filter를 사용해 FWHM를 계산한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icvs_*&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Intracranial volume scaling (ICVS)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Intracranial volume (ICV; 두개내액 체적)은 뇌와 두개액을 둘러싸고 있는 두개막 내 액체의 총량을 의미한다. ICVS는 MRI 에서 ICV를 기준으로 어떤 조직의 상대적인 비율을 나타낸다.&lt;/li&gt;
      &lt;li&gt;MRIQC에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volume_fraction()&lt;/code&gt; 함수로 cerebrospinal fluid (CSV; 뇌척수액), GM, WM의 ICVS를 계산한다.&lt;/li&gt;
      &lt;li&gt;각 ICVS가 정상 범위 내에서 변동하는지, 서로 간 이상적인 비율을 갖는지를 보고 뇌의 상태를 판단할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summary_*_*&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;MRIQC의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summary_stats()&lt;/code&gt; 함수는 MRI 내 배경(background), CSF, GM, WM 영역의 픽셀 분포에 관련된 다양한 통계량을 제공한다. 이러한 영상의 통계량을 quality 평가에 사용할 수 있다.&lt;/li&gt;
      &lt;li&gt;제공되는 통계량: 평균(mean), 중간값(median), 중간값 절대 편차(median absolute deviation; MAD), 표준 편차(standard deviation), 첨도(kurtosis), 하위 5% 백분위수(5th percentile), 상위 95% 백분위수(95th percentile), 픽셀 수(number of voxels).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tpm&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Tissue probability map (TPM)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;뇌 조직 유형(GM, WM 등)의 확률 분포를 가리킨다. MRIQC에서는 이미지에서 추정된 TPM과 ICBM nonlinear-asymmetric 2009c templete의 map 간의 중첩을 측정한다.&lt;/li&gt;
      &lt;li&gt;ICBM nonlinear-asymmetric 2009c templete: 표준 brain map을 제공하는 국제 협회인 International consortium for brain mapping (ICBM)이 제공하는 templete 중 하나이다.
        &lt;blockquote&gt;
          &lt;p&gt;A number of unbiased non-linear averages of the MNI152 database have been generated that combines the attractions of both high-spatial resolution and signal-to-noise while not being subject to the vagaries of any single brain (Fonov et al., 2011). … We present an unbiased standard magnetic resonance imaging template brain volume for normal population. These volumes were created using data from ICBM project.&lt;/p&gt;

          &lt;p&gt;6 different templates are available: …&lt;/p&gt;

          &lt;p&gt;ICBM 2009c Nonlinear Asymmetric template – 1×1x1mm template which includes T1w,T2w,PDw modalities, and tissue probabilities maps. Intensity inhomogeneity was performed using N3 version 1.11 Also included brain mask, eye mask and face mask.Sampling is different from 2009a template. … &lt;a href=&quot;https://nist.mni.mcgill.ca/icbm-152-nonlinear-atlases-2009/&quot;&gt;[Reference]&lt;/a&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mriqc.readthedocs.io/en/latest/iqms/t1w.html#ganzetti2016&quot;&gt;MRIQC’s Documentation - IQMs for Structural Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 3-1] Flask를 사용해 HTML 파일 열어보기</title>
   <link href="https://alatteaday.github.io/ko/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/"/>
   <updated>2024-05-21T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/dev%20tips%20&%20fixes/2024/05/21/html_flask</id>
   <content type="html">&lt;p&gt;MRIQC는 MRI 이미지의 퀄리티를 분석 및 평가 후 report를 HTML 파일로 출력해 줍니다. HTML 파일을 열어보기 위해서 Flask를 사용했는데요, 제가 사용한 방법을 정리해보겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;flask&quot;&gt;Flask&lt;/h1&gt;

&lt;p&gt;Flask는 Python으로 작성된 마이크로 웹 프레임워크입니다. 가볍고 유연한 구조로, 간단한 웹 애플리케이션과 API 서버를 빠르게 개발할 수 있도록 도와줍니다. 기본 기능만 포함하고 있어 확장성이 높고, 필요에 따라 다양한 플러그인과 확장 모듈을 추가할 수 있습니다. 또한, 배우기 쉽고 직관적인 코드 구조를 갖추고 있어 초보자에게도 적합합니다. 다만 최소한의 기능을 갖추고 있어서 복잡한 기능을 추가하기 위해선 외부 라이브러리를 사용해야 하며, 프로젝트 규모가 커질수록 유지보수가 어려워질 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;flask로-html-파일-열기&quot;&gt;Flask로 HTML 파일 열기&lt;/h1&gt;

&lt;h2 id=&quot;installing-flask&quot;&gt;Installing Flask&lt;/h2&gt;

&lt;p&gt;PyPI를 통해 설치할 수 있습니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install Flask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;static과-templates&quot;&gt;‘static/’과 ‘templates/’&lt;/h2&gt;

&lt;p&gt;Flask는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates/&lt;/code&gt; 두 폴더를 필요로 합니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt;에는 HTML 파일에 존재하거나 적용되는 이미지, CSS, JavaScript 등 정적 파일을 저장합니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates/&lt;/code&gt;에는 렌더링할 HTML 파일을 저장합니다.&lt;/p&gt;

&lt;p&gt;MRIQC 보고서를 여는 과정을 예시로 설명하겠습니다. 프로젝트 폴더에 위 두 폴더를 생성한 뒤, 정적 파일과 열고자 하는 HTML 파일을 각각 저장합니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/1.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;HTML 파일 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt;에 저장한 파일 경로가 이미 존재했다면 바뀐 경로로 수정해줍니다. MRIQC report HTML 파일을 열어보면 이미지 파일이 상대 경로로 지정되어 있습니다. 이미지를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt;에 옮겼으므로 이에 맞게 절대 경로로 바꿔줍니다:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/2.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/3.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;실행-코드-작성&quot;&gt;실행 코드 작성&lt;/h2&gt;

&lt;p&gt;그리고 HTML 파일을 렌더링하기 위한 코드를 작성합니다. 위 예시 이미지 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.py&lt;/code&gt;에 해당합니다:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@app.route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sub-001_ses-001_T1w.html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;0.0.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app = Flask(__name__)&lt;/code&gt;: Flask 애플리케이션 인스턴스를 생성합니다. __name__은 현재 모듈의 이름을 의미하며, Flask가 애플리케이션의 리소스를 찾는 데 사용됩니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@app.route(&quot;/&quot;)&lt;/code&gt;: 데코레이터로 기본 URL(/)에 대해 test 함수를 호출하도록 Flask에 지시합니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test()&lt;/code&gt;: 기본 URL이 요청될 때 실행될 함수입니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;return render_template(HTML_FILE_NAME.html)&lt;/code&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates/&lt;/code&gt; 디렉토리에 있는 HTML 파일을 렌더링하여 반환합니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.run(&quot;0.0.0.0&quot;, port=5001)&lt;/code&gt;: 애플리케이션을 0.0.0.0 주소와 5001 포트에서 실행합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;결과&quot;&gt;결과&lt;/h2&gt;

&lt;p&gt;입력한 주소에 들어가보면 HTML 파일이 잘 띄워진 것을 볼 수 있습니다:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/4.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://flask.palletsprojects.com/en/3.0.x/&quot;&gt;Flask’s Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://daeunnniii.tistory.com/103&quot;&gt;https://daeunnniii.tistory.com/103&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 3] MRIQC 실행하기: 사전 작업부터 결과 확인까지</title>
   <link href="https://alatteaday.github.io/ko/study/2024/05/20/mriqc_run/"/>
   <updated>2024-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/20/mriqc_run</id>
   <content type="html">&lt;style&gt;
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: -0.5em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
p.b{
   margin-top: 1em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
&lt;/style&gt;

&lt;p&gt;MRIQC는 입력된 MRI 이미지의 quality를 분석 및 평가하고, 관련 내용을 report로 정리해줍니다. MRIQC를 사용하기 위해서는 &lt;a href=&quot;https://alatteaday.github.io/study/2024/05/20/bids/&quot;&gt;BIDS&lt;/a&gt; 형식에 맞게 저장된 MRI 이미지가 필요합니다. 이번 포스트에서는 DICOM 파일을 가지고 MRIQC를 실행하고 분석 결과를 얻는 일련의 과정을 상세히 설명해보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;nii2dcm&quot;&gt;nii2dcm&lt;/h1&gt;

&lt;p&gt;여기서는 DICOM 파일을 사용하지만, NIfTI 형식의 파일 또한 일반적인 MRI 파일 포맷 중 하나입니다. NIfTI 파일을 사용하는 경우 NIfTI를 지원하는 BIDS converter를 사용하거나, NIfTI를 DICOM으로 변환한 뒤 DICOM 지원 BIDS converter를 사용할 수 있습니다. 개인적인 경험으로는 NIfTI 지원 BIDS converter들이 안정적으로 작동하지 않았습니다 (제가 실패한 것일 수도 있습니다만). &lt;a href=&quot;https://github.com/tomaroberts/nii2dcm&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nii2dcm&lt;/code&gt; 라이브러리&lt;/a&gt;를 사용해 NIfTI를 DICOM으로 변환할 수 있습니다. 아래 코드를 실행할 수 있습니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nii2dcm NIFTI_FILE_DIR OUTPUT_DIR -d MR
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NIFTI_FILE_DIR&lt;/code&gt;: 변환하고자 하는 NIfTI 파일 경로&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: 변환된 DICOM 파일을 저장할 경로&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;heudiconv&quot;&gt;Heudiconv&lt;/h1&gt;

&lt;p&gt;저는 BIDS converter로 Heudiconv를 사용했습니다. 공식 페이지에서 제공하는 튜토리얼를 참고하며 제가 성공적으로 실행한 방법을 정리했습니다. 사용 방법은 다음과 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;heudiconv-설치&quot;&gt;Heudiconv 설치&lt;/h2&gt;

&lt;p&gt;pip를 통해 설치합니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install heudiconv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;heuristicpy-작성&quot;&gt;heuristic.py 작성&lt;/h2&gt;

&lt;p&gt;각 이미지가 BIDS 형식에 맞춰 저장되도록 규칙을 정의하는 코드를 작성합니다. 튜토리얼에 제공된 데이터 저장소에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;heuristic.py&lt;/code&gt; 파일을 참고하거나 직접 수정할 수 있습니다. 이 파일은 입력된 이미지 파일의 모달리티를 판단하고, 각 모달리티별로 BIDS 형식에 맞는 파일 경로를 생성하여 이미지를 새로 저장합니다. 필요한 경우 판단 조건과 저장 경로를 수정할 수 있습니다.&lt;/p&gt;

&lt;p&gt;참고 및 수정할 함수는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;heuristic.py&lt;/code&gt; 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;infotodict()&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사용할 이미지의 모달리티를 인지합니다: T1WI, T2WI, DWI 등&lt;/li&gt;
  &lt;li&gt;사용할 모달리티가 아닌 경우 관련 코드를 삭제하거나 주석 처리합니다.&lt;/li&gt;
  &lt;li&gt;사용할 이미지의 모달리티가 저장될 경로 형식을 확인하고 필요한 경우 수정합니다.&lt;/li&gt;
  &lt;li&gt;각 모달리티를 구분할 수 있는 기준(차원, 현재 파일명 특징 등)을 조건문에 명시하여 수정합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;제가 수정한 예시 코드는 아래와 같습니다. T1WI과 DWI를 사용하는 경우, 이미지가 저장될 경로와 이미지의 모달리티를 판단한 조건을 설정하였습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/bids_ex1.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;heudiconv-실행&quot;&gt;Heudiconv 실행&lt;/h2&gt;

&lt;p&gt;설치 후 아래와 같이 파라미터를 설정하여 실행합니다. Heudiconv는 여러 개의 subject 데이터, 즉 DICOM 파일 묶음 여러 개를 한 번에 처리할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;heudiconv --files DICOM_FILE_DIRS -o OUTPUT_DIR -f HEURISTIC.PY -s SUB_ID -ss SES_ID -c dcm2niix -b minmeta --overwrite 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DICOM_FILE_DIRS&lt;/code&gt;: 여러 subject의 DICOM 파일을 글로빙(globbing) 형식으로 입력 (e.g. dataset/sub-001/ses-001/*/*.dcm)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: 변환된 BIDS 형식의 폴더가 저장될 경로&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HEURISTIC.PY&lt;/code&gt;: 위에서 작성한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;heuristic.py&lt;/code&gt; 파일의 경로&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUB_ID&lt;/code&gt;: Subject id (e.g. 001)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SES_ID&lt;/code&gt;: Session id (e.g. 001)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;실행 예시는 다음과 같습니다. 아래 코드를 입력할 경우:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;heudiconv --files data/*/*.dcm -o bids/data/ -f heuristic.py -s 0 -ss 0 -c dcm2niix -b minmeta --overwrite 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bids/data/&lt;/code&gt; 아래 다음과 같이 BIDS 형식의 폴더가 생성됩니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/bids_ex2.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mriqc&quot;&gt;MRIQC&lt;/h1&gt;

&lt;p&gt;BIDS 형식으로 저장된 MRI 이미지가 준비되었다면, 이를 MRIQC에 입력할 수 있습니다. MRIQC는 PyPI를 통해 다운로드하여 사용하거나, docker 컨테이너를 통해 사용할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;with-pypi&quot;&gt;With PyPI&lt;/h2&gt;

&lt;p&gt;우선 아래 코드를 통해 설치해줍니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m pip install -U mriqc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;설치 후 실행 코드는 아래와 같습니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mriqc BIDS_ROOT_DIR OUTPUT_DIR participant --participant-label SUB_ID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BIDS_ROOT_DIR&lt;/code&gt;: BIDS format 폴더의 루트 경로&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: MRIQC 결과를 저장할 경로&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant OR group&lt;/code&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant&lt;/code&gt;로 지정할 경우 subject를 단위로, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group&lt;/code&gt;으로 지정할 경우 루트 경로 하 모든 이미지를 대상으로 MRIQC 분석 결과를 얻습니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUB_ID&lt;/code&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant&lt;/code&gt; 모드의 경우 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--participant-label&lt;/code&gt;에 subject id를 입력하여 분석할 subject를 지정할 수 있습니다. 복수의 id를 한번에 입력할 수 있습니다 (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--participant-label 001 002 003&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;with-docker&quot;&gt;With Docker&lt;/h2&gt;

&lt;p&gt;저는 Docker를 통해 MRIQC를 사용했습니다. Docker 컨테이너는 프로그램의 실행에 필요한 모든 종속성을 포함하기 때문에 일관된 환경을 보장하는 장점이 있습니다. 아래 코드를 입력하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant&lt;/code&gt; level에서 MRIQC를 실행할 수 있습니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --rm -v BIDS_ROOT_DIR:/data:ro -v OUTPUT_DIR:/out nipreps/mriqc:latest /data /out participant --participant_label SUB_ID [--verbose-reports]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nipreps/mriqc&lt;/code&gt; 이미지가 다운로드되어 있지 않아도 코드를 실행할 시 자동으로 다운로드됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BIDS_ROOT_DIR&lt;/code&gt;: BIDS format 폴더의 루트 경로. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-v&lt;/code&gt; flag에 따라 컨테이너 내부의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/data&lt;/code&gt; 폴더와 연결됩니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ro&lt;/code&gt; 옵션은 ‘read only’로, 로컬 경로에서 컨테이너 경로로 읽기만 가능하다는 의미를 가집니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: MRIQC 결과를 저장할 경로. 컨테이너 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/out&lt;/code&gt; 폴더와 연결됩니다. 컨테이너의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/out&lt;/code&gt; 폴더 내용을 로컬로 복사해보면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;과 동일한 결과가 저장되어 있는 것을 확인할 수 있습니다.
    &lt;ul&gt;
      &lt;li&gt;컨테이너 내부 파일 내용 복사하기: 위 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run&lt;/code&gt; 실행 시 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--rm&lt;/code&gt; (작업 완료 후 삭제) 옵션을 삭제하고, 작업 완료 후 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker cp CONTAINER_NAME:FILE_PATH LOCAL_PATH&lt;/code&gt; 실행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUB_ID&lt;/code&gt;: Subject id. 여러 개의 id를 입력할 수 있습니다. (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--participant_label 001 002 003&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--verbose-reports&lt;/code&gt; (Optional): 이 flag를 입력하면 기본적으로 보고되는 visual report plot 외 다른 plot 4가지가 추가적으로 보고됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 코드 실행 후 docker image 및 container 리스트를 확인하면 실행된 MRIQC 관련 항목을 볼 수 있습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/docker_ex.png?raw=true&quot; alt=&quot;docker_ex&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mriqc-results&quot;&gt;MRIQC Results&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/mriqc_ex1_1.png?raw=true&quot; alt=&quot;mriqc_ex1_1&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;MRIQC 분석이 완료되면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt; 하에 위와 같은 파일들이 생깁니다. 이 중 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;figures&lt;/code&gt; 폴더 내 plot 이미지 파일 및 파일명을 이름으로 가지는 JSON과 HTML 파일, 위 예시에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-0_ses-0_T1w.json&lt;/code&gt; 과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-0_ses-0_T1w.html&lt;/code&gt;에 분석 결과가 담깁니다. Plot 이미지들과 JSON 파일을 기반으로 하여 HTML 파일로 결과 report가 작성됩니다.&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex1.png?raw=true&quot; alt=&quot;ex1&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex2.png?raw=true&quot; alt=&quot;ex2&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex3.png?raw=true&quot; alt=&quot;ex3&quot; style=&quot;width: 32%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;해당 &lt;a href=&quot;&quot;&gt;HTML 파일을 열면&lt;/a&gt; 위와 같은 report를 볼 수 있습니다. Visualized plot과 quality metric score를 종합하여 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/&quot;&gt;report를 해석&lt;/a&gt;하면 이미지의 quality를 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tomaroberts/nii2dcm&quot;&gt;nii2dcm Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://heudiconv.readthedocs.io/en/latest/&quot;&gt;Heudiconv’s Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mriqc.readthedocs.io/en/latest/&quot;&gt;MRIQC’s Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 2] Brain Imaging Data Structure (BIDS)</title>
   <link href="https://alatteaday.github.io/ko/study/2024/05/20/bids/"/>
   <updated>2024-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/20/bids</id>
   <content type="html">&lt;h1 id=&quot;brain-imaging-data-structure-bids&quot;&gt;Brain Imaging Data Structure (BIDS)&lt;/h1&gt;

&lt;p&gt;Brain Imaging Data Structure (BIDS)는 neuroimaging 데이터를 조직하고 공유하는 과정을 간소화하기 위해 만들어졌습니다. 연구자들이 각자의 스타일로 조작한 데이터를 공유하여 다른 연구자들이 사용하게 되면, 데이터를 재정리하는 데에 비효율적인 시간과 자원이 듭니다. BIDS는 표준화된 데이터 형식의 필요성에 따라 오해를 방지하고 불필요하게 소요되는 시간을 줄이며 재현성을 향상시키기는 것을 목적으로 합니다. BIDS는 데이터의 구조를 간단하고 직관적으로 제공하여 협력을 돕고 연구 속도를 높이며, 다양한 과학자들이 neuroimaging 데이터를 더 쉽게 접근할 수 있도록 합니다.&lt;/p&gt;

&lt;p&gt;BIDS는 파일을 포맷하고 이름을 지정하는 방법에 대한 상세한 지침을 제공하여 연구 간의 일관성을 보장합니다. BIDS는 MRI, MEG, EEG, iEEG 등 다양한 영상 형식을 지원하고, 새로운 데이터 유형과 메타데이터를 통합할 수 있게 확장할 수 있습니다. 또한 데이터를 검증, 분석 및 공유하는 연구 생테계 추세에 따라 BIDS는 연구 커뮤니티에서 유용성을 더욱 높이고 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;bids-format&quot;&gt;BIDS Format&lt;/h1&gt;

&lt;p&gt;BIDS는 OpenfMRI repository에서 사용된 형식을 따서 만들어 졌고, 현재는 OpenNeuro로 알려져 있습니다. BIDS format은 본질적으로 계층적 폴더 구조 내에서 데이터와 메타데이터를 조직하는 방법입니다. 데이터를 조작하는 데 최소한의 도구만을 요함으로써 유연하고 광범위한 호환성을 갖습니다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;folders&quot;&gt;Folders&lt;/h2&gt;

&lt;p&gt;폴더 계층 구조는 네 단계로 구성되어 있으며, 루트 폴더를 제외한 모든 하위 폴더는 특정한 형식의 이름으로 저장됩니다. 예시 폴더 구조와 이름은 다음과 같습니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Project/
└─ Subject (e.g. &apos;sub-01/&apos;)
  └─ Session (e.g. &apos;ses-01/&apos;)
    └─ Datatype (e.g. &apos;anat&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Project: dataset을 포함하는 폴더로, 어떤 이름으로도 저장될 수 있습니다.&lt;/li&gt;
  &lt;li&gt;Subject: 한 개체(subject)의 데이터를 포함합니다. 한 subject는 고유한 이름을 갖습니다.
    &lt;ul&gt;
      &lt;li&gt;Name format: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-PARTICIPANT LABEL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Session: 한 subject에 대해 촬영된 영상을 구분하는 폴더입니다. 각 subject는 여러 상황에서 촬영된 데이터가 있는 경우 여러 session을 가질 수 있습니다.
    &lt;ul&gt;
      &lt;li&gt;만약 session이 subject 당 하나만 존재하는 경우 session 단계는 생략될 수 있습니다.&lt;/li&gt;
      &lt;li&gt;Name format: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ses-SESSION LABEL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Datatype: 데이터의 타입을 명시합니다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anat&lt;/code&gt;: anatomical MRI data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;func&lt;/code&gt;: functional MRI data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fmap&lt;/code&gt;: fieldmap data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dwi&lt;/code&gt;: diffusion MRI data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt;: arterial spin labeling data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eeg&lt;/code&gt;: electroencephalography data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;meg&lt;/code&gt;: magnetoencephalography data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ieeg&lt;/code&gt;: intracranial EEG data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;beh&lt;/code&gt;: behavioral data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pet&lt;/code&gt;: positron emission tomography data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micr&lt;/code&gt;: microscopy data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nirs&lt;/code&gt;: near-infrared spectroscopy data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;motion&lt;/code&gt;: motion capture data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;files&quot;&gt;Files&lt;/h2&gt;

&lt;p&gt;파일의 유형으로는 기본적으로 세 가지가 있습니다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.json&lt;/code&gt; file: 메타데이터 관련 내용&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tsv&lt;/code&gt; file: 메타데이터 관련 내용 (특히 테이블 등)&lt;/li&gt;
  &lt;li&gt;Raw data files: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.jpg&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.nii.gz&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.dcm&lt;/code&gt; 등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;파일 이름은 아래의 기준에 따라 표준화됩니다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이름에 공백을 포함하지 않습니다.&lt;/li&gt;
  &lt;li&gt;문자, 숫자, -, _만 사용 가능합니다.&lt;/li&gt;
  &lt;li&gt;대소문자를 구분하지 않습니다: 운영체제에 따라 대소문자 구분이 되지 않을 수 있으므로&lt;/li&gt;
  &lt;li&gt;이름의 형식을 통일하여 체계적으로 이름을 짓습니다.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CamelCase&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snake_case&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;파일 이름 템플릿&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig3.png?raw=true&quot; alt=&quot;fig3&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;최종적으로 BIDS format 예시는 아래와 같습니다:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dataset/
 └─ participants.json
 └─ participants.tsv
 └─ sub-01/
   └─ anat/
     └─ sub-01_t1w.nii.gz
     └─ sub-01_t1w.json
   └─ func/
     └─ sub-01_task-rest_bold.nii.gz
     └─ sub-01_task-rest_bold.json
   └─ dwi/
     └─ sub-01-task-rest_dwi.nii.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://bids.neuroimaging.io/&quot;&gt;BIDS Official Website&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bids-standard.github.io/bids-starter-kit/index.html&quot;&gt;BIDS Starter Kit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bids-standard&quot;&gt;BIDS Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 1] MRIQC: Magnetic Resonance Imaging Quality Control</title>
   <link href="https://alatteaday.github.io/ko/study/2024/05/19/mriqc/"/>
   <updated>2024-05-19T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/19/mriqc</id>
   <content type="html">&lt;p&gt;MRI 영상을 대상으로 하는 연구의 진행과 퀄리티를 높이기 위해서는 영상 데이터의 상태를 체크하고 좋은 데이터를 확보해야 합니다. 그런데 MRI 품질 평가는 여러가지 요인으로 인해 난이도가 높습니다. MRI 촬영 시 발생할 수 있는 결함(artifact)의 종류가 많고, 사람마다 영상 품질에 대해 달리 평가하며, 일부 결함 사항은 사람이 인지하기 어렵기도 합니다. 이런 상황에서 객관적인 MRI 품질 관리(quality control; QC) 시스템은 MRI 품질 평가 초기에 도움이 될 수 있습니다. 또한 여러 스캔 사이트에서 매우 큰 영상 데이터 샘플을 획득하려 하는 최근의 추세에 따라 완전 자동화되고, 편향이 최소화된 QC 프로토콜이 점점 필요해지고 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;magnetic-resonance-imaging-quality-control-mriqc&quot;&gt;Magnetic Resonance Imaging Quality Control (MRIQC)&lt;/h1&gt;

&lt;p&gt;MRI 품질 평가 자동화 도구로 MRIQC (Magnetic Resonance Imaging Quality Control)를 사용할 수 있습니다. MRIQC는 구조적(anatomical) 및 기능적(functional) MRI 이미지의 품질을 평가하기 위해 설계된 오픈 소스 도구입니다. MRIQC는 별도의 참조 이미지를 사용하지 않고, 입력된 이미지 자체만으로 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/&quot;&gt;품질 지표(image quality metrics; IQMs)&lt;/a&gt;를 추출합니다. 더불어 다양한 출처나 세션에서 MRI 스캔을 평가하고 비교할 수 있는 표준화된 방법을 제공합니다.&lt;/p&gt;

&lt;h1 id=&quot;priciples&quot;&gt;Priciples&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Modular and Integrable&lt;/strong&gt;: Nipype 프레임워크를 기반으로 한 모듈형 workflow를 사용하여 ANTs나 AFNI와 같은 다양한 서드파티 소프트웨어 도구를 통합합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimal Preprocessing&lt;/strong&gt;: 전처리를 최소화하여 이미지의 원본 상태 혹은 원본에 가까운 상태에서 IQM을 추정하고, IQM이 가능한 원본 데이터를 정확하게 반영할 수 있도록 합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability and Standards&lt;/strong&gt;: &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/20/bids/&quot;&gt;Brain Imaging Data Structure (BIDS)&lt;/a&gt; 표준을 준수하여 상호운용성을 촉진하고, 다양한 neuroimaging workflow 통합을 용이하게 합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reliability and Robustness&lt;/strong&gt;: 다양한 데이터와 파라미터에 대해 일관된 성능을 보장할 수 있게 테스트됩니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual Report&lt;/strong&gt;: 각 이미지와 이미지 그룹에 대한 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/&quot;&gt;visual report&lt;/a&gt;를 제공합니다. Report는 각 이미지에 대한 모자이크 뷰(mosaic view) 및 segmentation contour, 그룹에 대한 scatter plot을 제공해 이상치를 식별할 수 있게 합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;image-quality-metrics-iqms&quot;&gt;Image Quality Metrics (IQMs)&lt;/h1&gt;

&lt;p&gt;MRIQC는 크게 네 분류의 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/&quot;&gt;IQM&lt;/a&gt;을 계산합니다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Noise-related metrics&lt;/strong&gt;: 이미지 내 노이즈의 영향과 특성을 평가합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Information theory-based metrics&lt;/strong&gt;: 지정된 마스크를 사용하여 정보의 공간 분포(spatial distribution)를 평가합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Artifact detection metrics&lt;/strong&gt;: 불균일성(inhomogeneity)과 움직임에 의한 신호 누출(signal leakage)과 같은 특정 결함를 식별하고 그 영향을 측정합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical and morphological metrics&lt;/strong&gt;: 조직(tissue) 분포의 통계적 특성과 이미지의 선명도/흐림(sharpness/blurriness) 정도를 특정합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;paper&quot;&gt;Paper&lt;/h1&gt;

&lt;p&gt;Esteban O, Birman D, Schaer M, Koyejo OO, Poldrack RA, Gorgolewski KJ (2017) MRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites. PLoS ONE 12(9): e0184661. https://doi.org/10.1371/journal.pone.0184661&lt;/p&gt;

&lt;h1 id=&quot;how-to-run-mriqc&quot;&gt;How to run MRIQC&lt;/h1&gt;

&lt;p&gt;MRIQC를 실행하기 위해서는 몇 개의 스텝을 거쳐야 합니다. 자세한 실행 과정은 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/20/mriqc_run/&quot;&gt;다음 포스트&lt;/a&gt;를 참고해주세요!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mriqc.readthedocs.io/en/latest/&quot;&gt;MRIQC’s Official Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184661&quot;&gt;Paper Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] Amyloid-β prediction machine learning model using source-based morphometry across neurocognitive disorders (2024)</title>
   <link href="https://alatteaday.github.io/ko/papers/2024/04/18/keioAbMRI/"/>
   <updated>2024-04-18T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2024/04/18/keioAbMRI</id>
   <content type="html">&lt;p&gt;Momota, Yuki, et al. “Amyloid-β prediction machine learning model using source-based morphometry across neurocognitive disorders.” &lt;em&gt;Scientific Reports&lt;/em&gt; 14.1 (2024): 7633.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41598-024-58223-3&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다양한 환자의 MRI를 기반으로 하는 machine leanring 모델을 사용해 Alzheimer’s disease (AD)를 예측하고자 한다.&lt;/li&gt;
  &lt;li&gt;Amyloid-beta (A$\beta$) 침착의 정도를 측정하기 위해 source-based morphometry (SBM)을 활용한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3D T1 weighted-image (WI)를 voxel-based 회백질 (gray matter; GM) 이미지로 전처리한 뒤 SBM에 적용했다.&lt;/li&gt;
  &lt;li&gt;Classifier로서 support vector machine (SVM)을 사용했다.&lt;/li&gt;
  &lt;li&gt;모델의 interpretability를 위해 SHapley Aditive exPlanations (SHAP)를 활용했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MR 이미지, 인지 검사 결과 및 apolipoprotein E (APOE)를 input feature로 사용한 최종 모델의 정확도가 89.8%를 달성했다.&lt;/li&gt;
  &lt;li&gt;MR 이미지만을 기반으로 한 모델의 경우 84.7%이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;AD는 A$\beta$ 플라크, 신경 섬유 매듭(neurofibrillary tangles), 뇌 위축(brain atrophy) 등으로 특정되는 신경퇴행성 질환이다.&lt;/li&gt;
  &lt;li&gt;A$\beta$는 AD를 정의하는 특징 중 하나이지만 임상 실무에서 실질적으로 감지하기 어렵다.
    &lt;ul&gt;
      &lt;li&gt;Position emission tomography (PET), cerebrospinal fluid (CSF) 검사, 혈액 바이오 마커 등의 방법은 아직 실무에 적용되지 못했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MRI 기반 A$\beta$ 예측은 위의 방법을 통한 정확한 진단 이전에 유용한 진단 도구로서 사용될 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/subfig1.png?raw=true&quot; alt=&quot;supfig1&quot; style=&quot;zoom: 90%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Participants and clinical measurements&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2018년 6월 ~ 2021년 8월, Keio 대학 병원의 memory clinic에서 모집되었다.&lt;/li&gt;
  &lt;li&gt;진단명: AD, MCI, HC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cognitive assessment&lt;/strong&gt; (9 measures)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;인지 기능 전반: Mimi-mental state examination (MMSE), Clinical dementia rating (CDR), Functional activity questionnaire (FAQ)&lt;/li&gt;
  &lt;li&gt;기억력: Wechsler Memory Scale-Revised (WMS-R) Lgical Memeory immediate recall (LM I) and delayed recall (LM II)&lt;/li&gt;
  &lt;li&gt;실행력 및 주의력: Word Fluency, Trail Making Test (TMT)&lt;/li&gt;
  &lt;li&gt;특정 인지 능력:  Japanese version of Alzheimer’s Disease Assessment Scale-Cognitive subscale (ADAS-cog-J), Japanese Adult Reading Test (JART)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;APOE genotyping&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Magnetic nanoparticle DNA extraction kit (EX1 DNA Blodd 200 $\mu$L Kit)&lt;/li&gt;
  &lt;li&gt;real-time polymerase chain reaction (PCR)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;[&lt;sup&gt;18&lt;/sup&gt;F] Florbetaben (FBB) amyloid-PET imaging&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[&lt;sup&gt;18&lt;/sup&gt;F] Florbetaben (FBB)
    &lt;blockquote&gt;
      &lt;p&gt;Florbetaben은 일반 임상에서 사용할 목적으로 개발된 진단 방사성 트레이서로, 아밀로이드 베타 플라크를 시각화하기 위해 만들어졌다.  [&lt;a href=&quot;https://en.wikipedia.org/wiki/Florbetaben_(18F)&quot;&gt;reference&lt;/a&gt;]&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mri&quot;&gt;MRI&lt;/h2&gt;

&lt;h3 id=&quot;acquisition---3d-t1-weighted-mr-이미지-t1-wi&quot;&gt;Acquisition - 3D T1 weighted MR 이미지 (T1 WI)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;MRI 스캐너: Discovery MR750 3.0 T scanner (GE Healthcare)&lt;/li&gt;
  &lt;li&gt;Coil: 32-channel head coil&lt;/li&gt;
  &lt;li&gt;Imaging parameters: field of view (FOV) 230mm, matrix size 256$\times$256, slice thickness 1.0mm, voxel size 0.9$\times$0.9$\times$1.0 mm&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pre-processing&quot;&gt;Pre-processing&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;: MR 이미지를 조직 유형(GM, white matter (WH), CSF)에 따라 segmentation한다. (Statistical Parametric Mapping toolbox CAT12 사용)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nomarlization&lt;/strong&gt;: segmented GM 이미지를 Montreal Neurological Institute (MNI) 템플릿에 맞춰 normalize한다.
    &lt;ul&gt;
      &lt;li&gt;Montreal Neurological Institute (MNI) Template: 신경 영상 연구에서 일반적으로 사용되는 뇌 표준판.
        &lt;blockquote&gt;
          &lt;p&gt;Standard anatomical templates are widely used in human neuroimaging processing pipelines to facilitate group level analyses and comparisons across different subjects and populations. The MNI-ICBM152 template is the most commonly used standard template, representing an average of 152 healthy young adult brains.  [&lt;a href=&quot;https://nist.mni.mcgill.ca/mni-ftd-templates/&quot;&gt;reference&lt;/a&gt;]&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resampling and Smoothing&lt;/strong&gt;: 이미지를 isotropic voxel size 2$\times$2$\times$2 mm&lt;sup&gt;3&lt;/sup&gt; 로 resampling한 후,  5mm full-width-at-half-maximum Gaussian kernel을 사용해 smoothing한다.
    &lt;ul&gt;
      &lt;li&gt;이미지 사이즈를 표준화하고 이미지 내 noise를 줄이는 데에 도움이 될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Source-based morphometry (SBM)&lt;/strong&gt;: 독립 성분 분석 (independent component analysis; ICA)을 통합하여 해부학적 뇌 이미지를 각 개체의 독립적인 spatial map으로 분해한다.
    &lt;blockquote&gt;
      &lt;p&gt;In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.  [&lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_component_analysis&quot;&gt;reference&lt;/a&gt;]&lt;/p&gt;

      &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/ica.png?raw=true&quot; alt=&quot;ica&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ICA processing&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3D GM 이미지 (91$\times$109$\times$91 voxels)를 1D 배열 (1$\times$902,629) 형식으로 변환한다.&lt;/li&gt;
      &lt;li&gt;Scikit-learn의 FastICA를 사용해 ICA에 선택된 voxel에 관한 brain mask를 생성한다.&lt;/li&gt;
      &lt;li&gt;추출된 독립 성분 (IC) 수는 모델링 시 하이퍼파라미터로 작용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Spatial Regression&lt;/strong&gt;: 추출된 IC는 각 GM 이미지의 공간 회귀 변수 (spatial regressor)로 사용되며, 가중 계수 (weighting coefficient) $\beta$는 각 IC의 GM 이미지에 영향을 얼마나 줄지를 결정한다.&lt;/p&gt;

\[I_{GM}=\beta_1 IC_1 + \beta_2 IC_2 + ... + \beta_K IC_K\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;machine-learning&quot;&gt;Machine learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input features: ICA의 $\beta$ 값, demographic characteristics (나이 및 성별), 인지 평가, APOE 유전형&lt;/li&gt;
  &lt;li&gt;Input conduction: 다양한 input feature 조합을 모델 학습 및 테스트 시 사용했다.
    &lt;ol&gt;
      &lt;li&gt;모든 input feature 사용&lt;/li&gt;
      &lt;li&gt;각 feature를 다양하게 조합하여 사용: 뇌 이미지만 사용, 뇌 이미지+인지 평가 사용 등&lt;/li&gt;
      &lt;li&gt;진단명 별 데이터를 다양하게 조합하여 사용: AD+HC, AD+MCI+HC 등&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;모델: Gaussian support vector machine (SVM)
    &lt;ul&gt;
      &lt;li&gt;5-fold cross-validation 방식으로 학습&lt;/li&gt;
      &lt;li&gt;모든 분할에서 테스트&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Interpretability: SHaply Additive exPlanations (SHAP)
    &lt;ul&gt;
      &lt;li&gt;게임 이론에 기초하여 구해지는 SHAP 값은 모델 예측 결과에 해당 feature가 미치는 영향을 나타낸다.&lt;/li&gt;
      &lt;li&gt;SHAP의 절댓값이 큰 feature일수록 예측에 강한 영향을 미친다.&lt;/li&gt;
      &lt;li&gt;양음성을 띠는 SHAP 값이 도출되는 임상적 feature는 A$\beta$의 양음성과 관련이 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;statistical-analysis&quot;&gt;Statistical analysis&lt;/h2&gt;

&lt;p&gt;변수 간 관계성 탐색으로서 진단명과 관련이 있는지, Alzheimer’s disease 관련 기존 연구 가설과 연관이 있는지 판단해보았다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two-tailed t-test / Chi-square test
    &lt;ul&gt;
      &lt;li&gt;Two tailed t-test: 두 그룹의 평균을 비교하여 그들 사이에 유의한 차이가 있는지 결정하는 데 사용된다.&lt;/li&gt;
      &lt;li&gt;Chi-square test: 범주형 변수 간 독립성 (independence)을 테스트하는 데 사용된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;feature 간 관계성: 연속성 변수에 대한 피어슨 상관 분석 (Pearson’s correlation analysis)
    &lt;ul&gt;
      &lt;li&gt;연속성 변수 pair 간 선형 관계 (linear relationship)의 강도와 방향을 측정한다.&lt;/li&gt;
      &lt;li&gt;변수간 관계를 이해하는 데 도움을 준다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;진단명과의 관련성: 분산 분석 (Analysis of variance; ANOVA)
    &lt;ul&gt;
      &lt;li&gt;한 표본 내에서 그룹 간 평균 차이를 분석한다.&lt;/li&gt;
      &lt;li&gt;그룹 평균 사이 통계적으로 유의미한 차이가 있는지 결정하므로, 비교할 그룹이 두 개 이상인 경우 특히 유용하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;최종 모델 구축에 118개 데이터가 사용되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table1.png?raw=true&quot; alt=&quot;table1&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-performance&quot;&gt;Model performance&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table2.png?raw=true&quot; alt=&quot;table2&quot; style=&quot;zoom: 80%;&quot; /&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 80%;&quot; /&gt; 
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table3.png?raw=true&quot; alt=&quot;table3&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A$\beta$ positivity prediction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;최종 모델: 뇌 이미지 + 인지 기능 점수 + APOE를 input feature로 사용한 모델&lt;/li&gt;
  &lt;li&gt;최종 모델로 최고 성능 (accuracy 89.8%, AUC 0.888)을 달성했다.&lt;/li&gt;
  &lt;li&gt;뇌 이미지만 input feature로 사용한 모델이 최저 성능 (accuracy 84.7%, AUC 0.830)을 기록했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;최종 모델로 각 진단명의 데이터에 대해 A$\beta$ positivity prediction을 시험한 결과&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;모든 진단명 데이터를 사용한 경우 최고 성능 (accuracy 89.8%)을 얻었다.&lt;/li&gt;
  &lt;li&gt;MCI 데이터만을 가지고 테스트한 경우에 최저 성능 (accuracy 75.9%)을 기록했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sbm&quot;&gt;SBM&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table4.png?raw=true&quot; alt=&quot;table4&quot; style=&quot;zoom: 100%;&quot; /&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/addfig2.png?raw=true&quot; alt=&quot;addfig2&quot; style=&quot;zoom: 100%;&quot; /&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;최종 SBM 모델에서 7개의 IC를 추출했다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각 component는 공간적으로 maximally independent GM volume 패턴을 나타낸다.&lt;/li&gt;
  &lt;li&gt;IC 1이 인지 검사 결과 및 A$\beta$ 양음성과 유의한 상관 관계를 보였다.&lt;/li&gt;
  &lt;li&gt;진단명 중에서는 AD와 IC 1만이 유의한 관련이 있었고, 다른 진단명은 어떤 IC와도 관련이 없었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;제안한 모델은 A$\beta$ positivity를 성공적으로 예측했다 (성능: accuracy 89.8%, AUC 0.888).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;여러 feature로 구성된 118개의 데이터만을 가지고 좋은 결과를 내었다.&lt;/li&gt;
  &lt;li&gt;비 Alzheimer’s disease (non-AD) 개체도 정확히 구분했다: FTLD 신드롬이나 다른 정신 질환 등&lt;/li&gt;
  &lt;li&gt;최종 모델의 공분산 (convariant) 중 IC 1이 A$\beta$ positivity prediction에 강한 영향을 미쳤다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Non-AD 개체가 갖는 feature의 다양성(heterogeneity)
    &lt;ul&gt;
      &lt;li&gt;AD 개체만을 기반으로 학습된 모델이 모든 경우에 대해 학습한 모델보다 성능이 조금 낮았다. (88.4%)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SBM의 장점
    &lt;ul&gt;
      &lt;li&gt;다양한 임상 인구를 기반으로 한 모델은 실제 임상 환경에서 적용되기에 더 적합할 것이다. (← 진료를 받으러 오는 환자들은 AD 외 다양한 인지 장애를 가지고 있을 것이다.)&lt;/li&gt;
      &lt;li&gt;뇌 이미지만을 사용하여 학습된 모델 (accuracy 84.7%)은 AD 관련 임상 시험에서 잠재적 환자를 선별하는 데 도움이 될 수 있을 것이다.&lt;/li&gt;
      &lt;li&gt;SBM은 기존의 아틀라스(atlas)에 의존하지 않고 ND 질환과 관련된 뇌 구조의 미묘한 형태학적 변화 및 알려지지 않은 패턴을 감지한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;봐줄만 한 MCI 환자 예측 성능
    &lt;ul&gt;
      &lt;li&gt;의사가 AD 환자를 70% 정확하게 진단하는데, 모델은 MCI 데이터만을 가지고 이것을 초과한 정확도 (75.9%)를 보였다.&lt;/li&gt;
      &lt;li&gt;다른 MRI 기반 모델의 MCI 개체 대상 예측 정확도와도 견줄만하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;feature-importance-of-the-model---shap&quot;&gt;Feature Importance of the model - SHAP&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/fig3.png?raw=true&quot; alt=&quot;fig3&quot; style=&quot;zoom: 80%;&quot; /&gt;
     &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/supfig3.png?raw=true&quot; alt=&quot;supfig3&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;모든 IC가 인구 통계 및 MMSE 등과 같은 인지적 특성보다 모델 예측에 더 중요하게 작용하는 것으로 나타났다. 모델에 제일 중요하게 작용한 feature 세 가지는 다음과 같다: IC 1, LM 1, LM II&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IC 1: A$\beta$ 양음성 및 인지 검사 결과와 유의한 상관 관계를 보였다.
    &lt;ul&gt;
      &lt;li&gt;IC 1의 공간적 패턴이 측두엽(parietal lobe)에서 관찰되는 AD의 신경 퇴행(neurodegeneration; ND) 피질 패턴(cortical pattern)과 유사했다.&lt;/li&gt;
      &lt;li&gt;전형적인 AD 양상인 내측두엽(medial temporal lobe; MTL) 위축이 어떤 IC에서도 관찰되지 않았다. 이것은 A$\beta$ 병변(pathodology)이 아닌 Tau pathodology를 가리킬 수도 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LM scores: AD의 주요 증상인 기억 장애를 반영한다.&lt;/li&gt;
  &lt;li&gt;APOE -$\epsilon$4의 유무도 중요한 요소로 나타났다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또한 IC 1과는 A$\beta$ 양음성이, IC 4와는 나이가 명확하게 관련되는 것으로 나타났다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이것은 모델이 뇌 이미징에서 AD로 인한 ND와 정상적인 노화를 구별하는 능력이 있다는 것을 나타낸다.&lt;/li&gt;
  &lt;li&gt;즉, AD의 pathdology 과정은 나이와 절대적으로 관련이 있지는 않을 수 있음을 시사한다. → 정상적인 노화 과정에서 관찰되는 뇌 손상 패턴은 신경퇴행성 질환의 뇌 손상과 구별될 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitation&quot;&gt;Limitation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;PET 검사로만 결정된 A$\beta$ 양음성 여부: 임상 전 단계에서는 CSF A$\beta$로 판단하는 것이 더 정확할 수 있다.&lt;/li&gt;
  &lt;li&gt;부족한 샘플 수: 모델의 정확도에 영향을 줄 수 있다.&lt;/li&gt;
  &lt;li&gt;Cross-sectional 접근: 이보다는 Longitudinal follow-up 데이터가 모델 성능을 더 향상시킬 수도 있다.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Tabtransformer: Tabular data modeling using contextual embeddings (2020)</title>
   <link href="https://alatteaday.github.io/ko/papers/2024/04/11/tabtf/"/>
   <updated>2024-04-11T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2024/04/11/tabtf</id>
   <content type="html">&lt;p&gt;Huang, Xin, et al. “Tabtransformer: Tabular data modeling using contextual embeddings.” &lt;em&gt;arXiv preprint arXiv:2012.06678&lt;/em&gt; (2020).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.06678&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TabTransformer&lt;/strong&gt;: contextual embedding을 활용한 novel tabular data 모델&lt;/li&gt;
  &lt;li&gt;Two-phase pre-training 방법을 통해 질 좋은 feature representation을 추출한다.&lt;/li&gt;
  &lt;li&gt;Supervised 및 semi-supervised learning에서 모두 SOTA를 달성했다.&lt;/li&gt;
  &lt;li&gt;데이터가 누락되거나 일관되지 않은(noisy) 데이터에서도 성능이 안정적이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;기존 tabular data에 대한 모델은 주로 트리 기반 앙상블 방식으로, Gradient boosted decision trees (GBDT)모델이 대표적이다. 그러나 이런 모델은 딥러닝 모델과 비교하여 여러 제한을 가지고 있다:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;스트리밍 데이터를 통한 continual learning에 적합하지 않다.&lt;/li&gt;
  &lt;li&gt;tabular data의 이미지나 텍스트 등 multi-modality를 end-to-end로 학습하는 데 효과적이지 않다.&lt;/li&gt;
  &lt;li&gt;Semi-supervised learning에 적합하지 않다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;한편 Multi-layer perceptron (MLP)은 이미지와 텍스트 인코더를 end-to-end로 학습하는 것을 가능하게 하지만, 이것 역시 단점이 있다:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;해석하기가(interpretability) 어렵다.&lt;/li&gt;
  &lt;li&gt;누락되거나 지저분한 데이터에 대해 취약하다.&lt;/li&gt;
  &lt;li&gt;Semi-supervised learning의 상황에서 성능이 제한된다.&lt;/li&gt;
  &lt;li&gt;성능이 트리 기반 모델보다 떨어진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/archi.png?raw=true&quot; alt=&quot;archi&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transformer layer는 categorical input만을 입력으로 받는다.&lt;/li&gt;
  &lt;li&gt;Continuous input은 Transformer의 출력값과 concatenate된다.&lt;/li&gt;
  &lt;li&gt;Pre-training 동안 Transformer layer는 unlabeled data에 대해 두 가지 task를 학습한다.
    &lt;ul&gt;
      &lt;li&gt;Pre-training에서는 continuous input을 배제하고 categorical input만 활용된다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code1.png?raw=true&quot; alt=&quot;code1&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pre-trained model은 labeled data를 가지고 MLP head와 같이 target 예측 task에 fine-tuning된다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continuous value는 fine-tuning 단계에서 categorical value와 concat되어 사용된다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code2.png?raw=true&quot; alt=&quot;code2&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;각 입력값 $x\equiv \lbrace x_{cat}, x_{cont}\rbrace$은 해당하는 라벨 $y$를 갖는다: $(x, y)$.&lt;/li&gt;
  &lt;li&gt;$x_{cat} \equiv \lbrace x_1, x_2, …, x_m\rbrace$는 입력값 $x_i (i \in {1, …, m})$가 categorical value인 경우의 feature를 가리킨다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$x_{cat}$은 $E_\phi$로 임베딩된다 (column embedding):&lt;/p&gt;

\[E_\phi(x_{cat}) \equiv \lbrace e_{\phi_1}(x_1), ..., e_{\phi_m}(x_m) \rbrace, \ e_{\phi_i}(x_i) \in \mathbb{R}^d\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 임베딩이 여러 Transformer layer를 통과한다 (contextual embedding):&lt;/p&gt;

\[\{h_1, ..., h_m\}=f_\theta(E_\phi(x_{cat})), \ h\in \mathbb{R}^d\]
  &lt;/li&gt;
  &lt;li&gt;$x_{cat}$의 contextual embedding은 $x_{cont} \in \mathbb{R}^c $와 concat된다 ($(d\times m+c)$ 차원).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Column Embedding&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/colemb.png?raw=true&quot; alt=&quot;colemb&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Categorical feature $x_i$는 각자의 embedding lookup table $e_{\phi_i}(.)$을 갖는다.&lt;/li&gt;
  &lt;li&gt;$d_i$개 클래스를 갖는 $i$th feature에 대해, embedding table $e_{\phi_i}(.)$은 $(d_1+1)$개의 embedding으로 구성되는데, 여기서 $d_1+1$번째 embedding은 누락된(masking된) 값을 표현하기 위해서 추가되었다.&lt;/li&gt;
  &lt;li&gt;각 embedding $e_{\phi_i}(j)$은 $[c_{\phi_i}, w_{\phi_{ij}}]$로 표현되는데,
    &lt;ul&gt;
      &lt;li&gt;$c_{\phi_i}$는 column $i$에 속하는 클래스를 다른 column 내 클래스와 구분하는 역할을 한다.&lt;/li&gt;
      &lt;li&gt;$w_{\phi_{ij}}$는 column $i$에 속하는 한 feature $j$의 클래스를 해당 column 내 다른 클래스들과 구분하는 역할을 한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;*차원 $d$는 코드 상으로 볼 때 hidden size인 $h$와 같은 것으로 보인다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code3.png?raw=true&quot; alt=&quot;code3&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h2&gt;

&lt;p&gt;Transformer layer는 categorical value $x_{cat}=\lbrace x_1, x_2, …, x_m\rbrace$로 구성된 입력을 가지고 두 가지 task를 수행하며 pre-training된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Masked language modeling (MLM)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;입력값 중 $k\%$의 feature를 랜덤으로 masking한다. 실험에서는 $k$를 30으로 설정했다.&lt;/li&gt;
      &lt;li&gt;masking된 feature의 값을 예측하는 multi-class classifier의 cross-entropy loss를 구하여 최소화하는 방향으로 학습한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Replaced token detection (RTD)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;입력값 중 일부의 feature를 랜덤하게 생성된 다른 값으로 바꾼다.&lt;/li&gt;
      &lt;li&gt;해당 feature가 바뀌었는지 아닌지를 예측하는 binary classifier의 loss를 최소화하는 방향으로 학습한다.&lt;/li&gt;
      &lt;li&gt;각 column은 embedding lookup table을 따로 가지므로, binary classifier 또한 각 column에 대해 따로 구현되었다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;settings&quot;&gt;Settings&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모든 모들은 15가지 public binary classification 데이터셋에 대해 평가되었다. 데이터셋 출처는 UCI repository, AutoML Challenge, Kaggle.&lt;/li&gt;
  &lt;li&gt;모든 데이터셋은 cross-validation을 위해 5개로 나뉘었다.&lt;/li&gt;
  &lt;li&gt;Training: Validation: Testing 비율 = 65:15:20 (%)&lt;/li&gt;
  &lt;li&gt;Categorical feature는 데이터셋마다 2에서 136가지로 분류된다.&lt;/li&gt;
  &lt;li&gt;Semi-supervised 및 supervised 실험 관련
    &lt;ul&gt;
      &lt;li&gt;Semi-supervised: $p$개의 labeled data와 unlabeled data로 학습 데이터를 구성하였다. $p$는 실험 세팅에 따라 $(50, 200, 500)$ 중 하나로 설정되었다.&lt;/li&gt;
      &lt;li&gt;Supervised: 모든 데이터가 labeled data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hidden dimension: 32&lt;/li&gt;
  &lt;li&gt;Transformer layer 수: 6&lt;/li&gt;
  &lt;li&gt;Attention head 수: 8&lt;/li&gt;
  &lt;li&gt;MLP layer 구조: $\lbrace 4\times l, \ 2\times l \rbrace$ ($l$은 입력의 size를 나타낸다).&lt;/li&gt;
  &lt;li&gt;매 cross-validation split마다 hyperparamter optimization (HPO)를 20번 수행했다.&lt;/li&gt;
  &lt;li&gt;Pre-training은 semi-supervised learning의 경우에만 적용되었다.
    &lt;ul&gt;
      &lt;li&gt;모든 데이터가 라벨이 있는 경우(labeled data)에는 pre-training 유무의 차이를 크게 찾지 못했다.&lt;/li&gt;
      &lt;li&gt;Unlabeled data 개수가 많고, labeled data가 적은 학습 상황에서 pre-training의 효과를 더 명확히 발견하였다: 모델이 pre-training을 통해 labeled data에서만으로는 배울 수 없는 representation을 형성할 수 있게 되는 것으로 보인다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Baseline model&lt;/strong&gt;: MLP 모델&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TabTransformer에서 Transformer layer를 제거한 상태의 모델&lt;/li&gt;
  &lt;li&gt;Transformer layer의 효과를 평가하기 위해 baseline으로 설정하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-effectiveness-of-the-transformer-layers&quot;&gt;The effectiveness of the Transformer Layers&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance comparison&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Supervised learning의 상황에서 TabTransformer와 MLP를 비교하였다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table1.png?raw=true&quot; alt=&quot;table1&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;TabTransformer가 14개의 dataset에서 AUC 상 평균적으로 1.0% 정도로 MLP보다 더 좋은 성능을 보였다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;t-SNE visualization of contextual embeddings&lt;/strong&gt;&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;각 점은 특정 클래스에 속하는 테스트 데이터의 2차원 좌표값을 평균내어 표시하였다.&lt;/li&gt;
      &lt;li&gt;마지막 Transformer layer의 t-SNE plot (왼쪽)에서, 의미가 비슷한 클래스끼리 embedding space 상 cluster를 형성하며 가까이 모여있는 것을 볼 수 있다.&lt;/li&gt;
      &lt;li&gt;Transformer layer를 통과하기 전 (중간)에도, 성격이 다른 feature의 embedding 간에 구별이 시작되는 것을 볼 수 있다.&lt;/li&gt;
      &lt;li&gt;MLP의 embedding (오른쪽)의 경우 어떤 뚜렷한 경향성을 보지 못했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Prediction performance of linear models using the embeddings from different Transformer layers&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Logistic regression 모델을 사용해 학습된 embedding의 퀄리티를 평가하였다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig3.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;각 모델은 embedding과 continuous value를 사용하여 $y$를 예측한다.&lt;/li&gt;
      &lt;li&gt;Metrics: Test data를 가지고 평가했을 때, AUC 내 cross-validation 점수&lt;/li&gt;
      &lt;li&gt;Normalization: 각 예측 점수는 TabTransformer를 해당 데이터에 학습했을 때 제일 잘 나온 점수에 대해서 normalization되었다.&lt;/li&gt;
      &lt;li&gt;Features: embedding은 concatenation 대신 평균한 후 maximum pooling하는 것으로 처리되었다.&lt;/li&gt;
      &lt;li&gt;Findings: Transformer layer가 깊어질 수록 embedding의 효과가 커지는 것으로 보인다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-robustness-of-tabtransformer&quot;&gt;The robustness of TabTransformer&lt;/h2&gt;

&lt;p&gt;데이터가 noisy한 경우와 누락된 경우에 대해 TabTransformer의 성능 안정성을 평가하였다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig4_5.png?raw=true&quot; alt=&quot;fig4_5&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Noisy data&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Method: 데이터에 noise를 만들기 위해 특정 값을 해당 columns 내에 존재하는 값 중 랜덤한 값으로 교체한다. 이 데이터로 이미 학습된 모델을 평가한다.&lt;/li&gt;
      &lt;li&gt;Findings: 데이터가 noisy할 수록 TabTransformer가 MLP보다 확실히 더 좋은 성능을 보이는 것을 관찰할 수 있다 (fig. 4).&lt;/li&gt;
      &lt;li&gt;TabTransformer embedding의 contextual한 성질이 noisy한 데이터에서 큰 효과를 나타내는 것으로 여겨진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data with missing value&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Method: 일부 값을 일부러 삭제하여 데이터를 조작한 후, 미리 학습된 모델을 평가한다.
        &lt;ul&gt;
          &lt;li&gt;학습된 모델의 embedding 중 특정 column의 모든 클래스의 embedding 평균값으로 누락된 값을 처리했다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Findings: TabTransformer가 값이 누락된 데이터에서도 MLP보다 더 안정적인 성능을 보였다 (fig. 5).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;supervised-learning&quot;&gt;Supervised learning&lt;/h2&gt;

&lt;p&gt;Supervised learning의 상황에서 TabTransformer의 성능을 4가지 카테고리의 모델과 비교했다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Logistic Regression and GBDT&lt;/li&gt;
  &lt;li&gt;MLP and sparse MLP&lt;/li&gt;
  &lt;li&gt;TabNet model&lt;/li&gt;
  &lt;li&gt;Variational Information Bottleneck (VIB) model&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table2.png?raw=true&quot; alt=&quot;table2&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Findings:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TabTransformer가 성능이 제일 좋은 GBDT와 견줄만한 성능을 보였다.&lt;/li&gt;
  &lt;li&gt;한편 TabNet과 VIB와 같이 tabular data에 대해 고안된 최신 deep learning 모델보다 확실히 좋은 성능을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-supervised learning&lt;/h2&gt;

&lt;p&gt;Semi-supervised learning의 상황에서는 TabTransformer의 성능을 다음 모델과 비교했다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Entropy Regularization (ER)&lt;/li&gt;
  &lt;li&gt;Pseudo Labeling (PL) combined with MLP, TabTransformer, and GBDT&lt;/li&gt;
  &lt;li&gt;MLP (DAE): An unsupervised pre-training method designed for deep models on tabular data, specifically the swap noise Denoising AutoEncoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table3_4.png?raw=true&quot; alt=&quot;table3_4&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Method:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pre-trained model (TabTransformer-RTD/MLM 및 MLP)의 경우 unlabeled data에 pre-training한 후, labeled data에 fine-tuning했다.&lt;/li&gt;
  &lt;li&gt;Semi-supervised learning method (ER 및 PL)의 경우 labeled data와 unlabeled data를 모두 사용하여 학습하였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Findings:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TabTransformer-RTD/MLM 두 모델이 다른 모델보다 좋은 결과를 나타냈다.&lt;/li&gt;
  &lt;li&gt;TabTransformer (ER), TabTransformer (PL) 및 GBDT (PL)은 다른 모델의 평균보다 더 안 좋은 성능을 보였다.&lt;/li&gt;
  &lt;li&gt;TabTransformer-RTD가 unlabeled data의 수가 줄어들 수록 더 나은 성능을 보였고, TabTransformer-MLM를 압도했다.
    &lt;ul&gt;
      &lt;li&gt;MLM task인 multi-class classification보다 RTD의 binary classification이 더 쉽기 때문에 학습이 잘 되어 나타난 차이라고 해석된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;50개의 data point를 가지고 평가했을 때, MLM (ER)과 MLM (PL)이 TabTransformer 모델보다 좋은 성능을 보였다.
    &lt;ul&gt;
      &lt;li&gt;TabTransformer 모델의 경우 unlabeled data에 대해 학습할 때 유용한 embedding을 추출하는 것에 주로 학습될 뿐, classifier 자체의 weight를 update하지 않으므로 나타난 결과라고 여겨진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;전반적으로 TabTransformer 모델이 unlabeled data에서 유용한 정보를 추출하는 데에 탁월하여, supervised learning 상황에서나, 특히 unlabeled data가 많은 상황에서도 잘 활용될 수 있을 것으로 보인다.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Github.io에서 markdown 수식 문법 적용이 안될 때</title>
   <link href="https://alatteaday.github.io/ko/error%20resolution/2024/03/15/GitioMathError/"/>
   <updated>2024-03-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/error%20resolution/2024/03/15/GitioMathError</id>
   <content type="html">&lt;p&gt;Github blog 포스트에 수식을 작성했는데, markdown 수식 문법 적용이 되지 않는 문제가 있었습니다. 해결 방법을 기록해두고자 포스팅합니다.&lt;/p&gt;

&lt;h2 id=&quot;1-_configyml-파일-수정&quot;&gt;1. _config.yml 파일 수정&lt;/h2&gt;

&lt;p&gt;markdown process 관련 설정을 확인하여 수정, 없으면 추가합니다. markdown engine을 kramdown으로 설정해야 한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml1.png?raw=true&quot; style=&quot;zoom:52%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-_includes-폴더-내-수식-문법-관련-html-파일-작성&quot;&gt;2. _includes 폴더 내 수식 문법 관련 HTML 파일 작성&lt;/h2&gt;

&lt;p&gt;일반적으로 github blog 내에는 _include 폴더가 존재합니다. 폴더 내에 수식 문법이 포스트에 적용될 수 있게끔 하기 위한 스크립트를 작성합니다. 아래 내용이 HTML 파일에 작성되면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html0.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inlineMath&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt; 항목에서 각각의 수식 문법 기호를 설정할 수 있습니다. 위 예시의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt; 와 같이 리스트 내에 여러 기호를 설정할 수 있습니다. 위 예시에 따르면 수식을  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt; 로 감싸거나, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt; 사이에 입력하면 display style로 작성할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;*&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt; 말고  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\]&lt;/code&gt; 로 문법을 설정하여 포스트에 적용하면, [ ] 괄호를 사용한 일반 텍스트까지 수식으로 처리되는 경우가 있었습니다.&lt;/p&gt;

&lt;h3 id=&quot;inline과-display-style&quot;&gt;Inline과 Display style&lt;/h3&gt;

&lt;p&gt;수식 입력 방식에는 inline style과 display style이 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inline style: 줄 바꿈 없이, 문장 내에서 수식을 표기하는 방법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Display style: 수식을 블록으로 생성해 표기하는 방법&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$2$ plus $3$ is $5$: $$2+3=5$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;$2$ plus $3$ is $5$: \[2+3=5\]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-2에서-작성한-html-스크립트를-포스트에-적용&quot;&gt;3. 2에서 작성한 HTML 스크립트를 포스트에 적용&lt;/h2&gt;

&lt;p&gt;위에서 작성한 스크립트를 실제 포스팅 시 적용하기 위해 layout에 관련한 HTML 파일을 수정합니다. _layout 폴더에 있는 HTML 파일 중 적합한 파일을 찾아 포스트의 내용 부분에 새로 작성한 HTML 파일의 내용을 가져와 적용합니다. 저는 ‘default.html’ 파일 중 content가 입력되는 부분을 찾아 수정했습니다. 아래 예시와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html1.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;content&quot;&lt;/code&gt; 블록 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ content }&lt;/code&gt; 의 위치에 작성한 포스트의 본문이 보여집니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;include file.html&lt;/code&gt; 은 ‘file.html’의 내용을 가져온다는 뜻입니다. 따라서 해당 블록 내에 ‘math.html’에서 작성한 수식 문법 사항을 적용하겠다는 의미의 코드가 됩니다.&lt;/p&gt;

&lt;p&gt;위 코드를 아래와 같이 수정하면 수식 문법 적용 여부를 포스팅 시 설정해 줄 수 있는데요,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt; 가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt; 이면 ‘math.html’ 내용을 적용한다는 의미의 코드입니다. 여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page&lt;/code&gt; 는 각 포스트를 의미합니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt; 을 설정하기 위해서는 매 포스트 작성 시 Front Matter에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math: true&lt;/code&gt; 를 추가해주면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;수식이 필요 없거나, 수식을 적용하기 싫은 포스트에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math&lt;/code&gt; 를 추가하지 않거나 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt; 로 설정하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://junia3.github.io/blog/markdown&quot;&gt;https://junia3.github.io/blog/markdown&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://an-seunghwan.github.io/github.io/mathjax-error/&quot;&gt;https://an-seunghwan.github.io/github.io/mathjax-error/&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>When mathematical expression syntax isn't applying on GitHub Pages</title>
   <link href="https://alatteaday.github.io/ko/dev%20tips%20&%20fixes/2024/03/15/GitioMathError/"/>
   <updated>2024-03-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/dev%20tips%20&%20fixes/2024/03/15/GitioMathError</id>
   <content type="html">&lt;p&gt;I wrote a math expression in a GitHub blog post, but there was an issue with applying markdown syntax. I’m posting this to document the solution that I applied.&lt;/p&gt;

&lt;h2 id=&quot;1-modify-the-_configyml-file&quot;&gt;1. Modify the _config.yml file&lt;/h2&gt;

&lt;p&gt;Check and modify the markdown-related settings in the _config.yml file like below. If they don’t exist, add them like below. It’s recommended to set the markdown engine to kramdown.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml1.png?raw=true&quot; style=&quot;zoom:52%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-write-a-html-file-of-math-expression-syntax-within-the-_includes-folder&quot;&gt;2. Write a HTML file of math expression syntax within the _includes folder&lt;/h2&gt;

&lt;p&gt;Generally, GitHub blogs contain an _include folder. Write a script within this folder to enable math expression syntax to be applied to posts. Let’s assume creating a html file named ‘math’&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html0.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can set each math syntax mark for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inlineMath&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt;. Similar to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt; item in the above code, you can specifiy multiple marks in the list. Following the example, if you wrap the formula in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt;, the math style will be displayed as the display style.&lt;/p&gt;

&lt;p&gt;*When setting the syntax as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\[&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\]&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt;, there might be instances where ordinary text enclosed within square brackets is also treated as part of the math expression.&lt;/p&gt;

&lt;h3 id=&quot;inline-and-display-style&quot;&gt;Inline and Display style&lt;/h3&gt;

&lt;p&gt;The inline style and the display style are two styles of math expression.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inline style: Representing math expression within a sentence without line breaks&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Display style: Generating math expression as blocks for representation&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$2$ plus $3$ is $5$: $$2+3=5$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;$2$ plus $3$ is $5$: \[2+3=5\]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-apply-the-html-script-created-in-2-to-the-post&quot;&gt;3. Apply the HTML script created in 2. to the post&lt;/h2&gt;

&lt;p&gt;To apply the script created above to an actual post, you’ll need to modify the HTML file related to the layout. Find an appropriate file in the _layout folder and incorporate the content of the html file into the section where the post’s content is inserted. For example, I found and modified the ‘default.html’ file like the example below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html1.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ content }&lt;/code&gt; displalys the main body of the post. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;include file.html&lt;/code&gt; means it includes the content of ‘file.html’. Therefore, within this block, it signifies applying the math syntax written in ‘math.html’&lt;/p&gt;

&lt;p&gt;You can modify the code and adjust if applying the math syntax or not,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The code &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt; being &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt; indicates that the content of ‘math.html’ will be applied. Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page&lt;/code&gt; refers to the each page. To set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt;, simply add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math: true&lt;/code&gt; to the Front Matter of each post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For posts where math expressions are not needed or you prefer not to apply them, simply omit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math&lt;/code&gt; tag or set it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://junia3.github.io/blog/markdown&quot;&gt;https://junia3.github.io/blog/markdown&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://an-seunghwan.github.io/github.io/mathjax-error/&quot;&gt;https://an-seunghwan.github.io/github.io/mathjax-error/&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Docker 명령어 정리</title>
   <link href="https://alatteaday.github.io/ko/dev%20tips%20&%20fixes/2024/02/20/dockercmd/"/>
   <updated>2024-02-20T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/dev%20tips%20&%20fixes/2024/02/20/dockercmd</id>
   <content type="html">&lt;h1 id=&quot;image&quot;&gt;Image&lt;/h1&gt;

&lt;h2 id=&quot;이미지-검색&quot;&gt;이미지 검색&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker search [OPTIONS] IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--automated=false&lt;/code&gt;  자동 빌드만 표시&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--no-trunc=false&lt;/code&gt;  모든 결과 표시&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-s=n&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--stars=n&lt;/code&gt;  star수가 n개 이상인 이미지만 표시&lt;/li&gt;
  &lt;li&gt;e.g.,
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker search --stars=100 mysql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;이미지-다운로드&quot;&gt;이미지 다운로드&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull [OPTIONS] IMAGE_NAME[:TAG_NAME]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt;  해당 이미지의 모든 버전 다운로드&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TAG_NAME&lt;/code&gt;  다운로드할 버전 정보, 지정하지 않으면 최신 버전(latest) 다운로드&lt;/li&gt;
  &lt;li&gt;e.g.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull ubuntu:22.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;이미지-목록-확인&quot;&gt;이미지 목록 확인&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker images [OPTIONS] [REPOSITORY]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--all&lt;/code&gt;  모든 이미지 표시&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--digests&lt;/code&gt;  digest 표시&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-q&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--quiet&lt;/code&gt;  이미지 ID만 표시&lt;/li&gt;
  &lt;li&gt;*컨테이너 목록 확인:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker ps&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;이미지-세부-정보-확인&quot;&gt;이미지 세부 정보 확인&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker image inspect IMAGE_ID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Full ID를 입력하지 않고 일부만 입력해도 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;이미지-삭제&quot;&gt;이미지 삭제&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rmi [OPTION] IMAGE_NAME:TAG_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-f&lt;/code&gt;  실행 중인 컨테이너의 이미지를 강제 삭제, 그러나 실질적으로 untagging만 되고 이미지와 컨테이너 모두 실제로 삭제되지 않는다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;여러 이미지를 한 번에 삭제&lt;/em&gt;:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rmi IMAGE_NAME_1 IMAGE_NAME_2 ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;특정 이미지의 실행 중인 컨테이너를 모두 종료시킨 후 이미지 삭제&lt;/em&gt;:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rm -f $(docker ps -a --filter ancestor=IMAGE_NAME)
docker rmi IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;이미지-저장-및-로드&quot;&gt;이미지 저장 및 로드&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker save -o DIRECTORY IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-o&lt;/code&gt;  (output) 이미지를 저장할 디렉토리를 지정&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker load -i DIRECTORY
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-i&lt;/code&gt;  불러올(input) 이미지의 디렉토리를 지정, 해당 디렉토리 내 이미지가 로드된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;이미지-태그-지정&quot;&gt;이미지 태그 지정&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker tag IMAGE_NAME:TAG NEW_NAME:NEW_TAG
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;존재하는 이미지를 복사하여 새로운 이름과 태그로 참조 가능하게 한다.&lt;/li&gt;
  &lt;li&gt;e.g.,
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker tag ubuntu:22.04 abcd:0.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;container&quot;&gt;Container&lt;/h1&gt;

&lt;h2 id=&quot;컨테이너-목록-확인&quot;&gt;컨테이너 목록 확인&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker ps [OPTION]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;실행 중인 컨테이너 목록을 확인한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt;  실행 여부와 상관 없이 (종료된 것까지) 모든 컨테이너 확인&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;컨테이너-세부-정보-확인&quot;&gt;컨테이너 세부 정보 확인&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker inspect CONTAINER_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;이미지에서-컨테이너-실행&quot;&gt;이미지에서 컨테이너 실행&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run [OPTIONS] IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--name CONTAINER_NAME&lt;/code&gt;  컨테이너 이름 설정&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--rm&lt;/code&gt;  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; 명령어 수행 후 컨테이너 삭제. 컨테이너 일회성 사용&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-it&lt;/code&gt;  컨테이너에 터미널 입력을 계속해서 전달
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-i&lt;/code&gt;  컨테이너에 접속하지 않은 상태에서도 stdin 활성화&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-t&lt;/code&gt;  pseudo-TTY 할당(TTY모드 사용), 쉘에 명령어 작성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-d&lt;/code&gt;  백그라운드 실행. 옵션 입력 시 실행된 컨테이너 id가 출력된다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-e&lt;/code&gt;  환경변수 추가. 추가하고 싶은 환경변수만큼 사용한다.
    &lt;ul&gt;
      &lt;li&gt;e.g.,
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -e APP_ENV=production APP2_ENV=dev ubuntu:22.04 env
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p HOST_PORT:CONTAINER_PORT&lt;/code&gt;  호스트에 연결된 컨테이너의 특정 포트를 호스트의 포트와 바인딩. 보통 웹서버의 포트를 외부로 노출하기 위해 사용한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-w DIR&lt;/code&gt;  작업 디렉토리 변경&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-v HOST_DIR:CONTAINER_DIR&lt;/code&gt;  호스트의 특정 디렉토리를 컨테이너에 마운트
    &lt;ul&gt;
      &lt;li&gt;e.g.,
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -v volume:/data ubuntu:22.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;현재 작업 디렉토리를 컨테이너에 마운트하기&lt;/em&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -v `pwd`:/opt ubuntu:22.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-u USER_ID&lt;/code&gt;  특정 user id로 컨테이너에 접속. 이미지 빌드 시 계정을 추가해야 가능하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;실행-중인-컨테이너에-명령어-입력&quot;&gt;실행 중인 컨테이너에 명령어 입력&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker exec CONTAINER_ID or NAME CMD
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-it&lt;/code&gt;  컨테이너 환경에서 shell 실행,&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;run-과-exec의-차이점&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; 과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exec&lt;/code&gt;의 차이점&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt;: 이미지에서 컨테이너를 실행&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exec&lt;/code&gt;: 이미 실행 중인 컨테이너에서 명령어 실행&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;컨테이너-중지&quot;&gt;컨테이너 중지&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker stop CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;실행 중인 컨테이너를 중지 (Graceful shutdown)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker kill CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;실행 중인 컨테이너를 강제 종료&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;컨테이너-재시작&quot;&gt;컨테이너 재시작&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker start CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;중지된 컨테이너를 다시 시작&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker restart CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;컨테이너를 중지시키고 다시 시작&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records (2023)</title>
   <link href="https://alatteaday.github.io/ko/papers/2024/02/15/transformehr/"/>
   <updated>2024-02-15T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2024/02/15/transformehr</id>
   <content type="html">&lt;p&gt;Yang, Zhichao, et al. “TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records.” &lt;em&gt;Nature Communications&lt;/em&gt; 14.1 (2023): 7857.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41467-023-43715-z&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;New pre-training objective: predicting all diseases or outcomes of a future visit&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Helps the model uncover the complex interrelations among different diseases and outcomes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;TransformEHR&lt;/strong&gt;, the generative encoder-decoder framework to predict patients’ ICD codes using their longitudinal EHRs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Validation of the generalizability using both internal and external datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Demonstrated a strong transfer learning capability of the model&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Could be great with limited data and computing resources&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Longitudinal electronic health records (EHRs) have been successfully used to predict clinical diseases or outcomes (congestive heart failure, sepsis mortality, mechanical ventilation, septic shock, diabetes, PTSD, etc.)&lt;/li&gt;
  &lt;li&gt;With the availability of large cohorts and computational resources, deep learning (DL) based models outperform traditional machine learning (ML) models (Med-BERT, BEHRT, BRLTM, etc.)&lt;/li&gt;
  &lt;li&gt;The existing pre-training tasks were limited in predicting a fraction of ICD codes within each visit :arrow_right: A novel pre-training strategy, which predicts the complete set of diseases and outcomes within a visit, might improve clinical predictive modeling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;*VHA: Veterans Health Administration, the largest integrated healthcare system in the US, providing care at 1,321 healthcare facilities&lt;/p&gt;

&lt;p&gt;Pre-training data: around 6M patients who received care from more than 1,200 facilities of the US VHA&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/table1.png?raw=true&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two common and uncommon disease/outcome agnostic prediction (DOAP) datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;ICD-10CM codes with more than a 2% prevalence ratio for common dataset&lt;/li&gt;
      &lt;li&gt;Those with a 0.04%-0.05% prevalence ratio for uncommon dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-VHA dataset: from MIMIC-IV dataset (29,482)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Only selected objects with ICD-10CM records to match the cohorts from VHA&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;longitudinal-ehrs&quot;&gt;Longitudinal EHRs&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/fig1.png?raw=true&quot; alt=&quot;image-20240306100731535&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Include demographic information (gender, age, race, and marital status) and ICD-10CM codes as predictors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Group ICD codes at the visit level&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Order the codes by priority, where the primary diagnosis is typically given the highest priority&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Form multiple visits as a time-stamped input of a sequence by date of visit&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;file:///Users/jiyun/Documents/Gitlog/alatteaday.github.io/post_images/2024-02-15-transformehr/KakaoTalk_Image_2024-03-13-09-14-52_007.png&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/fig2.png?raw=true&quot; alt=&quot;image-20240306103928325&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/code_embeds1.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-level embeddings: visit embeddings + time embeddings + code embeddings&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Time embeddings: embed days difference as relative time information by getting the difference between a certain visit and the last visit in the EHR
    &lt;ul&gt;
      &lt;li&gt;Includes the date of each visit to integrate temporal information, not only sequential order&lt;/li&gt;
      &lt;li&gt;Date is important as the importance of predictor in a visit can vary over time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;Encoder-decoder transformer-based architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/fig3.png?raw=true&quot; alt=&quot;image-20240306114845710&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encoder: performs cross-attention, unlikely BERT, over representations and assigns an attention weight for each representation
    &lt;ul&gt;
      &lt;li&gt;Cross-attention is implemented by masking the complete set of ICD codes of a future visit as shown in Fig. 2b&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Decoder: generates ICD codes of the masked future visit with the weighted representations from the encoder
    &lt;ul&gt;
      &lt;li&gt;Generates the codes following the order of code priority within a visit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;Metrics: PPV (precision), AUROC, AUPRC&lt;/p&gt;

&lt;p&gt;Baseline models: logistic regression, LSTM, BERT without pre-training, BERT with pre-training   &lt;span style=&quot;color:silver;&quot;&gt; # what’s the objective when pre-training BERT? MLM or the objective proposed in this paper? &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/table2_3_4.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h2&gt;

&lt;p&gt;Task: Disease or outcome agnostic prediction (DOAP); Predicting the ICD codes of a patient’s future visit based on longitudinal information up to the current visit&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ablation study&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Visit masking vs. code (part of visit) masking for an encoder-decoder model
    &lt;ul&gt;
      &lt;li&gt;Visit masking performed better; pre-training of all diseases outperform traditional pre-training objective (2.52-2.96% in AUROC)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encoder-decoder vs. encoder-only (BERT) on DOAP
    &lt;ul&gt;
      &lt;li&gt;​Encoder-decoder outperformed; 0.74-1.16% in AUROC   &lt;span style=&quot;color:silver;&quot;&gt;# the possibility if the parameter size affected this result? &lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Time embeddings O vs. X
    &lt;ul&gt;
      &lt;li&gt;The model with the time embeddings outperformed moderately; 0.43 in AUROC&lt;/li&gt;
      &lt;li&gt;Days difference is more effective than specific date as the embeddings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning&quot;&gt;Fine-tuning&lt;/h2&gt;

&lt;p&gt;Tasks: the pancreatic cancer onset prediction (Table 3) and intentional self-harm prediction in patients with PTSD (Table 4)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TransformEHR outperforms on the both tasks&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AUPRC was consistent when using different set of demographics&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Results with all visits was better than with recent few(five) visits&lt;/li&gt;
  &lt;li&gt;In generalizability evaluation,
    &lt;ul&gt;
      &lt;li&gt;When testing with internal dataset which is included data from VHA facilities not used for pre-training, there’s no statistical difference in AUPRC on the intentional self-harm prediction task among PTSD&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>[Study] white matter hyperintensity (WMH)와 아밀로이드 베타 (β-amyloid) 및 타우 축적 양상 간의 관계 논문 스터디</title>
   <link href="https://alatteaday.github.io/ko/study/2024/01/31/WMHandAbTau/"/>
   <updated>2024-01-31T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2024/01/31/WMHandAbTau</id>
   <content type="html">&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://newsimg-hams.hankookilbo.com/2023/03/25/0187e938-b461-4abc-9fa1-2279ecbc1f03.jpg&quot; alt=&quot;백질 변성이 오래되고 진행한 73세 환자의 뇌 MRI 사진. 뇌 중심부에 하얀색으로 넓게 퍼져 있음. 부천성모병원 제공&quot; style=&quot;zoom:30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;WMH: 뇌 백질 변성, 피질인 회백질 간 통로 역할을 하는 수질인 백질이 뇌혈관 변성에 의해 확장되어 손상된 형태. 보통 MR FLAIR 이미지를 통해 확인&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;WMH와 Alzhheimer’s disease biomarker인 Amyloid beta, Tau 간의 관계를 조사하기 위해 다음의 세 논문을 읽었습니다.&lt;/p&gt;

&lt;p&gt;Alban, Sierra L., et al. “The association between white matter hyperintensities and amyloid and tau deposition.” &lt;em&gt;NeuroImage: Clinical&lt;/em&gt; 38 (2023): 103383.  &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2213158223000724&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Graff-Radford, Jonathan, et al. “White matter hyperintensities: relationship to amyloid and tau burden.” &lt;em&gt;Brain&lt;/em&gt; 142.8 (2019): 2483-2491.  &lt;a href=&quot;https://academic.oup.com/brain/article/142/8/2483/5519094?login=false&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hedden, Trey, et al. “Cognitive profile of amyloid burden and white matter hyperintensities in cognitively normal older adults.” &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 32.46 (2012): 16233-16242.  &lt;a href=&quot;https://www.jneurosci.org/content/32/46/16233.short&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;keys&quot;&gt;Keys&lt;/h2&gt;

&lt;p&gt;위 세 논문에 따르면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WMH와 Amyloid beta의 축적 양상 간의 관계는 아직 명확하게 밝혀진 바가 없다: 관련성의 유무부터 관련된다면 어떻게 관련되는지까지&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;점차 WMH와 Ab간의 관계를 밝히려는 연구가 늘어나고 있는 추세이다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;WMH와 Tau의 축적 양상 간에는 관계성이 없다&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;The association between white matter hyperintensities and amyloid and tau deposition (2023)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;… Finally, the regions where β-amyloid and WMH count were most positively associated were &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the middle temporal region in the right hemisphere&lt;/span&gt; (r = 0.18, p = 0.002) and &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the fusiform region in the left hemisphere&lt;/span&gt; (r = 0.017, p = 0.005). β-amyloid and WMH have a clear association, though the mechanism facilitating this association is still not fully understood. The associations found between β-amyloid and WMH burden emphasize the relationship between β-amyloid and vascular lesion formation while factors like CVRFs, age, and sex affect AD development through various mechanisms&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The subset of ADNI-3 participants who had all the T1-weighted, 3D FLAIR, Amyloid, and Tau PET modalities available&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Snippets&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The percentage of white matter volume occupied by WMH was significantly and positively correlated with β-amyloid PET SUVR (Fig. 1; r = 0.28, p &amp;lt; 0.001). We observed &lt;span style=&quot;background-color:#fff5b1&quot;&gt;WMH volume to significantly predict global amyloid accumulation when controlling for age, sex, years of education, and scanner manufacturer&lt;/span&gt; (F(1, 309) = 13.9, p = 0.0002).&lt;/li&gt;
  &lt;li&gt;The correlational analyses were repeated after the log transformation of WMH volume and outcomes remained the same, resulting in &lt;span style=&quot;background-color:#fff5b1&quot;&gt;a significant positive correlation between WMH volume and β-amyloid&lt;/span&gt; (r = 0.24, p = 4.9e-5), and &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;a nonsignificant positive correlation between WMH volume and meta-temporal tau&lt;/span&gt; (r = 0.09, p = 0.12).&lt;/li&gt;
  &lt;li&gt;The inclusion of MOCA, MMSE, and Global CDR, as covariates, did not change the significant relationship between WMH volume and β-amyloid. We observed &lt;span style=&quot;background-color:#fff5b1&quot;&gt;a significant effect of hippocampal volume fraction on WMH volume&lt;/span&gt; (F(1, 580) = 16.9, p = 4.5e-5) and β-amyloid (F(1, 309) = 32.5, p = 2.8e-8).&lt;/li&gt;
  &lt;li&gt;WMH volume percent of participants with either amyloid (A+) or tau (T+) pathology was higher than controls (A-/T-) (Fig. 2). We observed &lt;span style=&quot;background-color:#fff5b1&quot;&gt;a significantly higher WMH percent in AD pathology participants (A+/T+) compared to controls (A-/T-)&lt;/span&gt; (p = 0.007, Cohen’s d = 0.4, t = -2.5). … &lt;span style=&quot;background-color:#fff5b1&quot;&gt;No significant association was found in the A-/T- group&lt;/span&gt; (r = 0.06, p = 0.45). A significant positive correlation was observed between β-amyloid SUVR and WMH count in the A+/T+ group only.&lt;/li&gt;
  &lt;li&gt;WMH count was used as another method of measuring WMH burden. … Both statistical tests on &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the WMH volume and the WMH count showed similar results, confirming that WMH count is an accurate measure to use alongside WMH volume.&lt;/span&gt; Correlations of WMH count with β-amyloid within A/T pathological groups also paralleled the WMH volume analysis result.&lt;/li&gt;
  &lt;li&gt;Our regional analysis showed that β-amyloid and WMH accumulation in &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the precentral, cuneus, fusiform, isthmus cingulate, lateral occipital, lingual, superior parietal, and supramarginal regions were most significantly associated across all pathological groups when averaged across hemispheres.&lt;/span&gt; Variations in the locations of increased WMHs are indicative of AD and its phase of progression, some of these regions being more implicated in cognitive decline than others.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We observed &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;neither a correlation nor an association between WMH and Tau uptake in the entire cohort&lt;/span&gt; (Fig. 4: correlation p = 0.25; association p = 0.4, controlling for age, sex, years of education, and scanner manufacturer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, &lt;span style=&quot;background-color:#E6E6FF&quot;&gt;WMH volume was only predicted by CN and MCI diagnoses, not AD.&lt;/span&gt; The relationship between WMH volume often predicts AD in the preclinical stages, likely accounting for the relationship we observed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#E6F2FF&quot;&gt;The inclusion of cognitive scores MMSE, MOCA, and Global CDR had no effect on the associations found between β-amyloid and WMH volume&lt;/span&gt;, therefore not significantly impacting this relationship. In the literature, &lt;span style=&quot;background-color:#E6F2FF&quot;&gt;worse performance on these cognitive tests has been associated with increased WMH volume&lt;/span&gt; (Wang et al., 2020).&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;   
  &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Gray727_cingulate_gyrus.png/250px-Gray727_cingulate_gyrus.png&quot; alt=&quot;img&quot; width=&quot;30%&quot; align=&quot;center&quot; /&gt;
  &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Gray726_temporal_pole.png/800px-Gray726_temporal_pole.png?20100306033920&quot; alt=&quot;File:Gray726 temporal pole.png&quot; width=&quot;30%&quot; style=&quot;zoom: 33%;&quot; align=&quot;center&quot; /&gt;
  &lt;figcaption align=&quot;center&quot; style=&quot;color: grey;&quot;&gt; above: the isthmus cingulate; bottom: the temporal pole &lt;/figcaption&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The relationship between higher MMSE scores and increased WMH volume showed significance with multiple comparison corrections while controlling for age and sex in the isthmus cingulate region&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For Global CDR scores, this spatial distribution was significant in the isthmus cingulate, temporal pole, and pars triangularis&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lastly, the MOCA scores showed significant positive spatial relationships in the isthmus cingulate, linguistic, and temporal pole regions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We also did not observe any significant effect of APOE-ε4 presence on the established relationship between β-amyloid and WMH volume or WMH count. … Although, we did have a small sample size of individuals with the homozygous APOE-ε4 genotype.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;White matter hyperintensities: relationship to amyloid and tau burden (2019)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;White matter hyperintense volumes in the detected topographic pattern correlated strongly with lobar cerebral microbleeds&lt;/span&gt; (P &amp;lt; 0.001, age and sex-adjusted Cohen’s d = 0.703). In contrast, &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;there were no white matter hyperintense regions significantly associated with increased tau burden&lt;/span&gt; using voxel-based analysis or region-specific analysis, among non-demented elderly, amyloid load correlated with a topographic pattern of white matter hyperintensities. Further, &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the amyloid-associated, white matter hyperintense regions strongly correlated with lobar cerebral microbleeds suggesting that cerebral amyloid angiopathy contributes to the relationship between amyloid and white matter hyperintensities.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://lh7-us.googleusercontent.com/8lRtc_4VpROGK82z1LNG6_csgW2j70aFK-1z6EtUHavsTxGrkjAOzyxf_l_6ZNFMVsEWDE2WBBkDnYTC8eSIGRPwtYTdmvVaMSYl-XidCzDWZu1aouniUOTX3hUtN76xS_ESzJ5gW6Hg0eZ9Lv0jo-U&quot; alt=&quot;img&quot; style=&quot;zoom: 60%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Participants, aged 50 to 89, were enrolled in the Mayo Clinic Study of Aging (MCSA), a population-based study of Olmsted County, Minnesota residents.&lt;/li&gt;
  &lt;li&gt;434 non-demented participants with FLAIR-MRI, tau-PET (AV-1451), and Pittsburgh compound B (PiB)-PET (amyloid) scans to assess the relationship between FLAIR WMH and Alzheimer’s disease pathologies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Snippets&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the study of non-demented individuals, we found that amyloid burden measured by PET was associated with a topographic pattern of WMH. &lt;span style=&quot;background-color:#fff5b1&quot;&gt;These amyloid-related WMH regions were associated with lobar CMBs suggesting that regional changes correlate with CAA.&lt;/span&gt; We found &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;no evidence to support an association between tau burden and WMH burden.&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We did not detect an association between tau burden and WMH in either the voxel-level analyses or region-level analyses.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Cognitive Profile of Amyloid Burden and White Matter Hyperintensities in Cognitively Normal Older Adults (2012)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Amyloid burden and WMH were not correlated with one another.&lt;/span&gt; &lt;span style=&quot;background-color:#E6F2FF&quot;&gt;Age was associated with lower performance in all cognitive domains&lt;/span&gt;, while &lt;span style=&quot;background-color:#E6F2FF&quot;&gt;higher estimated verbal intelligence was associated with higher performance in all domains.&lt;/span&gt; Hypothesis-driven tests revealed that &lt;span style=&quot;background-color:#fff5b1&quot;&gt;amyloid burden and WMH had distinct cognitive profiles&lt;/span&gt;, with &lt;span style=&quot;background-color:#fff5b1&quot;&gt;amyloid burden having a specific influence on episodic memory&lt;/span&gt; and &lt;span style=&quot;background-color:#fff5b1&quot;&gt;WMH primarily associated with executive function but having broad (but lesser) effects on the other domains.&lt;/span&gt; These findings suggest that even before clinical impairment, amyloid burden, and WMH likely represent neuropathological cascades with distinct etiologies and dissociable influences on cognition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;168 (95 female) cognitively normal, community-dwelling older adults (aged 65–86, M=73.24, SD=5.80).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Participants in the Harvard Aging Brain Study, an ongoing longitudinal study currently in the baseline assessment phase&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Because of the staged nature of the visits (all baseline visits must be completed within 6 months), positron emission tomography (PET) amyloid imaging and magnetic resonance imaging (MRI) estimates of WMH were currently available for 109 of the older adults.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] BEHRT: Transformer for Electronic Health Records (2020)</title>
   <link href="https://alatteaday.github.io/ko/papers/2024/01/22/BEHRT/"/>
   <updated>2024-01-22T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2024/01/22/BEHRT</id>
   <content type="html">&lt;p&gt;Li, Yikuan, et al. “BEHRT: transformer for electronic health records.” &lt;em&gt;Scientific reports&lt;/em&gt; 10.1 (2020): 7155.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41598-020-62922-y&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;BEHRT&lt;/strong&gt; (BERT for EHR): BERT-based architecture&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In EHR, certain diseases can be reversed, or the time interval between two diagnoses can be shorter or longer than recorded.&lt;/p&gt;

        &lt;p&gt;→ Bidirectional contextual awareness of the model’s representation is a big advantage with EHR data.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transfer Learning: pre-training on predicting of masked disease words, such as Masked Language Modeling (MLM), and then fine-tuning on three disease prediction tasks&lt;/li&gt;
  &lt;li&gt;Disease embeddings: show the relations between the various diseases&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In traditional research on EHR data, individuals are represented by models as features. Experts had to define the appropriate features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Studies applying Deep Learning (DL) to EHR started to show that DL models can outperform the traditional Machine Learning (ML) methods&lt;/li&gt;
  &lt;li&gt;With DL architectures for sequence data, such as Recurrent Neural Networks (RNNs), the application of DL models for EHR was improved in terms of capturing the long-term dependencies among events.&lt;/li&gt;
  &lt;li&gt;Similarities between sequences in EHR and natural language lead to the successful transferability of techniques&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;Clinical Practice Research Datalink (CPRD)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;one of the largest linked primary care EHR systems&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Contains longitudinal data from a network of 674 general practitioner practices in the UK&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/1_fig1.png?raw=true&quot; alt=&quot;스크린샷 2024-02-29 오후 8.29.23&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;from ​8 million patients (eligible for linkage to HES, meet CPRD’s quality standards) to 1.6 million patients  (having at least 5 visits in the EHR)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Only the data from GP practices considered in this study, which consented to record linkage with HES&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;input-features&quot;&gt;Input Features&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/2_fig2.png?raw=true&quot; alt=&quot;스크린샷 2024-02-29 오후 8.30.30&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Only consider the diagnoses and ages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The patient $p$’s EHR:
\[
V_p={v^1_p},{v^2_p},{v^3_p}, …,{v^n_p}
\]&lt;/p&gt;

&lt;p&gt;The patient $p$’s EHR of the $j$th visit: $v^j_p$&lt;/p&gt;

&lt;p&gt;$v^j_p$ is a list consisting of single or multiple $m$ diagnoses: $v^j_p={d_1, …d_m}$&lt;/p&gt;

&lt;p&gt;Input sequence:
\[
I_p={CLS, v^1_p, SEP, v^2_p, SEP,…,v^{n_p}_p, SEP}
\]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$CLS$ token: the start of a medical history&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$SEP$ token: between visits&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/3_fig3.png?raw=true&quot; alt=&quot;스크린샷 2024-02-29 오후 8.31.09&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A combination of 4 embeddings: disease + position + age + visit segment&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Disease embeddings: past diseases can improve the accuracy of the prediction for future diagnoses&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Positional encodings: determine the relative position in the EHR sequence&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;p&gt;Pre-training: prediction of masked disease words (MLM)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;86.5% of the disease words were unchanged, 12% of them replaced with the mask token, and the remaining 1.5% words replaced with randomly-chosen disease words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fine-tuning: 3 different disease prediction tasks&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Prediction of diseases in the next visit (T1)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Randomly choose an index $j (3&amp;lt;j&amp;lt;n_p)$ for each patient&lt;/li&gt;
      &lt;li&gt;Form input as $x_p={v^1_p, …, v^j_p}$&lt;/li&gt;
      &lt;li&gt;Output $y_p=w_{j+1}$, $w_{j+1}$ is a multi-one-hot vector, indexed for disease that exist in $v^{j+1}_p$&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;Each patient contributes only an input-output pair to the training and evaluation process.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prediction of diseases in the next 6 months (T2) &amp;amp; in the next 12 months (T3)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Not include patients that don’t have 6 or 12 months worth of EHR in the analysis&lt;/li&gt;
      &lt;li&gt;choose $j$ randomly from $(3, n*)$, where $n*$ is the highest index after 6 or 12 months&lt;/li&gt;
      &lt;li&gt;output $y_p=w_{6m}$ and $y_p=w_{12m}$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The number of patients for each task: 699K, 391K, 342K&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;h2 id=&quot;disease-embeddings&quot;&gt;Disease Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/4_fig4.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 9.39.18&quot; /&gt;&lt;/p&gt;

&lt;p&gt;T-SNE Results&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The natural stratification of gender-specific diseases&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The natural clusters are formed that in most cases consist fo disease of the same chapter&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;10 closest diseases by cosine similarity of their embeddings are founded as similar as those provided by a clinical researcher (0.757 overlab) - Supplementary table S3&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;attention-and-interpretability&quot;&gt;Attention and Interpretability&lt;/h2&gt;

&lt;p&gt;Found the relationships among events which goes beyond temporal/sequence adjacency&lt;/p&gt;

&lt;p&gt;Analysed the attention-based patterns by Vig&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Strong connections between rheumatoid arthritis and enthesopathies and synovial disorders → Attention can go beyond recent events and find long-range dependencies among diseases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;disease-prediction&quot;&gt;Disease Prediction&lt;/h2&gt;

&lt;p&gt;Metrics: average precision score(APS), AUROC&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;APS: a weighted mean of precision and recall achieved at different thresholds&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Performance scores&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/5_table1.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 9.49.39&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;→ Outperformed the best model by more than around 8% in predicting for a range of more than 300 diseases&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparing performance for each disease&lt;/p&gt;

    &lt;p&gt;APS and AUROC scores with the all \(y_p\) and \(y*_p\) vectors of a disease&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/6_fig6.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 10.08.39&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/7_table2.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 10.08.53&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Flexibility of BEHRT from employed 4 key concepts from EHR: disease, age, segment, position
    &lt;ul&gt;
      &lt;li&gt;gains insights about underlying generating process of EHR&lt;/li&gt;
      &lt;li&gt;Distributed/complex representations that are capable of capturing concepts of disease&lt;/li&gt;
      &lt;li&gt;Future work: adding new concepts to the embeddings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Disease embeddings provide great insights into how various diseases are related to each other: co-occurrence and closeness of diseases
    &lt;ul&gt;
      &lt;li&gt;Could be used for future research as reliable disease vectors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Important features of EHR for prediction&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Robust, gender-specific predictions without inclusion of gender&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Position was important&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Age embeddings might be vital in diagnosing age-related diseases&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>[LearnMRI] MRI로 관찰되는 알츠하이머 치매 관련 혈관성 변화</title>
   <link href="https://alatteaday.github.io/ko/study/2023/12/26/mri3/"/>
   <updated>2023-12-26T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2023/12/26/mri3</id>
   <content type="html">&lt;p&gt;Amyloid beta (A$\beta$)가 침착되면 Alzheimer’s demensia가 발병될 확률이 높아진다. A$\beta$ 침착에 의한 뇌 손상 이전에 혈관성 변화들이 관찰 되는데, 혈관성 변화는 생활 습관 개선이나 약물 치료로 개선될 수 있다. &lt;a href=&quot;https://alatteaday.github.io/ko/study/2023/12/26/mri2/&quot;&gt;다양한 형식의 MRI&lt;/a&gt;를 통해 관찰할 수 있는 혈관성 변화는 여러가지가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;choroid-plexus-chp&quot;&gt;Choroid plexus (ChP)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/chp.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MRI type: T1-weighted image (T1WI)&lt;/li&gt;
  &lt;li&gt;ChP: 뇌실(ventricle)에서 발견되는 혈관과 세포의 네트워크
    &lt;ul&gt;
      &lt;li&gt;혈액-뇌 사이에서 면역세포의 관문 역할을 한다.&lt;/li&gt;
      &lt;li&gt;뇌척수액(cerebrospinal fluid; CSF)을 생산하여 뇌세포의 노폐물 및 독성 제거에 도움을 준다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ChP의 부피가 클수록 인지 장애 정도가 심해진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;perivascular-space-pvs&quot;&gt;Perivascular space (PVS)&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/pvs1.png?raw=true&quot; style=&quot;zoom: 35%;&quot; /&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/pvs2.png?raw=true&quot; style=&quot;zoom: 25%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MRI type: T2-weighted image (T2WI)&lt;/li&gt;
  &lt;li&gt;PVS: 뇌를 관통하는 세동맥을 둘러싸고 있는 공간
    &lt;ul&gt;
      &lt;li&gt;혈관벽 내 및 주위의 체액, 세포 조직 등을 아우르는 개념이다.&lt;/li&gt;
      &lt;li&gt;Blood-brain barrier (BBB)의 한 구성 요소이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fluid dynamic
    &lt;ul&gt;
      &lt;li&gt;CFS와 간질액(interstitial fluid; ISF) 사이 네트워크 역할을 해 대뇌의 부산물을 정화한다.&lt;/li&gt;
      &lt;li&gt;뇌 속 림프계의 역할을 한다고 알려져 있다. → glymphatic system&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PVS의 확장(Enlarged PVS; EPVS)은 퇴행성뇌질환 및 다양한 뇌 질병과 관련이 있다.&lt;/li&gt;
  &lt;li&gt;PVS volume과 Amyloid beta (A$\beta$) 양성 확률은 비례한다.
    &lt;ul&gt;
      &lt;li&gt;Temporal lobe에서 경향성이 두드러진다: Temporal PVS를 A$\beta$ 양성 환자들이 많이 가지고 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;white-matter-hyperintensity-wmh&quot;&gt;White matter hyperintensity (WMH)&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/wmh.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MRI type: FLAIR&lt;/li&gt;
  &lt;li&gt;WMH: 백색물질 과집중
    &lt;ul&gt;
      &lt;li&gt;노화가 주 원인으로, 혈류의 상실, 세포 손상으로 나타난다.&lt;/li&gt;
      &lt;li&gt;Subcortical hyperintensity: basal ganglia 근처의 WMH&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;알츠하이머 치매와 직접적인 연관이 있다고 아직 확정되지는 않았으나, 환자들을 관찰했을 때 A$\beta$ 양성과 WMH volume 간 비례 관계가 있는 것으로 보인다고 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[LearnMRI] MR Modality 종류와 관찰 가능한 뇌 양상</title>
   <link href="https://alatteaday.github.io/ko/study/2023/12/26/mri2/"/>
   <updated>2023-12-26T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2023/12/26/mri2</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://alatteaday.github.io/ko/study/2023/12/26/mri/&quot;&gt;Spin echo 기법&lt;/a&gt;에 따라 T1 강조 영상(T1 weighted image; T1WI)과 T2 강조 영상(T2 weighted image; T2WI)을 얻을 수 있다. 이들에 영상 조작을 더하여 다양한 MR modality 영상을 생성할 수 있다. 영상마다 병변 조직의 신호 강도(signal intensity)가 달리 나타나므로, 강조되는 병변의 종류가 다르다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/t1_t2_flair.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;t1-강조-영상-t1-weighted-image-t1wi&quot;&gt;T1 강조 영상 (T1-weighted image; T1WI)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Spin echo: TR과 TE를 모두 짧게 준다
    &lt;ul&gt;
      &lt;li&gt;TR이 짧아지면 조직마다 다른 $Mz$의 90도 pulse 회복 시간(T1 relaxation time)이 강조된다. 어느 조직은 모두 회복되고, 어느 조직은 덜 된 상태로 두 번째 pulse의 영향을 받으면, 전자는 영향을 많이, 후자는 덜 받게 된다. 이 차이가 영상에 반영 된다.&lt;/li&gt;
      &lt;li&gt;TE를 짧게 주어야 T2 relxation time 값에 영향을 덜 줌으로써 이를 통제할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Signal intensity:
    &lt;ul&gt;
      &lt;li&gt;Signal intensity가 T2-weighted image보다 높아 해부학적(anatomical) 구조물이 더 명확히 구별된다.&lt;/li&gt;
      &lt;li&gt;피하지방(fat) 및 혈액(blood)은 밝게(hyperintense), 근육은 중간 정도의 밝기로, 수분(water)은 어둡게(hypointense) 보인다.&lt;/li&gt;
      &lt;li&gt;골수(marrow)는 지방이 풍부하므로, 골피질(cortex)은 수분이 적으므로 hyperintense하게 나타난다.&lt;/li&gt;
      &lt;li&gt;병변: 지방종, 아급성 출혈, 고단백질 함유 병소(점액낭종(mococele) 등)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Observe: 뇌 피질 형태(anatomical detail), 혈관 변화(vascular changes), 혈뇌장벽(blood-brain barrier) 손상 여부&lt;/li&gt;
  &lt;li&gt;Feature: cortical thickness, choroid plexus (ChP)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;t2-강조-영상-t2-weighted-image-t2wi&quot;&gt;T2 강조 영상 (T2-weighted image; T2WI)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Spin echo: TR과 TE를 모두 길게 준다
    &lt;ul&gt;
      &lt;li&gt;TR을 길게 주어 T1 relaxation time에 영향을 덜 준다.&lt;/li&gt;
      &lt;li&gt;TE가 길수록 조직마다 $Mxy$가 감소되는 수준의 차이가 강조되어 영상에 달리 표현된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Signal intensity:
    &lt;ul&gt;
      &lt;li&gt;수분(water)이 hyperintense한데, 병적 조직은 수분을 많이 함유하므로 병소(lesion)가 쉽게 관찰된다.
        &lt;ul&gt;
          &lt;li&gt;대부분의 병변은 T1에서는 저신호, T2에서는 고신호로 나타난다.&lt;/li&gt;
          &lt;li&gt;수분 함유량이 많은 조직일 수록 hyperintense하다: 낭종 &amp;gt; 부종 &amp;gt; 정상 조직 순으로 밝게 보인다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;뇌척수액(cerebrospinal fluid; CSF)도 hyperintense하여 병변과 구분이 잘 되지 않는다.&lt;/li&gt;
      &lt;li&gt;Muscle, fat, blood는 hypointense하다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Observation: 병소(lesion), hypo 병변 (급성혈종, 진균구 등), 동맥 (정맥은 혈류 속도가 느려 환자마다 신호 강도가 다르다)&lt;/li&gt;
  &lt;li&gt;Feature: &lt;a href=&quot;https://alatteaday.github.io/ko/study/2023/12/26/mri3/&quot;&gt;Perivascular space (PVS)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;flair-fluid-attenuation-inversion-recovery&quot;&gt;FLAIR (Fluid Attenuation Inversion Recovery)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;T2에서 CSF를 까맣게 처리한 이미지이다.&lt;/li&gt;
  &lt;li&gt;Signal intensity: Non-free-flowing water는 hyperintense, fat은 hypointense로 나타난다.&lt;/li&gt;
  &lt;li&gt;Observation: 뇌실(ventricle) 부근의 병소, 부종(edema, 부종은 정체된 수분이라 밝게 나타난다), 회백질의 차이(grey-white differentiation)&lt;/li&gt;
  &lt;li&gt;Feature: lesion, &lt;a href=&quot;https://alatteaday.github.io/ko/study/2023/12/26/mri3/&quot;&gt;white matter hyperintensity (WMH)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gre-gradient-echo-t2&quot;&gt;GRE (Gradient Echo; T2*)&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/gre.png?raw=true&quot; style=&quot;zoom: 25%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Signal intensity: 상자성체 물질(paramegnetic substances, blood, calcium, metal 등)이 hyperintense로 나타나, 철의 침착과 관련된 사항을 관찰하기 용이하다.&lt;/li&gt;
  &lt;li&gt;Observation: 뇌출혈 초기/말기, 미만성 축삭손상(diffuse axonal injury)에서 보이는 microbleed 검출에 탁월
    &lt;ul&gt;
      &lt;li&gt;*diffuse axonal injury: 뇌진탕 등 뇌손상의 구성요소 중 하나, 축삭 손상으로 인해 외상 후 혼수상태를 보이는 특징이 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature: bleeding&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>[LearnMRI] MRI 촬영 원리와 특징</title>
   <link href="https://alatteaday.github.io/ko/study/2023/12/26/mri/"/>
   <updated>2023-12-26T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2023/12/26/mri</id>
   <content type="html">&lt;p&gt;MRI와 MRI modalities, MRI에서 관찰 가능한 Alzheimer’s disease 관련 feature를 정리해봅니다.
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;magnetic-resonance-imaging-mri&quot;&gt;Magnetic Resonance Imaging (MRI)&lt;/h2&gt;

&lt;p&gt;자석으로 구성된 장치에서 인체에 고주파를 쏘아 신체부위에 있는 수소원자핵을 공명시켜 각 조직에서 나오는 신호의 차이를 디지털 정보로 변환한 이미지&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mri-촬영-원리&quot;&gt;MRI 촬영 원리&lt;/h2&gt;

&lt;p&gt;인체 조직은 물을 많이 포함하고 있다. 물의 수소원자핵(hydrogen nucleus)은 자성을 갖는다. 이 수소원자핵에 고주파를 발사하면 이를 공명시킬 수 있다. 고주파(radiofrequency; RF)를 순간적으로 발사하고 끊으면(RF pulse), 원자핵이 고주파 신호를 흡수했다가 방출한다(release). 여기서 MR 기기로 되돌아오는 신호의 크기 차이를 분석하고 극대화하여 2차원 영상으로 표현한 것이 MRI이다.&lt;/p&gt;

&lt;p&gt;방출되는 신호의 크기와 파형은 물분자의 농도, 혈류, 주변 화학구조물과의 결합 상태 등에 따라 다르다. 조직 내 물이나 혈액 구성에 따라 T1 이완 시간(relaxation time)과 T2 이완 시간이 달라진다. 질병에 따라 이들 구성이 달라지므로, 질병에 따라 얻을 수 있는 신호가 다르다는 의미가 된다. 이 신호 변화를 달리 포착하여 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2023/12/26/mri2/&quot;&gt;T1 강조 영상(weighted image; WI), T2WI, FLAIR 등 다양한 MRI&lt;/a&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/mr_axis.png?raw=true&quot; style=&quot;zoom: 40%;&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;T1 relaxation time과 T2 relaxation time은 양성자(proton)에 RF pulse를 90도로 가한 후에 각각 다른 기준에 의해 측정된다. proton의 자기화를 종축($Mz$)에서 횡축으로 눕히면 $Mxy$ vector가 형성된다. $Mz$ vector가 0%, $Mxy$ vector가 100%가 되는 순간부터 T1, T2 relaxation time을 측정한다. 조직마다 두 relaxation time 모두 상이하게 나타난다.&lt;/p&gt;

&lt;p&gt;T1 relaxation time은 $Mz$가 63%까지 회복하는 데에 걸리는 시간이다. 지방(fat) - 뇌세포 조직(brain tissue) - 뇌척수액(cerebrospinal fluid; CSF) 순으로 회복이 빠르다(T1 relaxation time이 빠르다).&lt;/p&gt;

&lt;p&gt;T2 relaxation time은 $Mxy$가 37%까지 감소하는 데에 걸리는 시간으로, 자장의 세기에 별로 영향을 받지 않는다. Fat - brain tissue - CSF 순으로 신호가 빨리 감소한다.&lt;/p&gt;

&lt;p&gt;T1 relaxation time이 짧은 조직은 T2 curve도 급격히 감소한다. 물과 지방은 T1과 T2에서 반대의 intensity를 갖는다 (opposite signal intensity).&lt;/p&gt;

&lt;p&gt;Spin echo는 RF pulse를 90도와 180도로 주어 repetition time (TR)과 echo time (TE)을 조작하며 영상을 찍는 기법이다. TR은  90도 pulse에서 다음 90도 pulse까지의 시간, TE은 90도 pulse에서 신호를 얻을 때까지의 시간을 말한다. 영상 촬영 시 pulse를 여러 번 반복하므로, TR과 TE를 조작하여 &lt;a href=&quot;https://alatteaday.github.io/ko/study/2023/12/26/mri2/&quot;&gt;다양한 영상&lt;/a&gt;을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mri의-장단점&quot;&gt;MRI의 장단점&lt;/h2&gt;

&lt;p&gt;장점&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CT에 비해 연부 조직의 contrast가 더 잘 드러난다.&lt;/li&gt;
  &lt;li&gt;해부학적, 생리학적, 기능적 정보 등을 관찰할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;단점&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;철자성 인공허상(ferromagnetic artifacts): 자장에 영향을 주는 금속물질 등이 체내 소량이라도 존재하면 자장의 균질성이 깨져 영상이 왜곡된다.&lt;/li&gt;
  &lt;li&gt;금니나 기타 삽입 물질이 있으면 영상의 질이 떨어진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;금기: 자성에 영향을 받을 수 있는 신체 내 삽입물 등을 가진 환자에게는 사용하지 말아야 한다.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (NIPS 2020)</title>
   <link href="https://alatteaday.github.io/ko/papers/2023/11/05/rag/"/>
   <updated>2023-11-05T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/11/05/rag</id>
   <content type="html">&lt;p&gt;Lewis, Patrick, et al. “Retrieval-augmented generation for knowledge-intensive nlp tasks.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 33 (2020): 9459-9474.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Retrieval Augmented Generation (RAG)&lt;/strong&gt; 모델은 retriever와 generator를 결합한 구조로 knowledge-intense task 수행 능력이 향상되었다.&lt;/li&gt;
  &lt;li&gt;RAG Variants: RAG-Sequence는 단일 문서를 사용해 output을 생성하고, RAG-Token 각 토큰을 생성하는 데 여러 문서를 통합한다.&lt;/li&gt;
  &lt;li&gt;RAG 모델은 open-domain QA, abstractive QA, Jeopardy question generation, and fact verification에서 baseline 모델을 능가하는 결과를 보였다.&lt;/li&gt;
  &lt;li&gt;RAG 모델은 non-parametric memory를 업데이트하는 간단한 방법으로 최신 자료를 쉽게 반영할 수 있는 실용적인 이점을 갖는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Large pre-trained Language models (LLMs)은 자체 parameter에 사실 관련 지식을 저장하고, 이것을 지식 base로 사용한다.&lt;/li&gt;
  &lt;li&gt;이러한 LLM은 자체적으로 이러한 지식을 담고 있는 메모리를 확장할 수 없고, 생성되는 output에 사실적인 통찰을 반영하거나 보장하기 어려우며, 나아가 hallucination 생성 가능성이 높다는 단점을 갖는다.&lt;/li&gt;
  &lt;li&gt;최근 REALM과 ORQA 같은 hybrid 모델은 미분 가능한 retriever를 사용하여 지식을 수정하고 확장함으로서 이런 문제를 해결한다. 이것으로 특히 open-domain question answering (QA)에서 좋은 결과를 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig1.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Retrieval-augmented generation (RAG)은 pre-trained generation 모델이 non-parametric memory를 사용해 보편적인 task를 수행할 수 있도록 fine-tuning 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Parametric memory: pre-trained seq2seq transformer&lt;/li&gt;
  &lt;li&gt;Non-parametric memory: Wikipedia로부터 pre-trained neural retriever를 통해 얻는 dense vector index.&lt;/li&gt;
  &lt;li&gt;Dense passage retriever (DPR): input을 조건으로 latent document를 검색하는 retriever.&lt;/li&gt;
  &lt;li&gt;BART: input과 latent document를 조건으로 output을 생성하는 generator. T5 등 다른 seq2seq 모델로 대체 가능하다. Retriever와 함께 fine-tuning 된다.&lt;/li&gt;
  &lt;li&gt;Latent document: top-K 근사를 통해 output(시퀀스) 별 또는 토큰 별로 marginalizing 된다.
    &lt;ul&gt;
      &lt;li&gt;RAG-Sequence Model: 하나의 문서가 모든 토큰의 출처가 된다고 가정한다.&lt;/li&gt;
      &lt;li&gt;RAG-Token Model: 여러 문서가 한 토큰을 생성하는 데 출처가 된다고 가정한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;RAG 모델은 input 시퀀스 $x$를 사용해 text document $z$를 검색하고, 이를 target 시퀀스 $y$를 생성할 때 추가적인 문맥으로 사용한다. RAG 두 가지 구성 요소로 이루어진다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Retriever $p_\eta(z\mid x)$: 쿼리인 $x$에 대한 문서들의 분포를 반환한다.
    &lt;ul&gt;
      &lt;li&gt;Truncated as top-K assumtion.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generator $p_\theta(y_i\mid x,z,y_{1:i-1})$: 이전 토큰들 $y_{1:i-1}$과 현재 input $x$ 및 검색된 내용 $z$을 기반으로 현재 스텝의 토큰을 생성한다.
Retriever와 Generator는 검색된 문서를 latent variable로 취급하여 end-to-end로 학습된다. latent document를 marginalize 하기 위한 방법으로 RAG-Sequence와 RAG-Token 두 가지 방법이 제안되었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;rag-sequence-and-rag-token&quot;&gt;RAG-Sequence and RAG-Token&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;RAG-Sequence Model&lt;/strong&gt;: 검색된 동일한 문서를 사용해 전체 시퀀스를 생성한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;검색된 문서는 top-k seq2seq 확률 $p(y\mid x)$을 얻기 위한 단일 latent variable로 간주된다.&lt;/li&gt;
  &lt;li&gt;Retriever로 top-K 문서를 검색하고, Generator로 각 문서에 대해 output 시퀀스에 대한 확률을 계산한다.&lt;/li&gt;
&lt;/ul&gt;

\[p_{RAG-Sequence}(y\mid x)\approx \sum_{z\in top-k(p(\cdot|x))}{p_\eta(z|x)p_\theta(y_i|x,z)} \\ = \sum_{z\in top-k(p(\cdot|x))}{p_\eta(z|x)}\prod_i^N p_\theta(y_i|x,z,y_{1:i-1})\]

&lt;ul&gt;
  &lt;li&gt;Use cases: 요약과 같은 문서의 전체적인 맥락이 중요한 작업에 적합하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RAG-Token Model&lt;/strong&gt;: 각 토큰을 생성할 때 다른 latent document를 사용한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generator는 여러 문서에서 내용을 추출하여 output을 생성한다.&lt;/li&gt;
  &lt;li&gt;Retriever는 top-K 문서를 검색하고 Generator는 각 문서에 대해 다음 output 토큰에 대한 분포를 계산한다.&lt;/li&gt;
&lt;/ul&gt;

\[p_{RAG-Token}(y|x)\approx \prod_i^N \sum_{z\in top-k(p(\cdot\mid x))}p_\eta(z\mid x)p_\theta(y_i\mid x,z_i,y_{1:i-1})\]

&lt;ul&gt;
  &lt;li&gt;Use cases: QA와 같이 여러 자료를 출처로 상세한 정보를 통합하는 작업에 적합하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;retriever-and-generator&quot;&gt;Retriever and Generator&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Retriever&lt;/strong&gt; $p_\mu(z\mid x)$: bi-encoder 구조를 갖는 DPR을 기반으로 한다:&lt;/p&gt;

\[p_\mu(z|x)\propto \exp(\bf d \rm (z)^\top \bf q \rm (x)) \\
\bf d \rm (z)=\rm BERT_d(z), \ \bf q \rm (x)=\rm BERT_q(x)\]

&lt;ul&gt;
  &lt;li&gt;$\bf d \rm (z)$: 문서에 대한 representation. $\rm BERT_{BASE}$ 기반 document encoder가 생성한다.&lt;/li&gt;
  &lt;li&gt;$\bf q \rm (x)$: 쿼리에 대한 representation. $\rm BERT_{BASE}$ 기반 query encoder가 생성한다.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Maximum inner product search (MIPS)&lt;/span&gt;: top-k $p_\eta(\cdot\mid x)$를 sub-linear에 근접한 시간 내 계산하는 방법&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Non-parametric memory&lt;/span&gt;: 문서의 인덱스. Retriever는 TriviaQA의 질문과 Natural Questions에 대해 답하는 데 필요한 문서를 검색하도록 학습된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Generator&lt;/strong&gt; $p_\theta(y_i\mid x,z,y_{1:i-1})$: 이 연구에서는 BART를 기반으로 하나, 어떤 encoder-decoder 모델로도 대체될 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\rm BART_{large}$: 400M 파라미터를 갖는 pre-trained seq2seq transformer. 다양한 noising function과 denoising objective로 pre-training 되었다.&lt;/li&gt;
  &lt;li&gt;Input $x$와 검색된 문서 $z$를 concatenate 하여 $\rm BART$ 모델에 입력하고 output을 생성한다.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Parametric memory&lt;/span&gt;: $\rm BART$ generator의 파라미터 $\theta$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;p&gt;Retriever와 generator는 어떤 문서를 검색해야 하는지에 대한 정답 없이 학습된다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Objective: input/output 쌍 $(x_j, y_j)$의 negative marginal log-likelihood 최소화, $\sum_j-\log(p(y_j\mid x_j))$.
    &lt;ul&gt;
      &lt;li&gt;Adam optimizer 사용.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Query encoder $\rm BERT_q$와 generator $\rm BART$만 finetune 한다.
    &lt;ul&gt;
      &lt;li&gt;Document encoder $\rm BERT_d$ 업데이트는 비용이 많이 들고 비효율적이다.
        &lt;ul&gt;
          &lt;li&gt;Document index를 주기적으로 업데이트해야 한다.&lt;/li&gt;
          &lt;li&gt;성능 향상에 그다지 유의미하지 않다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;decoding&quot;&gt;Decoding&lt;/h2&gt;

&lt;p&gt;테스트 시, RAG-Sequence와 RAG-Token은 $\arg \max_y{p(y\mid x)}$를 근사하기 위한 서로 다른 방법을 필요로 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RAG-Sequence model&lt;/strong&gt;: 각 문서 $z$에 대해 beam search를 사용한다. $p(y\mid x)$은 기존 보편적인 토큰별 확률로 분해할 수 없어 단일 beam search로 구할 수 없다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각 $z$의 hypothesis는 $p_\theta(y_i\mid x,z,y_{1:i-1})$로 점수가 매겨진다.&lt;/li&gt;
  &lt;li&gt;hypothesis 집합 $Y$에 포함된 일부 $y$는 모든 문서의 beam에서 나타나지 않을 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Thorough Decoding&lt;/span&gt;: $y$의 확률을 추정하기 위해, $y$가 beam에 나타나지 않는  $z$ 각각에 대해 추가적으로 forward pass를 진행하고, generator의 확률을 $p_\eta(z\mid x)$와 곱한 뒤, beam의 확률을 더한다.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Fast Decoding&lt;/span&gt;: 후보 집합 $Y$가 생성되었을 때, forward pass를 방지하기 위해 $p_\theta(y\mid x,z_i) \approx 0$로 근사해 효율적으로 decoding을 수행한다. 이 방법은  $x, z_i$에서 beam search를 했을 때 $y$가 생성되지 않은 경우 유효하다.&lt;/li&gt;
  &lt;li&gt;긴 output 시퀀스를 생성하는 경우, $\left\vert Y \right\vert$가 여러 forward pass를 수행하며 커질 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RAG-Token model&lt;/strong&gt;: transition 확률을 갖는 기본적인 autoregressive seq2seq generator와 같은 방식으로 작동한다:&lt;/p&gt;

\[p&apos;_\theta(y_i\mid x,y_{1,i-1})=\sum_{z\in top-k(p(\cdot \mid x))}p_\eta(z_i \mid x)p_\theta(y_i\mid x,z_i,y_{1:i-1})\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;RAG 모델이 knowledge-intensive NLP task에서 효과적인지에 대한 성능을 평가하기 위해 다양한 task를 설정했다. 모델 관련 setting 사항은 다음과 같다:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Wikipedia December 2018 dump를 non-parametric knwoledge source로 사용했다.&lt;/li&gt;
  &lt;li&gt;Wikipedia 문서를 100 단어씩 한 chunk로 나누어 총 2,100만 개 문서로 구성했다.&lt;/li&gt;
  &lt;li&gt;Document encoder $\rm BERT_d$로 각 문서에 대한 임베딩을 구하고, 빠른 검색을 위해 Hierarchical Navigable Small World approximation을 사용해 단일 MIPS 인덱스를 구축했다.&lt;/li&gt;
  &lt;li&gt;각 쿼리에 대해 top $k$개의 문서를 검색할 때, $k\in {5,10}$로 설정하여 학습과 테스트 시 반영했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Open-domain Question Answering (QA)&lt;/strong&gt;: 중요한 real-world application이자 보편적인 knowledge-intensive task이다.
    &lt;ul&gt;
      &lt;li&gt;텍스트 쌍 $(x,y)$는 질문과 답변에 매칭된다.&lt;/li&gt;
      &lt;li&gt;RAG는 답변 생성 시 negative log-likelihood를 최소화하도록 학습된다.&lt;/li&gt;
      &lt;li&gt;Close-book QA를 통한 비교도 진행: 검색 없이 오로지 모델에 내제된 parametric knowledge로 시퀀스를 생성한다.&lt;/li&gt;
      &lt;li&gt;Datasets: Natural Questions, TriviaQA, WebQuestions, CuratedTREC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Abstractrive Question Answering&lt;/strong&gt;: 자유 형식이나 추상적인 경우에서의 natural language generation (NLG) 성능을 테스트한다.
    &lt;ul&gt;
      &lt;li&gt;MSMARCO NLG Task v2.1 사용: 원래 존재하는 gold passage는 배제하고, 질문과 답변만 사용하여 open-domain abstractive QA task를 구성했다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Jeopardy Question Generation&lt;/strong&gt;: QA 상황이 아닌 경우의 생성 능력을 평가한다.
    &lt;ul&gt;
      &lt;li&gt;Jeopardy: 특정 entity에 대한 사실을 보고 entity를 추측하는 것.
        &lt;ul&gt;
          &lt;li&gt;예를 들어, “1986년 멕시코는 이 국제 스포츠 대회를 두 번 개최한 첫 번째 국가로 기록되었다.”라는 사실을 보고, 이 사실에 해당하는 entity인 “월드컵”을 추측해야 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;정확하고 사실적인 성격이 강한 task로, 답변인 entity를 조건으로 하여 생성하는 부분이 까다로운 knowledge-intensive task이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fact Verification&lt;/strong&gt; (FEVER): 고난이도의 함의 추론과 결합된 검색 문제이다.
    &lt;ul&gt;
      &lt;li&gt;텍스트가 Wikipedia에 따라 맞는 내용인지, 틀렸는지, 또는 판단할 충분한 정보가 없는지를 분류해야 한다.&lt;/li&gt;
      &lt;li&gt;모델의 생성 능력이 아닌 분류 능력을 테스트하기에 적절하다.&lt;/li&gt;
      &lt;li&gt;Two varients: 3-way classification (supports/refutes/not enough)과 2-way (support/refutes).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;RAG 모델은 여러 task에서 baseline 모델 이상의 성능을 보였다.&lt;/p&gt;

&lt;h2 id=&quot;open-domain-qa&quot;&gt;Open-Domain QA&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table1_2.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RAG 모델은 baseline을 크게 능가하는 점수를 기록했다.&lt;/li&gt;
  &lt;li&gt;특히 RAG-Token 모델은 여러 문서의 세세한 정보를 통합하는 능력에 의해 우수한 성능을 보였다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstractive-question-answering&quot;&gt;Abstractive Question Answering&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table3.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다수의 질문이 gold passage 없이 대답할 수 없었음에도 불구하고 RAG 모델이 SOTA 성능을 달성했다.&lt;/li&gt;
  &lt;li&gt;RAG 모델은 BART에 비해 hallucination을 적게 일으켰고 사실적으로 정확하면서도 다양한 텍스트를 생성했다 (Table 3).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;jeopardy-question-generation&quot;&gt;Jeopardy Question Generation&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table4_5.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig2.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;두 RAG model 모두 Q-BLEU-1에서 BART를 능가했다 (Table 2).&lt;/li&gt;
  &lt;li&gt;Human evaluator는 RAG가 생성한 결과가 42.7%의 경우에서 더 사실적이라고 평가해, SOTA인 generation 모델보다 더 나은 생성 능력을 입증했다 (Table 4).&lt;/li&gt;
  &lt;li&gt;RAG-Token 모델이 RAG-Sequence 모델보다 성능이 더 좋게 나타났는데, 여러 문서의 내용을 효과적으로 결합하기 때문인 것으로 생각된다 (Fig 2).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fact-verification&quot;&gt;Fact Verification&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;3-way classification에서 RAG는 특정 도메인에 대한 중간 검색에 대해 지도학습된 SOTA 모델 점수의 4.3% 범위 내의 점수를 기록했다.&lt;/li&gt;
  &lt;li&gt;2-way classification에서는 gold evidence를 기반으로 true/false classification을 학습한 SotA 모델과 비교해 2.7% 이내의 성능을 달성했다.&lt;/li&gt;
  &lt;li&gt;RAG가 검색한 문서는 FEVER의 gold evidence와 상당 부분 겹쳤다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;additional-results&quot;&gt;Additional Results&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generation Diversity&lt;/strong&gt;: 서로 다른 모델이 생성한 ngram의 비율을 계산하여 output의 다양성을 조사한 결과, RAG 모델이 BART보다 더 다양한 output을 생성했다. RAG-Token보다는 RAG-Sequence의 output이 더 다양했다 (Table 5).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Retrieval Ablations&lt;/strong&gt;: 학습 중 retriever를 freeze했을 때 원래 방식의 RAG 모델에 비해 성능이 하락했다. Retriever를 BM25로 대체하여 비교했을 때도 학습된 retriever의 성능이 더 높은 것을 볼 수 있었다 (table 6).&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table6.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Index hot-swapping&lt;/strong&gt;: December 2016 Wikipedia dump의 인덱스를 사용하여 non-parametric memory의 이점을 입증했다. RAG 모델은 인덱스가 바뀌었음에도 질문의 70%를 맞게 대답했다. 이로서 non-parametric memory를 단순히 교체하여 knowledge를 업데이트할 수 있음을 알 수 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Effect of Retrieving more documents&lt;/strong&gt;: 테스트 시 검색될 문서의 수 $k$를 조정한 결과, task에 따라서 특정 개수까지의, 혹은 많은 문서를 검색할수록 성능이 향상되는 것을 볼 수 있다 (fig 3).&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig3.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series (ACM 2022)</title>
   <link href="https://alatteaday.github.io/ko/papers/2023/09/15/STraTS/"/>
   <updated>2023-09-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/09/15/STraTS</id>
   <content type="html">&lt;p&gt;Tipirneni, Sindhu, and Chandan K. Reddy. “Self-supervised transformer for sparse and irregularly sampled multivariate clinical time-series.” &lt;em&gt;ACM Transactions on Knowledge Discovery from Data (TKDD)&lt;/em&gt; 16.6 (2022): 1-17.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/full/10.1145/3516367&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;points&quot;&gt;Points&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Self-supervised Transformer for Time-Series (STraTS) model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using observation triplets as time-series components: avoids the problems faced by aggregation and imputation methods for sparse and sporadic multivariate time-series&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continuous Value Embedding: encodes continuous time and variable values without the need for discretization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transformer-based model: learns contextual triplet embeddings&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Time series forecasting as a proxy task: leverages unlabeled data to learn better generalized representations&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Problems&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals.&lt;/li&gt;
  &lt;li&gt;Existing approaches, such as aggregation or imputation of values, suppress the fine-grained information and add undesirable noise/overhead into the model.&lt;/li&gt;
  &lt;li&gt;The problem of limited availability of labeled data is easily observed in healthcare applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The clinical domain portrays a unique set of challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Missingness and Sparsity: Not all the variables are observed for every patient. Also, the time-series matrices are very sparse.&lt;/li&gt;
  &lt;li&gt;Irregular time intervals and Sporadicity: Not all clinical variables are measured at regular time intervals. The measurements may occur sporadically in time depending.&lt;/li&gt;
  &lt;li&gt;Limited labeled data: expensive and even more limited for specific tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Existing methods&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Aggregation: could suppress important fine-grained information&lt;/li&gt;
  &lt;li&gt;Imputation/Interpolation: not reasonable as not considering the domain knowledge about each variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Self-supervised Transformer for Time-Series (STraTS)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/1_fig3.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Triplet Emgeddings&lt;/strong&gt; = Feature embedding + Value embedding + Time embedding
\(T=\{(t_i, j_i, u_i)\}^n_{i=1}\\
e_i=e_i^f+e_i^v+e_i^t\)
&lt;strong&gt;Continuous Value Embedding (CVE)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For continuous values of feature values and times&lt;/p&gt;

&lt;p&gt;A one-to-many Feed-Forward Network
\(FFN(x) = U tanh(Wx+b)\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature embeddings $e_i^f$: obtained from a simpole lookup table&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Value embeddings $e_i^v$ and Time embeddings$e_i^t$: through CVE&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Demographics Embedding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;the prediction models performed better when demographics were processed separately.
\(e^𝑑 = 𝑡𝑎𝑛ℎ(W^𝑑_2𝑡𝑎𝑛ℎ(W^𝑑_1d + b^𝑑_1) + b^𝑑_2) ∈ R^d\)
where the hidden layer has a dimension of 2d&lt;/p&gt;

&lt;h3 id=&quot;self-supervision&quot;&gt;Self-Supervision&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/2_fig2.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pre-training Tasks: Both masking and forecasting as pretext tasks for providing self-supervision&lt;/p&gt;

&lt;p&gt;The forecasitng improved the results on target tasks&lt;/p&gt;

&lt;p&gt;The loss is:
\(L_{ss}=\frac{1}{|N&apos;|}\sum_{k=1}^{N&apos;}\sum_{j=1}^{|F|}m_j^k\Big(\tilde{z}_j^k-z_j^k\Big)^2\)&lt;/p&gt;

&lt;h3 id=&quot;interpretability&quot;&gt;Interpretability&lt;/h3&gt;

&lt;p&gt;I-STraTS: an interpretable version of STraTS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The output can be expressed using a linear combination of components that are derived from individual features&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Differences with STraTS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combine the initial triplet embeddings in Fusion Self-attention module&lt;/li&gt;
  &lt;li&gt;Directly use the raw demographics vector as the demographics embedding&lt;/li&gt;
&lt;/ul&gt;

\[\tilde{y}=sigmoid\Big(\sum_{j=1}^{D}{\bold{w}_0[j]d[j]+\sum_{i=1}^{n}\sum_{j=1}^{d}\alpha_i\bold{w}_o[j+D]\bold{e}_i[j]+b_o}\Big)\]

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;Target Task: Prediction of in-hospital mortality&lt;/p&gt;

&lt;p&gt;Datasets: 2 EHR datasets; MIMIC-III and PhysioNet Challenge 2012&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MIMIC-III: 46,000 patients&lt;/li&gt;
  &lt;li&gt;PhysioNet-2012: 11,988 patients&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Baselines: Gated Recurrent Unit (GRU), Temporal Convolutional Network (TCN), Simply Attend and DIagnose (SaND), GRU with trainable Decays (GRU-D), Interpolation-prediction Network (InterpNet), Set Functions for Time Series (SeFT)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Used 2 dense layers for demographics encoding&lt;/li&gt;
  &lt;li&gt;Concatenated it to the time-series representation before the last dense layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Metrics&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ROC-AUC: Area under ROC curve&lt;/li&gt;
  &lt;li&gt;PR-AUC: Area under precision-recall curve&lt;/li&gt;
  &lt;li&gt;min(Re, Pr): the max of ‘min of recall and precision’ across all thresholds&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prediction-performance&quot;&gt;Prediction Performance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/3_table4.png?raw=true&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trained each model using 10 different random samplings of 50% labeled data from the train and validation sets&lt;/li&gt;
  &lt;li&gt;STraTS uses the entire labeled data and additional unlabeled data if avaliable&lt;/li&gt;
  &lt;li&gt;STraTS achieves the best performance&lt;/li&gt;
  &lt;li&gt;GRU showed better performance than interpolation-based models (GRU-D, InterpNet) on the MIMIC-III dataset, which was not expected&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Generalizability test of models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Lower propotions of labeled data can be observed in real-world when there are several right-censord samples.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/4_fig5.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;STraTS has an advantage compared to others in scarce labeled data settings, which can be attributed to self-supervision&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h3&gt;

&lt;p&gt;Compared STraTS and I-STraTS with and without self-supervision: ‘ss+’ and ‘ss-‘ indicate each case&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/5_table5.png?raw=true&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I-STraTS showed slightly worse performance as constrained its representations&lt;/li&gt;
  &lt;li&gt;Adding self-supervision improves performance of both models&lt;/li&gt;
  &lt;li&gt;I-STraTS(ss+) outperforms STraTS(ss-): self-supervision can compensate the performance which could get lower by introducing interpretability&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;interpretability-1&quot;&gt;Interpretability&lt;/h3&gt;

&lt;p&gt;How I-STraTS explains its predictions&lt;/p&gt;

&lt;p&gt;A case study: a 85 yrs old female patient from MIMIC-III&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;expired on the 6th day after ICU admission&lt;/li&gt;
  &lt;li&gt;had 380 measurements corresponding to 58 time-series variables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model predicts the probability of her in-hospital mortality as 0.94 using only the data collected the first day&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/6_table6.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Average contribution score: the average score along with the range, for multiple observations, or value, for only one observation&lt;/li&gt;
  &lt;li&gt;The top 5 variables are the most important factors in predicting she ‘s at high risk of mortality that the model observed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;amp;rarr Can be helpful to identify high-risk patients and also understand the contributing factors and make better diagnoses, especially at the early stages of treatment&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Study] 알츠하이머 치매의 ATN 바이오마커 간 관계 정리</title>
   <link href="https://alatteaday.github.io/ko/study/2023/08/29/demensiapapers/"/>
   <updated>2023-08-29T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2023/08/29/demensiapapers</id>
   <content type="html">&lt;h2 id=&quot;ai-전공자의-알츠하이머-치매-관련-brain-imaging-논문-스터디&quot;&gt;AI 전공자의 알츠하이머 치매 관련 Brain Imaging 논문 스터디&lt;/h2&gt;

&lt;p&gt;Amyloid Beta(A), Tau(T), Neurodegeneration(N)과 관련된 Alzheimer’s Disease(AD) 기전에 대해 이해하기 위하여 다음의 논문들을 읽고 정리한 내용입니다. 기반 지식이 없어 시각 자료와 사전을 찾아가며 읽었습니다. pdf는 찾아본 이미지와 필기한 내용이 담긴 논문 파일입니다.&lt;/p&gt;

&lt;p&gt;Ittner, Lars M., and Jürgen Götz. “Amyloid-β and tau—a toxic pas de deux in Alzheimer’s disease.” &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 12.2 (2011): 67-72. &lt;a href=&quot;https://www.nature.com/articles/nrn2967&quot;&gt;link&lt;/a&gt; &lt;a href=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-08-29-demensiapapers/nrn2967.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Vogel, Jacob W., et al. “Four distinct trajectories of tau deposition identified in Alzheimer’s disease.” &lt;em&gt;Nature medicine&lt;/em&gt; 27.5 (2021): 871-881. &lt;a href=&quot;https://www.nature.com/articles/s41591-021-01309-6&quot;&gt;link&lt;/a&gt; &lt;a href=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-08-29-demensiapapers/s41591-021-01309-6.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lee, Wha Jin, et al. “Regional Aβ-tau interactions promote onset and acceleration of Alzheimer’s disease tau spreading.” &lt;em&gt;Neuron&lt;/em&gt;110.12 (2022): 1932-1943. &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/35443153/&quot;&gt;link&lt;/a&gt; &lt;a href=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-08-29-demensiapapers/regional_onset.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Amyloid Beta(A)는 뉴런에 의해 생성되는 Amyloid Precursor Protein(APP)이 프로테아제에 의해 4부분으로 나눠질 때 생기는 펩타이드 중 하나로,&lt;/p&gt;

&lt;p&gt;뉴런 근처에 존재하여 기능 장애를 야기하는 것으로 알려졌다. A의 침착은 Alzheimer’s Disease(AD) 발병 10-20년 전부터 이뤄진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A는 dimers, oligomers, fibrils 등에 이어 plaque를 형성한다. A가 어느 형태에서 toxicity를 갖기 시작하는지는 확실하지 않다. 항 아밀로이드 치매 치료제는 이 plaque의 감소와 증식 및 생성 방지를 목적으로 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A의 toxicity는 postsynaptic compartment, 즉 dendrite(somatodendritic region)를 주 대상으로 하여 작용하고, 특정 수용체의 속성에 따라 세포막을 통해 간접적으로 뉴런에 영향을 끼칠 수 있다. 대표적인 특정 수용체로 NMDAR이 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tau(T)는 신경 세포에서 microtubule과 결합하는 단백질로, 주로 axon에 존재하여 microtubule의 안정화 및 axonal transition을 조절하는 역할을 한다.&lt;/p&gt;

&lt;p&gt;정상 상태의 뉴런의 dendrite에도 소량 존재한다.&lt;/p&gt;

&lt;p&gt;T는 A에 의해 과인산화되고(hyperphosphorylated Tau), 과인산화된 T는 Neurofibrillary Tangle(NFT)를 형성한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;T의 과인산화는 microtubule 형성을 방해하여 뉴런의 기능을 방해한다.&lt;/li&gt;
  &lt;li&gt;NFT는 Somatodendritic region에서 많이 관찰된다. T의 level이 높아지면 T가 dendrite에서 많이 관찰된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dendrite에서 T는 그곳에 위치한 여러 단백질과 상호작용하여 결과적으로 뉴런이 A의 toxicity에 약해지게 만든다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;T가 인산화되면 Tyrosine protein kinasen FYN과 강하게 작용한다. 과인산화된 T가 dendrite에서 증가함에 따라 FYN도 Soma에서 증가한다.&lt;/li&gt;
  &lt;li&gt;FYN은 NMDAR을 인산화한다. 인산화된 NMDAR은 Postsynaptic Density Protein 95(PDS95)와 상호작용한다.&lt;/li&gt;
  &lt;li&gt;이것의 결과로 NMDAR의 excitotoxicity가 나타난다(흥분독성상태). 수용체의 excitotoxicity로 A의 toxicity에 뉴런이 민감해지게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결과적으로 A와 T는 뉴런을 약화시키는 데에 있어 서로 시너지를 갖는다. A는 T의 과인산화를 촉진하고, 과인산화된 T는 뉴런이 A의 toxicity에 약해지게 만든다.&lt;/p&gt;

&lt;p&gt;이 시스템에서 A와 T는 세포의 다른 부분(각각 Complex I, Complex IV)에 악영향을 끼쳐 미토콘드리아 호흡을 방해하고, 결국 Neurodegeneration(N)을 야기한다.&lt;/p&gt;

&lt;p&gt;따라서 A의 침착과 T의 전파는 AD의 중요한 요인이다.&lt;/p&gt;

&lt;p&gt;T의 전파 양상은 Braak Staging System으로 체계화된 바 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transentorhinal cortex → medial and basal temporal lobe → neocortical associative regions → unimodal sensory and motor cortex&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그런데 이 system에 부합하지 않는 전파 양상 또한 관찰되었다. T의 전파 양상을 병의 진행과 뇌 영역의 시공간적 기준으로 분류하여 4가지 subtype으로 정의할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S1 limbic (Braak system), S2 MTL, S3 posterior, S4 Lateral Temporal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;침착된 A는 T의 전파에 영향을 준다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A는 heteromodal association cortex에 침착되고, T의 전파는 entorhinal cortex(EC)에서 시작되어 점차 뇌 전반으로 퍼진다. ( ← Braak system; S1 type ? )&lt;/li&gt;
  &lt;li&gt;Remote Interation: A와 T가 같은 영역에 있지 않은 상태에서, 먼저 A가 연결된 뉴런을 통해서 EC 영역에 있는 T에 영향을 준다. A의 영향으로 T는 점차 주위 영역으로 확산된다.&lt;/li&gt;
  &lt;li&gt;Local Interaction: T가 A와 직접적으로 접촉되어 있는 뉴런에 전파되어 만남으로서 T의 전파가 가속화된다(acceleration). 해당 뇌 영역은 Internal Temporal Gyrus(ITG)이다 (propagation hub).&lt;/li&gt;
  &lt;li&gt;T 전파의 acceleration이 진행되면 뇌 전반에서 A와 T의 상호작용이 일어나게 되어 N과 AD의 악화를 막기 어렵다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A와 T의 PET 데이터와 MRI 데이터를 병의 진행에 따라 살펴보면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A의 침착되는 정도는 뇌 전반에 걸쳐 점점 심해질 것이고&lt;/li&gt;
  &lt;li&gt;T는 뇌의 특정 부분에서 시작하여 점차 확산되는 양상으로 관찰되고&lt;/li&gt;
  &lt;li&gt;T의 슈퍼 전파가 관찰된 이후 MRI 상 전반적인 뇌 위축(N)의 정도가 심하게 나타날 것이다.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Recurrent Neural Networks (RNNs)</title>
   <link href="https://alatteaday.github.io/ko/study/2023/05/20/rnn/"/>
   <updated>2023-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2023/05/20/rnn</id>
   <content type="html">&lt;p&gt;Sequence data를 분석하기 위한 딥러닝 모델 구조로, Rumelhart et al., 1986에 근간을 둔다. Deep neural networks (DNNs)와는 달리 hidden state node 간 연결을 통해 이전 시점의 정보를 현재 시점에서 사용할 수 있게 디자인되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn.jpg?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;현재 시점 node인 $s_t$에 전 시점의 $s_{t-1}$ node에서 정보가 들어온다. 이 정보를 현재 시점의 입력인 $x_t$와 함께 받아 다음 node $s_{t+1}$로 전송될 값을 계산한다. 이 작업을 회귀적으로(recurrently) 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;weight-sharing&quot;&gt;Weight sharing&lt;/h2&gt;

&lt;p&gt;Weight $U$, $W$, $V$는 모든 시점에서 동일하다. 이것으로&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;학습에 필요한 weight 수를 줄일 수 있다.&lt;/li&gt;
  &lt;li&gt;데이터의 sequence 길이에 유연하다: 하나의 모델을 다른 길이의 sequence에 적용할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;다른 길이의 sequence에 같은 weight값을 계속 사용함으로써 next token generation이 가능하다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;rnn-계산&quot;&gt;RNN 계산&lt;/h2&gt;

&lt;p&gt;위 그림을 보면 hidden state $s_t$와 output $o_t$의 계산은 다음과 같다.&lt;/p&gt;

\[\begin{align*}
s_t&amp;amp;=\tau(Ws^{t-1})+Ux^t \\
o_t&amp;amp;=softmax(Vs^t) \\
\end{align*}\]

&lt;p&gt;여기서 node 수가 $D$, $J$, $K$인 경우 각 변수의 차원은 아래와 같다.&lt;/p&gt;

\[x\in\mathbb{R}^D, s\in\mathbb{R}^J, o\in\mathbb{R}^K, U\in\mathbb{R}^{J\times D}, W\in\mathbb{R}^{J\times J}, U\in\mathbb{R}^{K\times J}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;long-term-dependency-problem&quot;&gt;Long-term dependency problem&lt;/h2&gt;

&lt;p&gt;hidden state 연산은 다음과 같이 표현할 수 있다.&lt;/p&gt;

\[s^t=\tau(Ux^t+W\tau(Ux^{t-1}+Ws^{t-2}))\]

&lt;p&gt;$s$가 tanh activation function 내에서 중첩되는 것을 볼 수 있다. 이렇게 되면&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;feed forward 시 앞 단에서 입력된 정보가 점점 소실된다: tanh의 output은 $\tau(\cdot)\in(-1,1)$인데, 즉 tanh 연산의 중첩은 1보다 작은 값을 계속해서 곱하는 것과 같다. 이렇게 되면 앞에서 곱해진 값은 점점 작아진다.&lt;/li&gt;
  &lt;li&gt;back-propagation 시 기울기 소실(gradient vanishing) 혹은 폭발(explosion)의 문제가 생길 수 있다: tanh 함수에 의해 기울기가 0에 가깝게 되거나 너무 커지는 경우가 생긴다. 작은 gradient는 더 작아지고, 큰 gradient는 더 커진다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;*Gradient vanishing: back-propagation 시 반영되는 gradient 값이 layer를 지날 수록 소실되는 문제&lt;/p&gt;

&lt;p&gt;*Gradient explosion: gradient가 실제 값보다 증폭되어 loss 계산 시 정답과의 차이가 너무 커져, 업데이트에 과도하게 반영되는 문제&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;다양한-rnn-구조&quot;&gt;다양한 RNN 구조&lt;/h2&gt;

&lt;p&gt;입출력 형태에 따라 다양하게 RNN을 구성할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn_types.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One-to-One: hidden state가 1개인 모형, 기본적인 Neural network 구조&lt;/li&gt;
  &lt;li&gt;One-to-Many: 하나의 입력값을 받아 순차적으로 여러 개의 값(한 sequence)을 생성&lt;/li&gt;
  &lt;li&gt;Many-to-One: 한 sequence를 입력 받아 마지막에 하나의 값을 생성
    &lt;ul&gt;
      &lt;li&gt;e.g. sentence classification&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Many-to-Many: 한 sequence를 입력 받아 latent represenation을 구한 후 이것을 통해 sequence를 출력
    &lt;ul&gt;
      &lt;li&gt;e.g. machine translation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many-to-Many: 한 sequence의 매 token을 입력 받는대로 대응하는 token을 출력하여 한 seqeunce를 생성&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Long Short-term Memory (LSTM)</title>
   <link href="https://alatteaday.github.io/ko/study/2023/05/20/lstm/"/>
   <updated>2023-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2023/05/20/lstm</id>
   <content type="html">&lt;p&gt;Recurrent neural network (RNN)의 Long-term dependency 문제를 해결하고자 만들어진 프레임워크이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-lstm/lstm.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;핵심적인 아이디어는 이전 시점의 state 정보를 이후 state에 얼마나 반영할지를 결정하는 계산을 추가해주는 것이다. 이것을 위해 &lt;strong&gt;forget gate, input gate, output gate&lt;/strong&gt;의 3가지 Gate와 &lt;strong&gt;memory cell&lt;/strong&gt;이 추가 되었다.&lt;/p&gt;

&lt;p&gt;LSTM의 계산 방식과 비교하기 위해 RNN의 계산식을 되짚어보면 다음과 같다.&lt;/p&gt;

\[\begin{align*}
h_t&amp;amp;=\tau(Wh^{t-1}+Ux^t) \\
\hat{y}_t&amp;amp;=softmax(Vh^t) \\
\end{align*}\]

&lt;p&gt;이전 시점($t-1$)의 hidden state와 현재 시점($t$)의 input(둘 다 weighted)을 더하여 tanh를 통과시키면 현재 시점의 hidden state가 된다. 이것에 softmax를 취하면 output이 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lstm-계산식&quot;&gt;LSTM 계산식&lt;/h2&gt;

&lt;p&gt;LSTM은 RNN의 방식에 residual connection 구조에서 착안한 memory 기능을 더하여 long-term dependency 문제를 해결하고자 했다. Memory 기능을 위해 추가된 계산들은 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;forget-gate&quot;&gt;Forget gate&lt;/h3&gt;

&lt;p&gt;Forget gate $f$는 이전 시점의 정보를 얼마나 잊을지 결정하는 gate이다.&lt;/p&gt;

\[f_t=\sigma(W_fh_{t-1}+U_fx_t)\]

&lt;p&gt;이전 시점의 hidden state와 현재 시점의 input을 더한 뒤 sigmoid를 취한다. 이것이 이전 시점의 memory cell state에 곱해진다. sigmoid의 특성에 의해 1에 가까울수록 이전 정보가 이후 많이 반영된다.&lt;/p&gt;

&lt;h3 id=&quot;input-gate&quot;&gt;Input gate&lt;/h3&gt;

&lt;p&gt;Input gate $i$는 현재 시점의 input을 다음 시점에 얼마나 반영할지 결정하는 gate이다. 여기서 candidate $\hat(c)$라는 개념이 등장하는데, candidate는 이전 시점의 hidden state와 현재 시점의 input을 고려했을 때 현재의 정보가 어떠한지를 나타내는 cell state의 후보 격인 값이다. 계산 방식이 RNN의 hidden state와 동일하다. input gate 값과 candidate를 곱해 현재의 정보 상 input이 얼마나 반영되면 좋은지를 구하고, 이것을 최종적으로 cell state에 더한다.&lt;/p&gt;

\[\begin{align}
i_t&amp;amp;=\sigma(W_{in}h_{_t-1}+U_{in}x_{t})\\
\hat{C}_t&amp;amp;=\tau(W_{c}h_{t-1}+U_{c}x_t)
\end{align}\]

&lt;h3 id=&quot;memory-cell&quot;&gt;Memory cell&lt;/h3&gt;

&lt;p&gt;Memory cell (cell state)은 세 가지 gate와 함께 LSTM의 구현 목적을 위해 추가된 개념이다. 현재 시점의 cell state는 이전 시점의 cell state 및 현재 시점의 forget gate와 현재 시점의 input gate 및 candidate로 계산한다.&lt;/p&gt;

\[C_t=f_t*C_{t-1}+i_t*\hat{C}_t\]

&lt;p&gt;$*$는 pointwise operation&lt;/p&gt;

&lt;p&gt;이전 정보인 cell state와 현재 input을 얼마나 반영할지가 합해져 현재 시점의 cell state가 구해진다.&lt;/p&gt;

&lt;h3 id=&quot;output-gate&quot;&gt;Output gate&lt;/h3&gt;

&lt;p&gt;Output gate는 memory cell을 현재 시점의 hidden state에 얼마나 반영할지 결정한다.&lt;/p&gt;

\[\begin{align}
o_t&amp;amp;=\sigma(W_oh_{t-1}+U_ox_t) \\
h_t&amp;amp;=o_t\tau(C_t) \\
&amp;amp;=o_t\tau(f_t*C_{t-1}+i_t*\hat{C}_t) \\
\end{align}\]

&lt;p&gt;현재 시점의 hidden state는 이전 시점의 정보와 현재 시점의 input이 반영된 현재 시점의 cell state와 output gate의 결과값과 곱해져 최종 결정된다.&lt;/p&gt;

&lt;h3 id=&quot;output&quot;&gt;Output&lt;/h3&gt;

&lt;p&gt;최종 출력 $\hat{y}_t$은 RNN과 같이 계산된다.&lt;/p&gt;

\[\hat{y}_t=softmax(Vh_t)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lstm의-한계&quot;&gt;LSTM의 한계&lt;/h2&gt;

&lt;p&gt;LSTM은 cell state 도입을 통해 gradient vanishing 문제를 해결하고자 하였다. 하지만 RNN 구조를 기반으로 하고 있는 한 이 문제를 완벽하게 해결하기에 한계가 있다. 오히려 gate를 여러 개 사용하여 계산량이 증가하는 문제가 있다.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Alpaca: A Strong, Replicable Instruction-Following Model</title>
   <link href="https://alatteaday.github.io/ko/papers/2023/05/15/alpaca/"/>
   <updated>2023-05-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/05/15/alpaca</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://crfm.stanford.edu/2023/03/13/alpaca.html&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Alpaca aims to support academic research on instruction-following large language models (LLMs), addressing deficiencies like hallucinations, toxicity, and biases.&lt;/li&gt;
  &lt;li&gt;Uses the self-instruct approach to create an instruction-following dataset with text-davinci-003, costing under $500.&lt;/li&gt;
  &lt;li&gt;The LLaMA 7B model is fine-tuned using efficient techniques.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;LLMs trained through instruction-following, such as ChatGPT, have significantly impacted daily life. However, these models still face issues like generating misinformation, toxic content, and exhibiting social biases. To address these problems, academic research is essential. Closed-source models hinder this research, making it difficult to study instruction-following models.&lt;/p&gt;

&lt;p&gt;Alpaca is a model designed for academic research, fine-tuned from the LLaMA 7B model using 52k instruction-following data generated from OpenAI’s text-davinci-003. Commercial use of Alpaca is prohibitied by following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Non-commercial license: LLaMA&lt;/li&gt;
  &lt;li&gt;Data restrictions: Based on text-davinci-003 prohibiting competition with OpenAI&lt;/li&gt;
  &lt;li&gt;Deployment caution: Not designed with adequate safety mesuares for general use.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;training-recipe&quot;&gt;Training Recipe&lt;/h1&gt;

&lt;p&gt;To train a high-quality instruction-following model under an academic budget, two key challenges are addressed:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Strong pre-trained language model: LLaMA models&lt;/li&gt;
  &lt;li&gt;High-quality instruction-following data: Self-instruct method&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;self-instruct-method&quot;&gt;Self-instruct method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Seed set: 175 human-written instruction-following output pairs from self-instruct seed set.&lt;/li&gt;
  &lt;li&gt;Data generation: Prompting text-davinci-003 to generate more instructions using the seed set as examples.&lt;/li&gt;
  &lt;li&gt;Efficiency: Improved the self-instruct method, generating 52k unique instructions and outputs for less than $500 using the OpenAI API.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-15-alpaca/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-the-model&quot;&gt;Fine-tuning the model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Process: LLaMA models are fine-tuned with the generated instruction-following dataset using fully shared data parallel (FSDP) and mixed precision trianing.&lt;/li&gt;
  &lt;li&gt;Cost and time: Fine-tuning a 7B LLaMA model took 3 hours on eight 80GB A100s, costing less than $100 on most cloud compute providers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;preliminary-evaluation&quot;&gt;Preliminary Evaluation&lt;/h1&gt;

&lt;p&gt;Human evaluation was conducted on inputs from the self-instruct evaluation set. Key findings include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Comparison: Alpaca 7B vs. text-davinci-003&lt;/li&gt;
  &lt;li&gt;Performance: Alpaca wins 90 to 89 comparisons.
    &lt;ul&gt;
      &lt;li&gt;Given Alpaca’s smaller size and limited data, it performed similarly to text-davinci-003.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generation style: Alpaca’s outputs tend to be similar with text-davinci-003, and reflect the general style of the training dataset.&lt;/li&gt;
  &lt;li&gt;Evaluation limitation: The evaluation data’s limitations should be noted.&lt;/li&gt;
  &lt;li&gt;An interactive demo was released to gather further feedback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;known-limitiations&quot;&gt;Known Limitiations&lt;/h1&gt;

&lt;p&gt;Alpaca shares common deficiencies with LLMs, such as hallucinations, toxicity, and stereotypes. It struggles particularly with hallucination, sometimes producing well-written misinformation. Despite these issues, Alpaca provides a lightweight model for studying these deficiencies, aiding academic research.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;release&quot;&gt;Release&lt;/h1&gt;

&lt;p&gt;Released assets:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Demo: Interactive demo for evaluation&lt;/li&gt;
  &lt;li&gt;Data: 52k demonstrations used to fine-tune Alpaca&lt;/li&gt;
  &lt;li&gt;Data generation process: Code for generating the data&lt;/li&gt;
  &lt;li&gt;Training code: Fine-tuning code using Hugging Face API&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Future release:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Model weights: Pending guidance from Meta&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The release aims to support academic studies on instruction-following LMs and developing new technique to address the existing deficiencies.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] Llama: Open and efficient foundation language models (2023)</title>
   <link href="https://alatteaday.github.io/ko/papers/2023/05/10/llama/"/>
   <updated>2023-05-10T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/05/10/llama</id>
   <content type="html">&lt;p&gt;Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.” &lt;em&gt;arXiv preprint arXiv:2302.13971&lt;/em&gt; (2023).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;효율적 inference를 위한 smaller model: LLaMA 모델은 효율성을 위해 대규모 데이터셋으로 학습된 작은 사이즈의 모델을 사용한다. 특히 inference 시 비용 면에서 효율적일 뿐 아니라 state-of-the-art (SOTA)의 성능을 달성했다.&lt;/li&gt;
  &lt;li&gt;Publicly available data: 기존의 많은 모델은 공개되지 않는 독점 데이터를 사용해 학습되었다. 이와 달리 LLaMA는 공개된 데이터만으로 학습되어 투명성 및 호환성, 오픈 소스 원칙을 보장한다.&lt;/li&gt;
  &lt;li&gt;다양한 Benchmark Performance: LLaMA 모델은 common sense reasoning, question answering, reading comprehension 등 다양한 task에서 경쟁력 있는 성능을 보여주었고, 더 큰 사이즈의 모델을 능가하기도 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;Large language model (LLM)은 최소한의 지시(instruction)나 예제로도 새로운 task를 수행할 수 있는 능력을 보여주었다. 그러나 최근 연구에 따르면 작은 모델을 큰 데이터셋으로 학습하면, 큰 사이즈 모델 이상의 성능을 달성할 수 있다는 것이 보고되었다. 한편 모델을 실시간으로 서빙해야 하는 관점에서 보면 학습 도중의 효율성보다는 inference 시 비용 절감과 효율성 확대가 더 중요하다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;p&gt;LLaMA는 다양한 inference 비용에 맞춰 최적화된 성능을 내도록 설계된 언어 모델 (LM) 시리즈로, 7B부터 65B 파라미터를 갖는다. 모든 모델은 공개된 데이터만을 사용하여 학습되었다.&lt;/p&gt;

&lt;h2 id=&quot;pre-training-data&quot;&gt;Pre-training data&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table1.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;오픈 소스 원칙을 보장하는, 다양한 도메인의 공개 데이터셋을 사용했다:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;English CommonCrawl [67%]: 2017-2020년의 다섯 개 CommonCrawl dump에서 전처리한 데이터를 사용했고, 영어가 아니거나 품질이 낮은 콘텐츠는 필터링했다.&lt;/li&gt;
  &lt;li&gt;C4 [15%]: CommonCrawl과 유사하게 전처리되었다. 이 전처리 방식이 성능 향상에 도움이 되는 것으로 보인다.&lt;/li&gt;
  &lt;li&gt;Github [4.5%]: Google BigQuery에서 line 수와 알파벳 문자 비율을 기준으로 필터링하여 구성하였다.&lt;/li&gt;
  &lt;li&gt;Wikipedia [4.5%]: 2022년 중반의 덤프 데이터로 여러 언어를 포함한다.&lt;/li&gt;
  &lt;li&gt;Gutenberg and Books3 [4.5%]: 공개적으로 사용 가능한 서적 데이터로, 중복 콘텐츠를 제거했다.&lt;/li&gt;
  &lt;li&gt;ArXiv [2.5%]: 과학 관련 내용을 포함하는 데이터로, 필수적이지 않은 내용은 제거했다.&lt;/li&gt;
  &lt;li&gt;Stack Exchange [2%]: 점수에 따라 정렬하여 퀄리티가 좋은 것을 골라낸 Q&amp;amp;A 콘텐츠 데이터이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;tokenization&quot;&gt;Tokenization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Byte Pair Encoding (BPE) tokenizer를 사용했다.&lt;/li&gt;
  &lt;li&gt;수를 개별 숫자 단위로 나누고, 알 수 없는 UTF-8 문자는 분해했다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;학습 데이터는 중복을 최소화하여 약 1.4T 토큰을 포함한다 (fig 1).&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;LLaMA 모델은 Transformer 구조를 기반으로 하는데, 몇 가지 수정된 사항이 있다:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-normalization [GPT3]: 각 Transformer 하위 레이어의 입력을 RMSNorm을 사용해 정규화하여 학습 안정성을 강화했다.&lt;/li&gt;
  &lt;li&gt;SwiGLU activation function [PaLM]: ReLU 대신 SwiGLU를 사용해 성능을 향상시켰다. PaLM에서 사용된 $4d$ 대신 $2\over3 4d$ 차원을 사용한다.&lt;/li&gt;
  &lt;li&gt;Rotary Embeddings [GPTNeo]: 각 레이어에서 absolute positional embedding 대신 Rotary embedding (RoPE)을 사용했다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;optimizer&quot;&gt;Optimizer&lt;/h2&gt;

&lt;p&gt;AdamW optimizer를 사용해 학습한다. 최적화 옵션은 다음과 같다:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\beta_1=0.9, \beta_2=0.95$.&lt;/li&gt;
  &lt;li&gt;최대 학습률의 10%로 끝나는 Cosine learning rate schedule.&lt;/li&gt;
  &lt;li&gt;Weight decay 0.1과 gradient clipping 1.0.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모델 크기에 따라 다양한 leanring rate와 batch size로 2,000 warmup-steps (table 2).&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table2.png?raw=true&quot; alt=&quot;table2&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;efficient-implementation&quot;&gt;Efficient implementation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Causal multi-head attention: xformer library를 사용해 메모리와 실행 시간을 효율적으로 줄이고자 하였다.&lt;/li&gt;
  &lt;li&gt;Activation reductions: 체크포인팅을 사용해 backward pass 동안, 특히 계산 비용이 많이 드는 레이어에 대해 activation을 다시 계산한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;main-results&quot;&gt;Main Results&lt;/h1&gt;

&lt;p&gt;20개의 벤치마크에서 zero-shot 및 few-shot task로 평가했고, GPT-3, Gopher, Chinchilla, PaLM 등 비공개 모델 및 OPT, GPT-J, GPT-Neo 등의 오픈 소스 모델과 결과를 비교했다.&lt;/p&gt;

&lt;h2 id=&quot;common-sense-reasonging&quot;&gt;Common sense reasonging&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table3.png?raw=true&quot; alt=&quot;table3&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA의 8개 표준 벤치마크에 대해 평가했다. 이 데이터셋들에는 Cloze 및 Winograd style task와 multiple choice question answering (QA)이 포함된다.&lt;/li&gt;
  &lt;li&gt;Results
    &lt;ul&gt;
      &lt;li&gt;LLaMA-65B는 대부분의 벤치마크에서 Chinchilla 70B와  PaLM-540B를 능가했다.&lt;/li&gt;
      &lt;li&gt;LLaMA-13B는 훨씬 작은 사이즈의 모델임에도 대부분의 벤치마크에서 GPT-3보다 좋은 성능을 보였다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;close-book-question-answering&quot;&gt;Close-book question answering&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table4.png?raw=true&quot; alt=&quot;table4&quot; style=&quot;zoom: 30%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table5.png?raw=true&quot; alt=&quot;table5&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: Natural Questions과 TriviaQA. 모델은 질문에 대한 답에 관련된 단서를 참조하지 않고도 답을 얼마나 잘 맞추는지를 평가 받는다.&lt;/li&gt;
  &lt;li&gt;Results:
    &lt;ul&gt;
      &lt;li&gt;LLaMA-65B zero-shot과 few-shot 세팅 모두에서 state-of-the-art (SOTA) 성능을 달성했다.&lt;/li&gt;
      &lt;li&gt;LLaMA-13B은 더 큰 모델인 GPT-3와 Chinchilla에 뒤쳐지지 않는 성능을 보였다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reading-comprehension&quot;&gt;Reading comprehension&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table6.png?raw=true&quot; alt=&quot;table6&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Benchmark: RACE reading comprehension 벤치마크를 사용했다. 중국 중고등학교 영어 독해 시험에서 수집되었다.&lt;/li&gt;
  &lt;li&gt;Results: LLaMA-65B는 PaLM-540B와 유사한 성능을 보였고, LLaMA-13B는 GPT-3을 능가했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mathematical-reasoning&quot;&gt;Mathematical reasoning&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table7.png?raw=true&quot; alt=&quot;table7&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Benchmarks: MATH와 GSM8k 벤치마크를 사용했다. MATH는 12,000개 중고등학교 수학문제, GSM8k는 중학교 수준 수학 문제로 구성된다.&lt;/li&gt;
  &lt;li&gt;Results: LLaMA-65B는 GSM8k에서 Minerva-62B를 뛰어 넘은 성능을 보였다.
    &lt;ul&gt;
      &lt;li&gt;Minerva는 ArXiv와 Math Web Pages에서 추출한 38.5B개 토큰으로 fine-tune된 PaLM model 시리즈이다. 한편 PaLM과 LLaMA는 수학 문제 데이터에 finetune 되지 않았다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code-generation&quot;&gt;Code generation&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table8.png?raw=true&quot; alt=&quot;table8&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: HumanEval과 MBPP 벤치마크를 사용했다. 모델은 자연어로 묘사한 내용을 보고 코드를 작성하는 능력에 대해 평가된다.&lt;/li&gt;
  &lt;li&gt;Results:
    &lt;ul&gt;
      &lt;li&gt;LLaMA 모델은 LaMDA와 PaLM을 포함한 다른 모델을 능가한다. LLaMA-13B는 LaMDA-137B를, LLaMA 65B는 PaLM-62B보다 좋은 성능을 보였다.&lt;/li&gt;
      &lt;li&gt;코드에 특화된 데이터로 fine-tune하면 성능이 더욱 향상되는 것을 볼 수 있었다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;massive-multitask-language-understanding&quot;&gt;Massive multitask language understanding&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table9.png?raw=true&quot; alt=&quot;table9&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Massive multitask language understanding (MMLU): MMLU는 인문학, STEM, 사회과학 등 다양한 영역의 지식을 포괄하는 다중 선택 질문으로 구성된다.&lt;/li&gt;
  &lt;li&gt;Results: LLaMA-65B는 Chinchilla-70B와 PaLM-540B에 비해 낮은 성능을 보였는데, 이것은 학술적인 데이터를 다른 모델 만큼 충분히 학습하지 않았기 때문인 것으로 추측된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evolution-of-performance-during-training&quot;&gt;Evolution of performance during training&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Result: 성능은 학습 중 지속적으로 향상된다. 또한 성능과 모델의 복잡성 간 상관관계가 나타났다.&lt;/li&gt;
  &lt;li&gt;SIQA와 WinoGrande에 대해서는 예외적인 결과를 보였다: SIQA의 경우 성능의 변동이 나타나는 것으로 보아 신뢰하기 어려운 벤치마크일 가능성이 있다. WinoGrande에서는 모델 복잡성과 성능 간 관계성이 나타나지 않았다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;instruction-fine-tuning&quot;&gt;Instruction Fine-tuning&lt;/h1&gt;

&lt;p&gt;Fine-tuning은 성능을 향상시키고 instruction을 따르는 능력을 개선한다. LLaMA-I는 MMLU에 대해 instruction과 함께 fine-tune한 모델이다. 이를 비슷한 사이즈를 가지는 finetuned 모델인 OPT-IML 및 Flan-PaLM 시리즈와 비교했다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table10.png?raw=true&quot; alt=&quot;table10&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;65B 파라미터의 LLaMA-I는 기존 instruction fine-tuned 모델보다 좋은 성능을 보였다. 그러나 GPT ‘code-davinci-002’에는 미치지 못했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;bias-toxicity-and-misinformation&quot;&gt;Bias, Toxicity and Misinformation&lt;/h1&gt;

&lt;p&gt;LLM은 학습 데이터의 내용에 따라 편향을 가질 수 있으며, 공격적인(toxic/offensive) 콘텐츠를 생성할 수 있다. LLaMA 모델은 웹에서 수집한 데이터를 많이 학습했기 때문에 이러한 콘텐츠 생성의 가능성을 확인할 필요가 있다. toxic content generation 및 stereotypes detection을 평가하기 위해 다양한 벤치마크를 사용했다.&lt;/p&gt;

&lt;h2 id=&quot;realtoxicityprompts&quot;&gt;RealToxicityPrompts&lt;/h2&gt;

&lt;p&gt;모델이 toxic 콘텐츠를 얼마나 생성하는지를 평가하는 벤치마크이다. 모델은 약 10만개의 프롬프트를 완성하고, 점수는 PerspectiveAPI에 의해 0(non-toxic)부터 1(toxic)로 자동 평가된다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table11.png?raw=true&quot; alt=&quot;table11&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLaMA는 다른 모델과 비슷한 toxic score를 얻었다. 예를 들어 Chinchilla의 경우 0.087의 toxicity score를 보였다.&lt;/li&gt;
  &lt;li&gt;모델이 클수록 toxicity가 강하게 나타났다. 이전 연구에서도 유사한 결과가 있었다. 한편 Gopher와 Chinchilla의 경우 Gopher가 사이즈가 더 작음에도 Chinchilla보다 더 toxic하다고 평가된 바가 있어 예외적이었다. 이를 고려할 때 toxicity와 모델 크기 간 관계가 같은 모델 시리즈 내에서만 적용된다고 짐작할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;crows-pairs&quot;&gt;CrowS-Pairs&lt;/h2&gt;

&lt;p&gt;모델의 bias를 9개 카테고리에 따라 평가한다: gender, religion, race, sexual orientation, age, nationality, disability, physical appearance 및 socioenconomic status.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table12.png?raw=true&quot; alt=&quot;table12&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLaMA는 특히 religion, age 및 gender 카테고리에서 약간의 bias를 보였다. 모델이 학습한CommonCrawl 데이터에서 비롯된 것일 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;winogender&quot;&gt;WinoGender&lt;/h2&gt;

&lt;p&gt;모델의 gender 카테고리에 대한 bias를 체크하기 위한 벤치마크이다. 모델의 co-reference resolution 성능이 성별 관련 대명사에 영향을 받는지를 평가한다. 특히 직업과 관련된 사회적 편견을 모델이 학습했는지를 볼 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table13.png?raw=true&quot; alt=&quot;table13&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;성능은 성별 대명사의 종류에 따라 다양하게 나타났다: “her/her/she”와 “his/him/he” 대명사에 관한 성능보다 “their/them/someone” 대명사에서 관한 성능이 더 좋다.&lt;/li&gt;
  &lt;li&gt;큰 모델이 더 큰 gender bias를 가졌다: “gotcha” 사례의 경우, LLaMA-65B가 더 큰 에러를 보이며 gender에 대해 더 편향되어 있음을 보였다.
    &lt;ul&gt;
      &lt;li&gt;“gotcha”는 해당 사례 내 대명사가 보편적이라고 인식되는 직업의 대명사와 일치하지 않는데, 그것이 옳은 답변인 경우를 말한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;truthfulqa&quot;&gt;TruthfulQA&lt;/h2&gt;

&lt;p&gt;모델이 내용의 진위 여부를 판단할 수 있는지를 평가하고, 잘못된 정보를 생성할 위험을 얼마나 갖는지 측정한다. 모델 답변의 truthfulness를 평가한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table14.png?raw=true&quot; alt=&quot;table14&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLaMA 모델은 GPT-3보다 나은 결과를 보였다. 하지만 여전히 정답률이 낮기 때문에 잘못된 정보를 생성할 가능성이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;carbon-footprint&quot;&gt;Carbon footprint&lt;/h1&gt;

&lt;p&gt;모델 훈련과 배포에 있어 환경에 미치는 영향을 설명한다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table15.png?raw=true&quot; alt=&quot;table15&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Training language models to follow instructions with human feedback (2022)</title>
   <link href="https://alatteaday.github.io/ko/papers/2023/04/30/rlhf/"/>
   <updated>2023-04-30T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/04/30/rlhf</id>
   <content type="html">&lt;style&gt;
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: 2.5em;
   margin-bottom: -0.5em;
   margin-left: 0em;
   margin-right: 0em;
}
&lt;/style&gt;

&lt;p&gt;Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” &lt;em&gt;Advances in neural information processing systems&lt;/em&gt; 35 (2022): 27730-27744.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;point&quot;&gt;Point&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Employs &lt;strong&gt;Reinforcement Learning from Human Feedback (RLHF)&lt;/strong&gt; to fine-tune GPT-3 models, aligning them with human intentions while reducing unintended behaviors like hallucinations and toxicity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;InstructGPT&lt;/strong&gt; models outperforms GPT-3 in truthfulness and reliability, generalizing well to new tasks like non-English and coding instructions.&lt;/li&gt;
  &lt;li&gt;Highlights the need for diverse stakeholder input and suggest combining RLHF with other methods to improve model alignment and safety.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;Language models (LMs) often generate misinsformation, toxic or biased content and this issue cannot be resolved simply by increasing the model size. Understanding user intent is crucial for these models. Fine-tuning with human feedback can align the models with user intentions across various tasks.&lt;/p&gt;

&lt;p&gt;Large language models (LLMs) frequently exhibit uninteded behaviors, such as hallucinations, toxic text generation, failing to follow user instructions. These are influenced by the model’s objective, which typically involves predicting the next token based on web data, differing from the goal of “following the user instructions helpfully and safely”.&lt;/p&gt;

&lt;p&gt;To align LMs, this paper employs &lt;strong&gt;Reinforcement Learning from Human Feedbak (RLHF)&lt;/strong&gt; to fine-tune GPT-3 to follow instructions. Human preferences serve as a reward signal for this fine-tuning process.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;methods-and-experimental-details&quot;&gt;Methods and experimental details&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;high-level-methology&quot;&gt;High-level methology&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Preparation: Utilize pre-trained language models (GPT-3), prepare a distribution of prompts for alignment, and train human labelers.&lt;/li&gt;
  &lt;li&gt;Collect demonstration data and train a supervised policy: Labelers provide input prompts as desired behavior responses. The model is fine-tuned on this data using supervised learning.&lt;/li&gt;
  &lt;li&gt;Collect comparison data and train a reward model: Labelers compare model outputs and indicate their preferences. A reward model (RM) is trained using these comparisons to predict human-preferred outputs.&lt;/li&gt;
  &lt;li&gt;Optimize a policy aganst the RM using PPO: The RM’s output serves as a scalar reward. The supervised policy (trained GPT-3) is fine-tuned using the PPO algorithm to optimize this reward.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Step 2 and 3 can be iterative: More comparison data is collected on the current best policy, used to train a new RM and subsequently a new policy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;Source of prompts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Consists of text prompts submitted to the OpenAI API, specifically those using an earlier version of InstructGPT models on the Playground interface.&lt;/li&gt;
  &lt;li&gt;The paper does not include data from customers using the API in production.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Deduplication and filtering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Heuristically deduplicated by checking for prompts that share a long common prefix.&lt;/li&gt;
  &lt;li&gt;The number of prompts is limited to 200 per user ID.&lt;/li&gt;
  &lt;li&gt;Validation and test sets contain no data from users whose data is in the training set.&lt;/li&gt;
  &lt;li&gt;All prompts in the training split were filtered for personally indentifiable information (PII).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Initial source of prompts: Human-written prompts were used as an initial source of instruction to bootstrap the process.&lt;/p&gt;

&lt;p&gt;Datasets for fine-tuning:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SFT dataset: Labelers’ demonstrations (13k prompts, from the API and labeler-written examples).&lt;/li&gt;
  &lt;li&gt;RM dataset: Labeler rankings of model outputs (33k, from the API and labeler-written examples).&lt;/li&gt;
  &lt;li&gt;PPO dataset: Inputs for RLHF fine-tuning. Human labels were not used (31k, only from the API).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Use cases: Most of the use-cases have are generative, rather than classification of prompts submitted to InstructGPT models&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;p&gt;Datasets for training tasks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sources: The datasets are sourced from prompts written by labelers and those submitted to early versions of InstructGPT models via API.&lt;/li&gt;
  &lt;li&gt;Labeler Instructions: Labelers are trained and instructed to write prompts with specific intents or implicit goals in mind to ensure the model aligns with desired behaviors.&lt;/li&gt;
  &lt;li&gt;Language: The datasets are predominately in English (95%). However, the paper also reports the models’ performance in other languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;human-data-collection&quot;&gt;Human data collection&lt;/h2&gt;

&lt;p&gt;Selection of Labelers: A diverse group of labelers was selected to ensure a broad demographic representation. It aims to generate inputs with a wide range of perspectives and to identify potentially harmful outputs.&lt;/p&gt;

&lt;p&gt;Training and Evaluation: Labelers underwent tests designed to measure their performance in labeling according to the set standards. This included their ability to generate diverse prompts and accurately identify harmful content.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;Pre-trained GPT models are utilized as basis. These models are trianed on a broad distribution of Internet data and can be used for various tasks but initially exhibit poorly characterized behavior. The GPT-3 models are then further trained using three different techniques:&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;supervised-fine-tuning-sft&quot;&gt;Supervised fine-tuning (SFT)&lt;/h3&gt;

&lt;p&gt;This method fine-tunes GPT-3 on labeler demonstrations using supervised learning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training details: 16 epochs using a cosine learing rate decay and a residual dropout of 0.2.&lt;/li&gt;
  &lt;li&gt;Model selection: Based on the model’s RM score on the validation set.&lt;/li&gt;
  &lt;li&gt;Finding: Training for more epochs improves both the RM score and human preference ratings, depite some overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reward-modeling-rm&quot;&gt;Reward modeling (RM)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Base model: Starts with a pre-trained SFT model but the final unembedding layer is removed. This layer maps the model’s representations to the vocabulary space for generating output tokens.&lt;/li&gt;
  &lt;li&gt;Input and output: The model takes a prompt and a response are as input and outputs a scalr reward, representing theh quality of the response for the given prompt.&lt;/li&gt;
  &lt;li&gt;Model size: Utilizes 6B reward model (RM) for efficiency. A larger 175B RM was found to be unstable and unsuitable for use as the value function in RL.&lt;/li&gt;
  &lt;li&gt;Data: Uses comparisons between two model outputs for the same input to determine which output is preferred by human labelers.&lt;/li&gt;
  &lt;li&gt;Loss: Trained with cross-entropy loss, using the comparisons as labels. The reward difference reflect the log odds of one response being preferred over the other by a labeler.&lt;/li&gt;
  &lt;li&gt;Speed-up comparison collection: Labelers are presented with $K$ responses to rank for each prompt, where $K$ ranges from 4 to 9. This results in $K(K-1) \over 2$ comparisons for each prompt.&lt;/li&gt;
  &lt;li&gt;Training efficiency and overfitting:
    &lt;ul&gt;
      &lt;li&gt;Comparisons within each labeling task are very correlated. If all comparisons are shuffled into one dataset and processed in a single pass, the model tends to overfit.&lt;/li&gt;
      &lt;li&gt;To address this, the training treats all $K(K-1) \over 2$ comparisons from each prompt as a single batch element, offering several benefits:
        &lt;ul&gt;
          &lt;li&gt;Requires only one forward pass for each set of $K$ responses, instead of $K(K-1) \over 2$ forward passes.&lt;/li&gt;
          &lt;li&gt;Prevents overfitting by avoiding isolated highly correlated comparisons.&lt;/li&gt;
          &lt;li&gt;Improves computational efficiency, and achieves better validation accuracy and log loss.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function:&lt;/p&gt;

\[loss(\theta)=-{1\over \binom{K}{2}} E_{(x,y_w,y_l)~D}[\log(\sigma(r_\theta(x,y_w)-r_\theta(x,y_l)))]\]

    &lt;ul&gt;
      &lt;li&gt;$r_\theta(x,y)$ is the scalar output of the RM for promt $x$ and completion $y$ with parameters $\theta$.&lt;/li&gt;
      &lt;li&gt;$y_w$ is preferred completion out of the pair of $y_w$ and $y_l$.&lt;/li&gt;
      &lt;li&gt;$D$ is the dataset of human comparisons.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning-rl&quot;&gt;Reinforcement learning (RL)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Base model: The SFT model is fine-tuned using Proximal Policy Optimization (PPO) in an environment.&lt;/li&gt;
  &lt;li&gt;Training environment: A bandit environment. It this context, a bandit environment presents a random customer prompt, expects a response, produces a reward determined by the RM, and ends the episode.&lt;/li&gt;
  &lt;li&gt;Input and output: The model takes the prompt and response as input and outputs a reward determined by the RM.&lt;/li&gt;
  &lt;li&gt;KL penalty: A per-token Kullback-Leibler (KL) penalty is added from the SFT model at each token.
    &lt;ul&gt;
      &lt;li&gt;This penalty mitigates over-optimization of the RM and prevents the model from deviating too far from the behavior learned during supervised fine-tuning.&lt;/li&gt;
      &lt;li&gt;The value funciton used in PPO is initialized from the RM.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PPO and PPO-ptx models:
    &lt;ul&gt;
      &lt;li&gt;PPO models: Fine-tuned with PPO.&lt;/li&gt;
      &lt;li&gt;PPO-ptx models: Involve an additional experiment where pre-training gradients are mixed into PPO gradients to address performance regressions on public NLP datasets.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The objective function for PPO-ptx:&lt;/p&gt;

\[\begin{aligned}
\text{objective}(\phi) = &amp;amp; \ \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{RL}}} \left[ r_\theta(x, y) - \beta \log \left( \frac{\pi_\phi^{RL}(y | x)}{\pi^{SFT}(y | x)} \right) \right] \\
&amp;amp; + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log(\pi_\phi^{RL}(x)) \right]
\end{aligned}\]

        &lt;p&gt;where:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_\phi^{RL}$ is the learned RL policy and $\pi^{SFT}$ is the supervised fine-tuned model.&lt;/li&gt;
          &lt;li&gt;$D_{\pi^{RL}}$ is the distribution of data under the RL policy, and $D_{pretrain}$ is the pre-training distribution.&lt;/li&gt;
          &lt;li&gt;$\beta$ is the KL reward coefficient, controlling the strength of the KL penalty.&lt;/li&gt;
          &lt;li&gt;$\gamma$ is the pre-training loss coefficient, controlling the influence of pre-training gradients. For PPO models $\gamma$ is set to 0.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In this paper, InstructGPT refers to the PPO-ptx models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;The performance of PPO models is compared against several baselines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SFT models: Fine-tuned using supervised learing.&lt;/li&gt;
  &lt;li&gt;GPT-3: The standard GPT-3 model without additional fine-tuning.&lt;/li&gt;
  &lt;li&gt;GPT-3 Prompted: Provided with a few-shot previx to prompt it into an instruction-following mode, where the prefix is prepended to the user-specified instruction.&lt;/li&gt;
  &lt;li&gt;InstructGPT is compared to 175B GPT-3 models fine-tuned on FLAN and T0 datasets. These datasets include various NLP tasks combined with natural language instructions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;The definition of “alignment” to evaluate models is based on their ability to act in accordance with user intentions. The practical evaluation framework checks if the model is helpful, honest and harmless.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Helpfulness: The model should follow instructions and infer intentions from prompts or a patterns.
    &lt;ul&gt;
      &lt;li&gt;Since the intention could be unclear, labeler preference ratings are considered mainly for evaluation.&lt;/li&gt;
      &lt;li&gt;There may be divergence between actual user intentions and labeler interpretations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Honesty: Truthfulness is measured instead of comparing the model’s output to its actual belief.
    &lt;ul&gt;
      &lt;li&gt;Two metrics are used:
        &lt;ul&gt;
          &lt;li&gt;The model’s tendency to fabricate information on closed domain tasks&lt;/li&gt;
          &lt;li&gt;Performance on the TruthfulQA dataset.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Harm: Harmfulness depends on the context in which the model is used, and assessing potential harm requires significatn speculation.
    &lt;ul&gt;
      &lt;li&gt;More specific proxy criteria are used:
        &lt;ul&gt;
          &lt;li&gt;Whether a deployed model could be harmful.&lt;/li&gt;
          &lt;li&gt;Labelers evaluate if an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content.&lt;/li&gt;
          &lt;li&gt;Benchmarks like RealToxicityPrompts and CrowS-pairs are used to measure bias and toxicity.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-on-api-distiribution&quot;&gt;Evaluation on API distiribution&lt;/h3&gt;

&lt;p&gt;When using prompts from the API for evaluting human preference ratings, only prompts not included in training are selected.&lt;/p&gt;

&lt;p&gt;Since prompts for InsturctGPT models are not suitable for the GPT-3 baselines, prompts submitted to the GPT-3 API are also used for evaluation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The GPT-3 prompts are not in an instruction-following style.&lt;/li&gt;
  &lt;li&gt;The 175B SFT model is chosen as the baseline due to its average performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each model is evaluated based on how often its outputs are preferred, and labelers judge the overall quality of each response on a 1-7 Likert scale.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-on-public-nlp-datasets&quot;&gt;Evaluation on public NLP datasets&lt;/h3&gt;

&lt;p&gt;Two types of public datasets are used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Safety evaluation: Focuses on truthfulness, toxicity, and bias. Includes evaluations of toxicity using the RealToxicityPrompts dataset.&lt;/li&gt;
  &lt;li&gt;Zero-shot performance: Assesses performance on traditional NLP tasks such as question anwering (QA), reading comprehension, and summarization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The experimental results are organized into three parts: results on the API prompt distribution, results on public NLP datasets, and qualitative results.&lt;/p&gt;

&lt;h2 id=&quot;results-on-the-api-distribution&quot;&gt;Results on the API distribution&lt;/h2&gt;

&lt;h3 id=&quot;1-labelers-significantly-prefer-instructgpt-outputs-over-outputs-from-gpt-3&quot;&gt;1. Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;175B InstructGPT outputs are preferred to GPT-3 outputs around 85% of the time and around 71% compared to few-shot GPT-3.&lt;/li&gt;
  &lt;li&gt;The preference order is GPT-3 &amp;lt; GPT-3 Prompted &amp;lt; SFT &amp;lt; PPO.&lt;/li&gt;
  &lt;li&gt;Adding updates on the pre-training mix during PPO does not lead to significant changes in labeler preference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;a&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig3.png?raw=true&quot; alt=&quot;fig3&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This preference trend remains consistent when evaluating models on prompts submitted to GPT-3 models on the API, though PPO-ptx models perform slightly worse at larger sizes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;a&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig4.png?raw=true&quot; alt=&quot;fig4&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;InstructGPT outputs are rated favorably on more concrete axes: They follow constraints and instruction better and hallucinate less.&lt;/li&gt;
  &lt;li&gt;This suggests that InstructGPT models are more reliable and easier to control than GPT-3.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-instructgpt-models-generalize-to-the-preferences-of-held-out-labelers-that-did-not-produce-any-training-data&quot;&gt;2. InstructGPT models generalize to the preferences of “held-out” labelers that did not produce any training data.&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;InstructGPT models’ outputs are rated better than GPT-3 baselines by held-out labelers, indicating InstructGPT models are not simiply overfitting to the preferences of training labelers.&lt;/li&gt;
  &lt;li&gt;RMs also demonstrate generlization capabilties with cross-validation results: 69.6% accuracy in predicting the preferences of held-out labelers, which is slightly lower than 72.4% accuracy in the predicting preferences within the training set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-public-nlp-datasets-are-not-reflective-of-how-the-lms-are-used&quot;&gt;3. Public NLP datasets are not reflective of how the LMs are used.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig5.png?raw=true&quot; alt=&quot;fig5&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When comparing InstructGPT to 175B GPT-3 baseline fine-tuned on FLAN and T0, these models perform better than GPT-3 with a good prompt but worse than the SFT baseline. This suggests the datasets are not sufficiently diverse to improve API prompt distribution.&lt;/li&gt;
  &lt;li&gt;InstructGPT may outperform FLAN and T0 because:
    &lt;ul&gt;
      &lt;li&gt;Public NLP datasets are desinged to capture typical tasks that are easy to evaluate (e.g., classification, QA). However, open-ended generation and brainstorming constitute most (57%) of tasks the API users want.&lt;/li&gt;
      &lt;li&gt;Public NLP datasets may lack the high diversity of inputs that real-world users are interested in.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-on-public-nlp-datasets&quot;&gt;Results on public NLP datasets&lt;/h2&gt;

&lt;h3 id=&quot;1-instructgpt-models-show-improvements-in-truthfulness-over-gpt-3&quot;&gt;1. InstructGPT models show improvements in truthfulness over GPT-3.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig6.png?raw=true&quot; alt=&quot;fig6&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PPO models demonstrate significant improvements on the TruthfulQA dataset.&lt;/li&gt;
  &lt;li&gt;The 1.3B PPO-ptx model performs slightly worse than GPT-3 of the same size.&lt;/li&gt;
  &lt;li&gt;Training with an “Instruction+QA” prompt helps the model avoid generating false information.
    &lt;ul&gt;
      &lt;li&gt;Instruction+QA: Instructs the model to respond with “I have no comment” when it’s uncertain of the correct answer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-instructgpt-shows-small-improvements-in-toxicity-over-gpt-3-but-not-bias&quot;&gt;2. InstructGPT shows small improvements in toxicity over GPT-3, but not bias.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig7.png?raw=true&quot; alt=&quot;fig7&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Toxicity: Evaluated using the RealToxicityPrompts benchmark.
    &lt;ul&gt;
      &lt;li&gt;Evaluation method: Toxicity scores are obtained through the Perspective API with model samples and labelers rate the samples.&lt;/li&gt;
      &lt;li&gt;InstructGPT outputs are less toxic than those of GPT-3 when instructed to generate respectful outputs. Without any prompt, the models are similar, and InstructGPT can be more toxic when prompted to produce toxic content.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bias: Evaluated using the Winogender and CrowS-Pairs benchmarks.
    &lt;ul&gt;
      &lt;li&gt;Evaluation method: Calculates the relative probabilities of producing sentences in each pair and the entropy of the associated binary probability distributions.
        &lt;ul&gt;
          &lt;li&gt;Unbiased models will show no preference, thus having maximum entropy.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;InstructGPT and GPT-3 show similar levels of bias. The PPO-ptx model shows higher bias when instructed to act respectfully, with unclear patterns.&lt;/li&gt;
      &lt;li&gt;Instructed models tend to be more certain of their outputs, regardlessly with stereotypes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-modifying-rlhf-fine-tuning-procedures-can-minimize-performance-regressions-on-public-nlp-datasets&quot;&gt;3. Modifying RLHF fine-tuning procedures can minimize performance regressions on public NLP datasets.&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Alignment tax: PPO model experience a decrease in performance on public NLP datasets, referred to as “alignment tax”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig28.png?raw=true&quot; alt=&quot;fig28&quot; style=&quot;width: 49%; vertical-align:text-top;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig29.png?raw=true&quot; alt=&quot;fig29&quot; style=&quot;width: 49%; vertical-align:text-top;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mitigation strategies: Mixing pre-training updates to the PPO fine-tuning (PPO-ptx) reduces performance regressions across all datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;a&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig33.png?raw=true&quot; alt=&quot;fig33&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PPO-ptx performs better than merely increasing the KL coefficient. Changing the KL model from the PPO initialization to GPT-3 yields similar improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;qualitative-results&quot;&gt;Qualitative results&lt;/h2&gt;

&lt;h3 id=&quot;1-instructgpt-models-show-promising-generlization-to-instructions-outside-of-the-rlhf-fine-tuning-distribution&quot;&gt;1. InstructGPT models show promising generlization to instructions outside of the RLHF fine-tuning distribution.&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;InstructGPT models can follow non-English instructions, and perform coding tasks, despite limited training data in these formats.&lt;/li&gt;
  &lt;li&gt;Alignment methods can generalize to produce desired behaviors on inputs not directly supervised.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig8.png?raw=true&quot; alt=&quot;fig8&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;175B PPO-ptx model can answer questions about code and non-English instructions, but often responds in English to questions in other languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-instructgpt-still-makes-simple-mistakes&quot;&gt;2. InstructGPT still makes simple mistakes.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig9.png?raw=true&quot; alt=&quot;fig9&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model sometimes incorrectly assumes a false premise in an instruction is true.&lt;/li&gt;
  &lt;li&gt;It can overly hedge even when the answer is clear.&lt;/li&gt;
  &lt;li&gt;It struggles with generating responses when there’re multiple or challenging constraints in an instruction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;h2 id=&quot;implications-for-alignment-research&quot;&gt;Implications for alignment research&lt;/h2&gt;

&lt;p&gt;Improving the alignment of current AI systems provides a clear empirical feedback loop, esssential for refining alignment techniques.&lt;/p&gt;

&lt;p&gt;Moreover, RLHF is an important building block for aligning superhuman systems, especially for tasks difficult to evaluate.&lt;/p&gt;

&lt;p&gt;General lessons for alignment research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The cost of increasing model alignment is modest relative to pre-training&lt;/strong&gt;: The significant costs lie in data collection and computation. With RLHF, larger LMs become more helpful, suggesting investing in aligning existing LMs is more efficient than training new, larger models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;There is evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in&lt;/strong&gt;: E.g., non-English and code tasks. This is important as creating supervised models for each task is expensive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The proposed fine-tuning can mitigate most of the performance degradations&lt;/strong&gt;: Low alignment tax techniques are needed for future AI systems capable of understanding human intents, and RLHF is effective in this regard.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alignment techniques are validated in the real world&lt;/strong&gt;: This work grounds alignment research in real-world applications, providing valuable insights for AI systems used by actual users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;who-are-we-aligning-to&quot;&gt;Who are we aligning to?&lt;/h2&gt;

&lt;p&gt;Factors influencing the fine-tuning data and key sources of alignment preferences:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Labelers’ preferences: The models are aligned to the preferences of hired labelers who generate the training data. They are mostly English speakers, with around 73% agreement among them.&lt;/li&gt;
  &lt;li&gt;Researchers’ preferences: Researchers design the study, write instructions, and guide labelers on edge cases, thereby influencing the alignment. More research is needed to understand the impact of different instructions and interfaces on the collected data and model behavior.&lt;/li&gt;
  &lt;li&gt;Customer prompts: Training data includes prompts from OpenAI customers using the API. There is potential misalignment between customer goals and end-user well-being.&lt;/li&gt;
  &lt;li&gt;Customer representation: The customers are not representative of all potential or current LM users. The initial user base was biased towards OpenAI’s networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Challenges and future directions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Designing a fair and transparent alignment process is complex.&lt;/li&gt;
  &lt;li&gt;This paper demonstrates that the alignment method can work for a specific human reference group but doesn’t claim these group preferences are ideal.&lt;/li&gt;
  &lt;li&gt;Multiple stakeholders need consideration, including model trainers, developers, end-users, and the broader impacted population.&lt;/li&gt;
  &lt;li&gt;Aligning a system to everyone’s preferences simultaneously is impossible, and not all trade-offs will be universally endorsed.&lt;/li&gt;
  &lt;li&gt;One potential approach is to train models for different group preferences so that it can reflect diverse values. However, this may still impact broader society, raising decisions about prioritizing preferences.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Methodology:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Contractor influence: InstructGPT is influenced by the human feedback from about 40 contractors.
    &lt;ul&gt;
      &lt;li&gt;Contractors’ identity, beliefs, cultural backgrounds, and personal history may affect their judgments.&lt;/li&gt;
      &lt;li&gt;They were selected based on their performance with sensitive prompts and labeling tasks.&lt;/li&gt;
      &lt;li&gt;The small team size allowed for better communication but is not representative of the broader population will use the models.&lt;/li&gt;
      &lt;li&gt;They are mostly  English-speaking, and the data is almost entirely in English.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data collection improvements: Most comparisons are labeled by only one contractor to reduce costs.
    &lt;ul&gt;
      &lt;li&gt;Multiple labelings could help identify disagreement areas, indicating where a single model may not align with all labelers.&lt;/li&gt;
      &lt;li&gt;Averaging labeler preferences for disagreements might not be ideal, especially for minority groups, whose preferences should be weighted more heavily.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Imcomplete alignment and safety: InstructGPT is not fully aligned or safe.
    &lt;ul&gt;
      &lt;li&gt;It still generates toxic or biased outputs, misinformations, and sexual or violent content.&lt;/li&gt;
      &lt;li&gt;It sometimes fails to generate reasonable outputs for certain inputs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Following potentially harmful instructions: InstructGPT often follows instructions even if it could lead to real-world harm.
    &lt;ul&gt;
      &lt;li&gt;It produces more toxic outputs than GPT-3 when instructed to be maximally biased.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Deep learning for image super-resolution: A survey (2020)</title>
   <link href="https://alatteaday.github.io/ko/papers/2020/12/22/srsurvey/"/>
   <updated>2020-12-22T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2020/12/22/srsurvey</id>
   <content type="html">&lt;p&gt;Wang, Zhihao, Jian Chen, and Steven CH Hoi. “Deep learning for image super-resolution: A survey.” &lt;em&gt;IEEE transactions on pattern analysis and machine intelligence&lt;/em&gt; 43.10 (2020): 3365-3387.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9044873?casa_token=ZvibT-s3inQAAAAA:7z3uDjyf2cDsJhnY-NLadsaG1exlVS3qQAPck6JXaj6awV7I5Gcc8XXbjjw5uugCWXfE6tXJNB4&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Super-resolution (SR)은 저화질(low-resolution; LR) 이미지를 고해상도(high-resolution; HR) 이미지로 변환하는 과정이다.&lt;/li&gt;
  &lt;li&gt;LR 이미지에는 여러 HR 이미지가 있을 수 있다는 점에서 SR은 불완전한 문제이다.&lt;/li&gt;
  &lt;li&gt;Deep learning (DL)은 SR의 발전에 크게 기여했는데, CNN (SRCNN) 및 GAN (SRGAN)과 같은 방법이 사용되었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;problem-setting-and-terminology&quot;&gt;Problem Setting and Terminology&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Problem Definition: LR input에서 HR 이미지를 근사하는 SR 모델 개발&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image Quality Assessment (IQA): 인간의 주관적 판단 및 객관적 계산을 포함하여, full-reference, reduced-reference, no-reference 방법으로 분류된다.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;supervised-super-resolution&quot;&gt;Supervised Super-Resolution&lt;/h1&gt;

&lt;h2 id=&quot;sr-framework&quot;&gt;SR Framework&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-Upsampling Framework: 전통적인 upsampling 방식으로 LR 이미지를 확대한 후 DL network로 화질을 정제한다. (e.g., SRCNN).&lt;/li&gt;
  &lt;li&gt;Post-Upsampling Framework: End-to-end DL 모델을 사용해 upsampling 한다.&lt;/li&gt;
  &lt;li&gt;Progressive Upsampling Framework: CNNs을 cascade로 사용해 단계적으로 이미지를 정제한다.&lt;/li&gt;
  &lt;li&gt;Iterative Up-and-Down Sampling: DBPN이나 SRFBN 모델과 같이 LR-HR 간 dependency를 더 잘 포착하는 방법.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;upsampling-methods&quot;&gt;Upsampling Methods&lt;/h2&gt;

&lt;h3 id=&quot;interpolation-based&quot;&gt;Interpolation-Based&lt;/h3&gt;
&lt;p&gt;Nearest-neighbor, bilinear, bicubic interpolation을 포함한다. DL 기반 방법이 등장하기 전까지 이미지 크기를 조정하는 데 사용되었다.&lt;/p&gt;

&lt;h3 id=&quot;learning-based&quot;&gt;Learning-Based&lt;/h3&gt;
&lt;p&gt;Transposed convolution layer나 sub-pixel layer를 사용해 end-to-end로 모델을 학습한다&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Transposed Convolution Layer (Deconvolution Layer)&lt;/strong&gt;: convolution output과 같은 크기의 feature map을 기반으로 input을 예측하여, 0을 삽입한 후 convolution을 수행해 이미지 크기를 키운다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/deconv.jpg?raw=true&quot; alt=&quot;deconv&quot; style=&quot;zoom: 70%;&quot; /&gt;
 &lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;이미지 크기를 키우면서 패턴의 연결성을 유지하지만, 각 축에서 불균일하게 겹치는 부분이 생기면서 checkerboard 같은 결함을 초래할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sub-Pixel Layer (Pixelshuffle)&lt;/strong&gt;: Convolution을 통해 다수의 채널을 생성하고 이것을 재구성한다.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
     &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/subpixel.png?raw=true&quot; alt=&quot;subpixel&quot; style=&quot;zoom: 70%;&quot; /&gt;
 &lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Input size가 $(h \times w \times c)$일 때, $s^2$배의 채널이 만들어진다. $s$는 scaling factor이다. Output size는 $(h \times w \times s^2c)$가 되고, 이것을 $(sh \times sw \times c)$로 재구성(shuffle)한다.&lt;/li&gt;
      &lt;li&gt;Transposed convolution layer보다 receptive field가 커, 맥락과 현실적인 세부 사항을 더 반영할 수 있다. 하지만 receptive field의 분포가 균일하지 않아 블록 사이 경계 부근에서 결함이 발생할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;network-design&quot;&gt;Network Design&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/networks.png?raw=true&quot; alt=&quot;networks&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;residual-learning&quot;&gt;Residual Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;LR과 HR를 직접 매핑하는 것 대신 둘 간의 잔차(residual)에 집중하여 학습을 단순하게 한다. 변환 작업의 복잡도를 줄여준다.&lt;/li&gt;
  &lt;li&gt;Input 이미지와 target 이미지 간 residual만 학습함으로써 모델이 세부적인 사항에 집중할 수 있고, 이것으로 성능이 향상될 뿐 아니라 수렴 속도도 빨라진다.&lt;/li&gt;
  &lt;li&gt;Example: ResNet architecture는 residual block을 사용해 네트워크가 아주 깊어도 효과적으로 학습시킬 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recursive-learning&quot;&gt;Recursive Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;동일한 모듈을 반복적으로 적용하여 high-level feature를 캡쳐한다.&lt;/li&gt;
  &lt;li&gt;feature를 반복적으로 정제하여 더 디테일하고 정확한 이미지를 reconstruct 하게끔 한다.&lt;/li&gt;
  &lt;li&gt;Example: Deep Recursive Convolutional Network (DRCN)은 단일 convolutional layer를 여러 번 사용해 receptive field를 확장하면서도 파라미터 수를 크게 증가시키지 않는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-path-learning&quot;&gt;Multi-Path Learning&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Local Multi-Path Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;병렬적인 경로를 통해 feature를 추출하고, 이를 융함하여 더 나은 모델링을 가능하게 한다. 이미지의 다양한 측면을 동시에 캡쳐하는 데 도움이 된다.&lt;/li&gt;
      &lt;li&gt;각 경로는 각각 다른 규모나 유형의 feature에 집중할 수 있고, 이것을 결합해 전체적인 representation을 개선한다.&lt;/li&gt;
      &lt;li&gt;Example: Multi-scale Residual Network (MSRN)은 다양한 커널 크기를 가진 multiple convolutional layer를 사용해 multi-scale feature를 캡쳐한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scale-Specific Multi-Path Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;단일 네트워크 내에서 다양한 scaling factor에 대해 별도의 경로를 갖으면서도, 네트워크가 여러 scale을 더 효과적으로 처리할 수 있게 한다.&lt;/li&gt;
      &lt;li&gt;Example: MDSR (Multi-Scale Deep Super-Resolution)은 대부분의 파라미터를 공유하지만, 다른 upscaling factor를 다루기 위해 scale-specific layer를 사용한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dense-connections&quot;&gt;Dense Connections&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;각 레이어를 서로 feed forward 방식으로 연결해, gradient flow와 feature 재사용을 촉진한다. 이것으로 gradient가 이전 레이어로 직접 흐를 수 있게 하여 학습 효율을 높인다.&lt;/li&gt;
  &lt;li&gt;feature를 재사용하게끔 하여 효율적이고 컴팩트한 네트워크를 만든다.&lt;/li&gt;
  &lt;li&gt;Example: DenseNet은 각 레이어를 모든 레이어에 연결해 feature의 propagation을 촉진하여 gradient vanishing의 위험을 줄인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;group-convolution&quot;&gt;Group Convolution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Input 채널을 그룹화하고, 각 그룹 내에서 convolution을 수행하여 계산 복잡도와 파라미터 수를 줄인다.&lt;/li&gt;
  &lt;li&gt;경량 모델에서 성능과 효율성을 균형 있게 가져가기 위해 사용된다.&lt;/li&gt;
  &lt;li&gt;Example: Xception과 MobileNet은 depthwise separable convolution을 사용하여 파라미터와 계산량을 줄인다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pyramid-pooling&quot;&gt;Pyramid Pooling&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;여러 scale에서 pooling을 해 global 및 local 맥락 정보를 파악한다. 이미지를 다양한 해상도로 이해하는 데 도움이 된다.&lt;/li&gt;
  &lt;li&gt;Example: PSPNet (Pyramid Scene Parsing Network)은 pyramid pooling을 사용해 다양한 scale에서의 얻은 정보를 결합하여 feature representation을 향상시킨다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-mechanisms&quot;&gt;Attention Mechanisms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Channel Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Feature 채널 간 상호의존성(interdependency)에 집중한다. 각 채널에 다른 weight를 주어 중요한 feature를 강조하고 덜 중요한 feature는 덜 반영한다.&lt;/li&gt;
      &lt;li&gt;Example: Squeeze-and-Excitation Networks (SENet)은 spacial 차원에서 feature map을 squeeze하고 채널 별 feature를 재조정하는 excitation 연산을 한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spatial Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;feature의 공간적 위치에 집중한다. 다른 위치에 weight를 할당하여, 모델이 이미지에 관련된 영역에 집중할 수 있게 한다.&lt;/li&gt;
      &lt;li&gt;Example: Convolutional Block Attention Module (CBAM)은 채널 및 공간 정보 관련 attention을 결합해 의미 있는 부분에 집중해 representation을 개선한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-Local Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;멀리 떨어진 픽셀 간 dependency를 파악한다. 이는 global context가 중요한 SR 작업에 특히 유용하다.&lt;/li&gt;
      &lt;li&gt;Example: Non-local Neural Networks는 self-attention mechanism을 사용해 feature map 내 모든 위치 간 관계를 계산해 global context와 dependency를 캡쳐한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Combined Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;여러 가지 attention mechanism 방식을 결합해 각 유형의 강점을 활용할 수 있다. 예를 들어 channel attention과 spatial attention을 결합해 더욱 포괄적으로 해석 가능한 attention mechanism을 구현할 수 있다.&lt;/li&gt;
      &lt;li&gt;Example: The Residual Channel Attention Network (RCAN) residual 네트워크 내에서 channel attention 모듈을 사용해 중요한 feature를 캡쳐하는 능력을 향상시켰다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;learning-strategies&quot;&gt;Learning Strategies&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Loss Functions: 초기에는 pixel-wise L2 loss를 사용했는데, 최근에는 content loss, adversarial loss, perceptual loss와 같은 더 복잡한 loss를 통합 사용하여 이미지의 품질을 향상시킨다.&lt;/li&gt;
  &lt;li&gt;Training Techniques: curriculum learning, multi-supervision, progressive learning과 같은 기법을 사용해 학습 과정 및 모델 성능을 개선한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-super-resolution&quot;&gt;Unsupervised Super-Resolution&lt;/h1&gt;

&lt;p&gt;Unsupervised method는 페어링된 LR-HR dataset에 의존하지 않는다. 대신 adversarial training을 사용해 LR를 HR로 매핑하는 것을 생성 모델에 학습시킨다. 예를 들어 CycleGAN은 LR을 HR로, 또 그 역으로도 매핑함으로써 이미지 변환을 학습한다.&lt;/p&gt;

&lt;h1 id=&quot;domain-specific-super-resolution&quot;&gt;Domain-Specific Super-Resolution&lt;/h1&gt;

&lt;p&gt;Domain-specific method는 face SR, text SR, medical image SR과 같이 특정 응용 분야에 중점을 둔다. 도메인 지식을 활용해 특정 context에서의 SR 품질을 향상시킨다.&lt;/p&gt;

&lt;h1 id=&quot;benchmark-datasets-and-performance-evaluation&quot;&gt;Benchmark Datasets and Performance Evaluation&lt;/h1&gt;

&lt;p&gt;Set5, Set14, BSD100, Urban100 등 여러 benchmark dataset이 SR 모델 평가에 사용된다. 일반적인 metric으로는 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index (SSIM)가 있다.&lt;/p&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;PSNR은 널리 사용되나, 품질에 대한 사람의 인식과 잘 일치하지는 않는다. 한편 SSIM은 밝기, 대비, 구조 등을 고려해 이 점을 보완한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PSNR: 제일 보편적인 reconstruction quality 측정 metric이다. SR의 경우 최대 픽셀 값($L$)과 이미지 간 평균 제곱 오차(mean squared error; MSE)를 통해 PSNR을 정의한다.&lt;/p&gt;

\[PSNR=10\cdot\log_{10}\big({L^2\over{1\over N}\sum_{i=1}^N(I(i)-\hat{I}(i))^2}\big)\]

    &lt;ul&gt;
      &lt;li&gt;$I(i)$와 $\hat{I}(i)$는 각각 원본 이미지와 생성된 이미지의 픽셀 값을 나타낸다. $N$은 총 픽셀 수이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SSIM: 밝기, 대비, 구조의 측면에서 이미지를 각각 비교하여 구조적 유사성을 측정한다. 인간의 시각 시스템(human visual system; HVS)이 자연스럽고 익숙하게 이미지 구조를 파악한다고 가정한다.&lt;/p&gt;

\[SSIM(I,\hat{I})={(2\mu_I\mu_{\hat{I}}+C_1)(2\sigma_{I\hat{I}}+C_2)\over(\mu_I^2+\mu_{\hat{I}}^2+C_1)(\sigma_I^2+\sigma_\hat{I}^2+C_2)}\]

    &lt;ul&gt;
      &lt;li&gt;$\mu_I$와 $\mu_\hat{I}$은 각각 원본 이미지와 생성된 이미지의 평균 픽셀 값이다. $\sigma_I^2$와 $\sigma_\hat{I}^2$는 분산,  $\sigma_{I\hat{I}}$는 $I$와 $\hat{I}$의 공분산이다. $C_1$와 $C_2$는 분모가 작은 경우를 대비해 계산의 안정성을 위해 사용되는 상수이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;challenges-and-future-directions&quot;&gt;Challenges and Future Directions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Scalability: 다양한 scale과 resolution을 효율적으로 처리할 수 있는 SR 모델 개발하기.&lt;/li&gt;
  &lt;li&gt;Real-World Applications: 다양한 원인에 의해 해상도가 낮은 실제 이미지에서 잘 작동하도록 SR 모델 개선하기.&lt;/li&gt;
  &lt;li&gt;Efficiency: 높은 성능을 유지하며 계산 복잡도와 메모리 샤용량 줄이기.&lt;/li&gt;
  &lt;li&gt;Generality: 다양한 유형과 도메인의 이미지에서 일반화될 수 있는 SR 모델 개발하기.&lt;/li&gt;
  &lt;li&gt;Perceptual Quality: 시각적으로 깔끔하고 결함이 없는 이미지를 생성하도록 모델 발전시키기.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;이 survey paper는 supervised, unsupervised, domain-specific method 등으로 유형화하여 DL 기반의 SR 기술에 대해 심층적으로 검토한다. Benchmark dataset과 성능 평가 metric을 설명하고, SR 연구의 현재 상태에 대해 포괄적인 개요를 제공한다. 나아가 다양한 네트워크, upsampling 및 학습 기술을 살펴보고, 이 분야의 발전 방향을 제시한다.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] SinGAN: Learning a Generative Model from a Single Natural Image (2024)</title>
   <link href="https://alatteaday.github.io/ko/papers/2020/12/11/singan/"/>
   <updated>2020-12-11T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2020/12/11/singan</id>
   <content type="html">&lt;p&gt;Shaham, Tamar Rott, Tali Dekel, and Tomer Michaeli. “Singan: Learning a generative model from a single natural image.” &lt;em&gt;Proceedings of the IEEE/CVF international conference on computer vision&lt;/em&gt;. 2019.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.html&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;GAN은 현실감 있고 질 좋은 이미지를 생성하는 성공적인 결과를 보여주었다. 그러나 이 장점은 모델이 특정한 클래스 내 데이터를 학습하여 해당 클래스에 속하는 이미지를 생성하는 것에 국한된다는 한계를 동반한다. 여러 개의 클래스를 갖는 다양한 데이터에서 어떠한 분포를 찾아내는 것은 여전히 어려운 문제이다. 이를 해결하기 위해 생성할 때 다른 input signal을 추가하여 조절하거나 모델을 특정한 task에 맞게 학습시키는 것이 요구된다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 기존의 한계점을 벗어나기 위해 “Unconditional generation learned from a single natural image” 를 제안한다. 단일 이미지(single image) 내부 패턴의 통계량만을 가지고도 충분히 생성 모델을 학습시킬 수 있다는 것이 이 논문의 아이디어이다. 이미지 하나에서 충분히 복잡한 구조와 질감을 얻어낼 수 있기 때문이다. 이런 방식으로 GAN 모델을 학습할 수 있다면 같은 클래스에 속하는 여러 개의 이미지 데이터에 의존할 필요가 없다.&lt;/p&gt;

&lt;p&gt;Single image를 다루는 기존의 모델들은 이미지를 생성할 때 저마다의 한계를 갖는다. InGAN은 최초로 Single image를 다룬 GAN 기반의 모델이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/ingan.png?raw=true&quot; alt=&quot;dddd&quot; style=&quot;zoom: 40%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;핵심적인 아이디어는 Generator가 입력 이미지인 $x$를 가지고 생성된 이미지인 $y$를 “retarget”하게 한다는 것이다. 그러나 InGAN은 입력 이미지에 conditional하다는 한계를 갖는다. 입력 이미지가 있어야 이미지를 생성할 수 있다. unconditional한 single image GAN 모델의 경우 texture generation에 관하여만 존재했다[3, 4, 5]. 이 모델들은 texture에 국한된 이미지만 생성할 수 있을 뿐, 보통 이미지라고 인식될 만큼의 자연스러운 결과물의 생성이 불가하다.&lt;/p&gt;

&lt;p&gt;Image manipulation task를 다루는 최근의 생성 모델들은 대부분 GAN을 기반으로 한다. 관련 task는 teractive image editing, sketch2image, image-to-image translation, super resolution 등이다. 그런데 기존 모델들은 특정한 클래스의 데이터를 학습하게 되어 있어 다양한 task를 유연하게 해결하지 못하는 등의 한계가 있다.&lt;/p&gt;

&lt;p&gt;이 논문에서 제안하는 SinGAN 모델은 기존의 한계점을 해결할 수 있다. SinGAN은 unconditional하게 입력 이미지 없이 noise만으로 이미지를 생성한다. 그러면서도 기존의 unconditional texture generation 모델과 달리 자연스러운 이미지를 생성할 수 있다. 나아가 특정한 클래스의 데이터의 공통적인 특성을 학습하는 것에 주력하지 않고, 하나의 이미지를 가지고 scale을 변화시키며 내부적인 특성을 학습한다. 이는 모델이 한정되지 않은 다양한 task를 수행하면서도 좋은 성능을 가지게끔 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p&gt;SinGAN은 하나의 입력 이미지의 내재된 통계량을 가지고 unconditional하게 이미지를 생성하도록 만들어진 모델이다. 학습 방식의 핵심적인 부분은 이미지의 scale을 여러 단계를 거칠 때마다 변화시키며 이미지의 특성을 파악하게끔 한다는 것이다. 아래는 모델의 전체적 구조를 나타낸 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig4.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;모델의 구조는 전반적으로 데이터 scale이 위로 올라갈수록 정교해지는 pyramid 형태를 갖는다. Generator $G: {G_0, …, G_N}$는 입력 이미지인 $x:{x_0, …, x_N}$로 학습된다. $x_n$은 값 $r_n (r&amp;gt;1)$에 따라 원본 이미지를 downsampling한 것으로, $x_N$은 제일 coarse한 scale을 갖는다. 각 $G$는 이전 단계에서 생성된 이미지 $\tilde{x}$와 해당 단계의 scale에 맞는 noise $z$를 입력 받는다. 그리고 대응되는 Discriminator $D :{D_0, …, D_N}$를 속이는 방향으로 이미지를 생성하며 학습한다. D는 원본 이미지 $x_n$를 $G$가 생성한 $\tilde{x}_n$와 비교하여 판별해내는 방향으로 학습된다. 이 때 $x_n$과 $\tilde{x}_n$ 이미지 전체를 기준으로 비교하는 것이 아니라 이미지의 일부분을 두고 비교하는데, 마치 이미지 위에 겹쳐져 비교할 부분을 가리키는 것을 patch라고 한다. 이 patch size는 pyramid 단계에서 올라갈수록 작아진다.&lt;/p&gt;

&lt;p&gt;Scale이 제일 coarse한 단계의 $G_N$은 white gaussian noise $z_N$만을 입력으로 받아 이미지를 생성한다.&lt;/p&gt;

&lt;p&gt;\[
\tilde{x}_N=G_N(z_N)
\]&lt;/p&gt;

&lt;p&gt;맨 처음 단계의 patch size는 보통 원본 이미지 높이의 절반 정도가 된다. 따라서 $\tilde{x}_N$은 이미지의 대략적인 배치와 구조를 나타내게 된다. 이후 단계가 올라갈수록 이전 단계에서 표현되지 못한 디테일들을 가지는 이미지가 생성된다. 이것을 위해 $G_n$의 입력으로 $z_n$과 함께 이전 단계에서 생성된 이미지를 upsampling한 이미지가 주어진다.&lt;/p&gt;

&lt;p&gt;\[
\tilde{x}_n=G_n(z_n, (\tilde{x}_{n+1})\uparrow^r), n&amp;lt;N
\]&lt;/p&gt;

&lt;p&gt;각 단계에서 $G_n$의 내부적 구조는 5개의 Conv-block으로 이뤄져 있다. 아래의 그림과 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig5.png?raw=true&quot; style=&quot;zoom: 40%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;\[
\tilde{x}_n=(\tilde{x}_{n+1})\uparrow^r+\psi_n(z_n, (\tilde{x}_{n+1})\uparrow^r)
\]&lt;/p&gt;

&lt;p&gt;$z_n$을 $(\tilde{x}_{n+1})\uparrow^r$ 에 더하는 데, 이것을 Convolutional layer 이전에 하여 $G$가 noise를 누락하지 못하게 한다. 종종 randomness를 조건으로 다루는 GAN 연구에서 발생하는 문제점을 이렇게 해결하였다. 또한 데이터를 5개의 Conv-block을 통과시킨 후 $(\tilde{x}_{n+1})↑^r$을 한 번 더 더해주는 residual learning 방식을 사용한다. 각 Conv layer는 Conv(3X3), BatchNorm, LeakyReLU로 구성되어 있다. 오로지 Conv layer만 사용한 점은 test 시 noise의 차원을 변경하여 다양한 크기의 이미지를 생성할 수 있다는 이점을 주기도 한다. 한편 $D_n$의 구조는 $G_n$의 5-Conv net과 동일하다.&lt;/p&gt;

&lt;p&gt;각 단계의 Loss function은 Adversarial Loss와 Reconstruction Loss으로 이뤄져 있다.&lt;/p&gt;

&lt;p&gt;\[
min_{G_n}max_{D_n}L_{adv}(G_n, D_n)+{\alpha}L_{rec}(G_n)
\]&lt;/p&gt;

&lt;p&gt;Adversarial loss는 xn과 $\tilde{x}_n$의 분포 차이를 작게 하기 위해 사용된다. Reconstruction loss는 이미지를 생성할 때 필요한 원본 이미지의 중요한 특징 정보들을 보존하게끔 하기 위해 사용된다.&lt;/p&gt;

&lt;p&gt;Adversarial loss로는 WGAN-GP Loss[6]를 사용했다. WGAN-GP는 Wasserstein GAN(WGAN)의 weight clipping의 문제점을 해결하기 위한 방법으로 gradient penalty를 도입한 모델이다. GAN은 원본 이미지와 $G$에 의해 생성된 이미지 분포의 차이로써 Jensen-Shannon divergence를 사용한다. $G$는 이것을 최소화 하도록 학습된다. 그런데 이 과정이 반복되다보면 $D$가 포화되면서 gradient vanishing 문제가 발생한다. WGAN은 이 문제를 해결하기 위해 분포 간 차이의 척도로서 Wasserstein-1 distance를 사용한 것이다. 또한 $D$-해당 논문에서는 ‘the critic’-는 1-Lipschitz Function으로, 미분 계수가 거의 모든 곳에서 1을 넘지 않는다. 이 Lipschitz constraint를 강화하기 위해 $D$의 weight를 콤팩트 공간에 가둬두는 weight clipping이 더불어 제시되었다. 이것으로 gradient vanishing 문제가 발생하는 것을 방지한다.&lt;/p&gt;

&lt;p&gt;WGAN-GP는 weight clipping이 모델 최적화를 어렵게 한다는 것을 보이면서, 이 문제를 해결하기 위해 만들어졌다. Gradient penalty는 weight clipping 대신 $D$의 Lipschitz constraint를 강화하기 위한 장치로서 제시되었다. 1-Lipschitz function인 D의 미분 계수를 입력값에 따라 직접적으로 제한하는 방식이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/loss.png?raw=true&quot; style=&quot;zoom: 43%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SinGAN에서는 학습 안정성이 높은 WGAN-GP를 사용하였다. 또한 loss function을 patch 몇 개가 아닌 이미지 전체에 걸쳐 적용하였는데, 이것으로 모델이 boundary condition을 학습하게끔 했다.&lt;/p&gt;

&lt;p&gt;Reconstruction loss는 원본 이미지를 생성하는 noise map의 존재를 가정하기 위함이다. $\tilde{x}_n^{rec}$은 $n$번째 단계에서 noise map ${z_N^{rec}, z_{N-1}^{rec}, …, z_0^{rec}}={z^*, 0, …, 0}$을 가지고 생성된 이미지이다. $z^*$은 학습 내내 고정되는 noise map이다.&lt;/p&gt;

\[\begin{aligned}
L_{rec} &amp;amp;= \lVert G_n(0, (\tilde{x}\_{n+1}^{rec}\uparrow^r))-x_n \rVert^2, \ n&amp;lt;N \\
L_{rec} &amp;amp;= \lVert G_N(z^*)-x_N \rVert^2, \ n&amp;lt;N
\end{aligned}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;result&quot;&gt;Result&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig6.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SinGAN은 Single image를 학습하여 이미지를 자연스럽고도 다양하게 생성하였다. 원본 이미지의 전반적인 배치와 패턴 구조를 보존하여 현실감 있는 결과물을 만들어 냈다. 그림자, 물에 반사되는 모습 등이 자연스럽게 표현되었다. 그러면서도 patch의 새로운 조합을 생성하여 원본 이미지와 완전히 다른 이미지를 만들거나, 학습한 이미지보다 더 높은 화질을 갖는 이미지를 생성하기도 했다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig8.png?raw=true&quot; style=&quot;zoom: 45%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Scale이 pyramid 형태인 모델 구조는 좋은 결과를 보였다. 학습 시 설정하는 scale 수에 따라 모델이 이미지를 생성하는 데에 이미지의 특성을 반영하는 정도가 결정되는 것을 볼 수 있었다. Scale 수가 작으면 coarse한 단계의 patch가 작아지므로 집약적인 디테일을 학습한다. Scale 수가 증가할 수록 patch가 커져 이미지의 전반적인 배치나 특성을 보존하도록 학습한다. 이것을 시험하기 위해 scale을 지정하여 원본 이미지를 얼마나 변화시켜 생성할 것인지 결정하였다. n=N일 때 얼룩말의 모습은 n=N-1, n=N-2일 때보다 부자연스럽다.&lt;/p&gt;

&lt;p&gt;모델이 생성한 이미지가 얼마나 자연스러운지를 평가하기 위해 두 가지 metric이 사용되었다. 첫 번째 방법은 user study이다. 고용된 사람들에게 두 가지 질문을 했다. 하나는 원본 이미지와 생성된 이미지를 한 번에 1초 간 보여준 뒤 어떤 것이 가짜인지 묻는 paired case이다. 다른 하나는 하나의 이미지를 1초 간 보여준 뒤 그것이 가짜인지 아닌지를 묻는 unpaired case이다. 50개의 가짜 이미지를 랜덤하게 제공했다. 한편 coarsest scale을 N과 N-1로 달리하여 이미지를 준비하기도 했다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table1.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결과적으로 비교 기준이 없는 unpaired case에서, coarsest scale에 관해서는 전반적 구조가 더 보존되는 N-1일 때 사람들이 더 헛갈려 했다. 그러나 N일 때에도 40% 이상의 혼동율로, 분간할 수 없는 수준을 가리키는 50%에 근접하였다. SinGAN이 생성한 이미지가 사람이 보기에 꽤 자연스럽다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;두 번째 방법은 Single Image Frechet Inception Distance(SIFID) metric이다. Frechet Inception Distance(FID)는 원본 이미지와 생성된 이미지 feature 분포의 편차를 측정하는 것이다. SIFID는 원본 이미지의 통계량을 얼마나 보존하였는지를 측정하기 위해 이 논문에서 제안한 것이다. SIFID는 FID가 이미지 당 하나의 vector를 사용하는 것과 달리, 이미지 내 위치 당 하나의 vector를 사용하여 원본 이미지와 그것으로부터 생성된 이미지 feature 간의 통계량 차이를 비교하는 기준이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table2.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Coarsest scale에 관해서는 user study의 경우와 같이 N-1일 때가 결과가 더 좋았다. 한편 user study와 달리 paired case의 결과가 더 좋았는데, SIFID는 원본과 생성 이미지를 비교하여 계산되기 때문이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table3.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SinGAN은 이미지 생성 뿐만아니라 다양한 image manipulation task에 사용될 수 있다. 모델 구조를 바꾸거나 fine-tuning을 적용하지 않고도 super resolution, editing, paint-to-image 등의 여러 task를 수행한다. 단 입력 이미지와 같은 특징 분포를 갖는 이미지만을 생성할 수 있다. n번째 scale 단계에서 down sampled된 이미지를 넣어 해당 이미지의 patch들의 분포에 맞게 학습해 나가는 방식이다. 성능 또한 각 task를 목적으로 만들어진 모델들에 비해 성능 또한 떨어지지 않는다. 예를 들어 super resolution의 경우 SRGAN, EDSR, DIP, ZSSR과 SinGAN을 비교했는데, single data 기반 SOTA 모델인 DIP, ZSSR 및 dataset 기반 모델인 EDSR보다 성능이 좋았고, SRGAN과는 거의 근접한 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;SinGAN은 Single image로 GAN을 학습하여 자연스러운 이미지를 생성하고, 다양한 이미지의 변용이 가능한 장점이 있다. 우선 SinGAN은 입력 데이터 하나만으로도 학습이 가능하다. 기존의 모델들은 특정 클래스에 속하는 이미지를 생성하는 모델을 학습시키기 위해 해당 클래스의 많은 이미지 데이터가 필요했다. SinGAN은 이 점에서 많은 데이터를 구할 필요가 없어 학습이 용이하다. 그럼에도 SinGAN은 사람이 보기에도 자연스러운 이미지를 생성해낼 수 있다. 기존의 Single image 기반 GAN 모델은 texture 이미지 생성에만 국한되었다. 나아가 이미지의 전반적인 패턴과 특징을 유지하면서도 물체의 배치 등을 변화시켜 입력 이미지와 다른 다양한 이미지를 생성할 수 있다.&lt;/p&gt;

&lt;p&gt;많은 image manipulation task를 다루는 모델들은 특정한 task를 수행하기 위한 목적으로 만들어졌다. 그러나 SinGAN은 모델 구조 수정이나 tuning 등의 추가 작업을 하지 않으면서도 다양한 task를 해결할 수 있다. Scale을 변화시켜 학습하고, 테스트 시 적절한 단계에 이미지를 입력함으로써 super resolution, editing, paint-to-image, single image animation 등을 수행한다.&lt;/p&gt;

&lt;p&gt;다만 user study에서 볼 수 있듯 결과물이 실제 이미지와 비교하였을 때 가짜인지 구분이 되는 정도이다. 더 자연스러운 이미지를 생성해내는 성능을 위해 발전할 여지가 있다.&lt;/p&gt;
</content>
 </entry>
 

</feed>
