<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      Coffee Chat &middot; Brewing AI Knowledge
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/ko/page5/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/ko/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/ko/about">About</a>
    <a class="sidebar-nav-item active" href="https://alatteaday.github.io/ko/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/ko/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/ko/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/ko/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/ko/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/ko/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/ko/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/ko/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        
            <a href=" /page5/">eng</a>
        
    

    
    
        kor
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/study/2023/05/20/rnn/">
        Recurrent Neural Networks (RNNs)
      </a>
    </h1>
    <!--<span class="post-date">20 May 2023</span>-->
    <p class="post-date">20 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/ko/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Sequence data를 분석하기 위한 딥러닝 모델 구조로, Rumelhart et al., 1986에 근간을 둔다. Deep neural networks (DNNs)와는 달리 hidden state node 간 연결을 통해 이전 시점의 정보를 현재 시점에서 사용할 수 있게 디자인되었다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn.jpg?raw=true" style="zoom: 50%;" />
</p>

<p>현재 시점 node인 $s_t$에 전 시점의 $s_{t-1}$ node에서 정보가 들어온다. 이 정보를 현재 시점의 입력인 $x_t$와 함께 받아 다음 node $s_{t+1}$로 전송될 값을 계산한다. 이 작업을 회귀적으로(recurrently) 진행한다.</p>

<p><br /></p>

<h2 id="weight-sharing">Weight sharing</h2>

<p>Weight $U$, $W$, $V$는 모든 시점에서 동일하다. 이것으로</p>
<ul>
  <li>학습에 필요한 weight 수를 줄일 수 있다.</li>
  <li>데이터의 sequence 길이에 유연하다: 하나의 모델을 다른 길이의 sequence에 적용할 수 있다.
    <ul>
      <li>다른 길이의 sequence에 같은 weight값을 계속 사용함으로써 next token generation이 가능하다</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="rnn-계산">RNN 계산</h2>

<p>위 그림을 보면 hidden state $s_t$와 output $o_t$의 계산은 다음과 같다.</p>

\[\begin{align*}
s_t&amp;=\tau(Ws^{t-1})+Ux^t \\
o_t&amp;=softmax(Vs^t) \\
\end{align*}\]

<p>여기서 node 수가 $D$, $J$, $K$인 경우 각 변수의 차원은 아래와 같다.</p>

\[x\in\mathbb{R}^D, s\in\mathbb{R}^J, o\in\mathbb{R}^K, U\in\mathbb{R}^{J\times D}, W\in\mathbb{R}^{J\times J}, U\in\mathbb{R}^{K\times J}\]

<p><br /></p>

<h2 id="long-term-dependency-problem">Long-term dependency problem</h2>

<p>hidden state 연산은 다음과 같이 표현할 수 있다.</p>

\[s^t=\tau(Ux^t+W\tau(Ux^{t-1}+Ws^{t-2}))\]

<p>$s$가 tanh activation function 내에서 중첩되는 것을 볼 수 있다. 이렇게 되면</p>
<ol>
  <li>feed forward 시 앞 단에서 입력된 정보가 점점 소실된다: tanh의 output은 $\tau(\cdot)\in(-1,1)$인데, 즉 tanh 연산의 중첩은 1보다 작은 값을 계속해서 곱하는 것과 같다. 이렇게 되면 앞에서 곱해진 값은 점점 작아진다.</li>
  <li>back-propagation 시 기울기 소실(gradient vanishing) 혹은 폭발(explosion)의 문제가 생길 수 있다: tanh 함수에 의해 기울기가 0에 가깝게 되거나 너무 커지는 경우가 생긴다. 작은 gradient는 더 작아지고, 큰 gradient는 더 커진다.</li>
</ol>

<p>*Gradient vanishing: back-propagation 시 반영되는 gradient 값이 layer를 지날 수록 소실되는 문제</p>

<p>*Gradient explosion: gradient가 실제 값보다 증폭되어 loss 계산 시 정답과의 차이가 너무 커져, 업데이트에 과도하게 반영되는 문제</p>

<p><br /></p>

<h2 id="다양한-rnn-구조">다양한 RNN 구조</h2>

<p>입출력 형태에 따라 다양하게 RNN을 구성할 수 있다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn_types.png?raw=true" style="zoom: 50%;" />
</p>

<ul>
  <li>One-to-One: hidden state가 1개인 모형, 기본적인 Neural network 구조</li>
  <li>One-to-Many: 하나의 입력값을 받아 순차적으로 여러 개의 값(한 sequence)을 생성</li>
  <li>Many-to-One: 한 sequence를 입력 받아 마지막에 하나의 값을 생성
    <ul>
      <li>e.g. sentence classification</li>
    </ul>
  </li>
  <li>Many-to-Many: 한 sequence를 입력 받아 latent represenation을 구한 후 이것을 통해 sequence를 출력
    <ul>
      <li>e.g. machine translation</li>
    </ul>
  </li>
  <li>
    <p>Many-to-Many: 한 sequence의 매 token을 입력 받는대로 대응하는 token을 출력하여 한 seqeunce를 생성</p>

    <p><br /></p>
  </li>
</ul>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/study/2023/05/20/lstm/">
        Long Short-term Memory (LSTM)
      </a>
    </h1>
    <!--<span class="post-date">20 May 2023</span>-->
    <p class="post-date">20 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/ko/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Recurrent neural network (RNN)의 Long-term dependency 문제를 해결하고자 만들어진 프레임워크이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-lstm/lstm.png?raw=true" style="zoom: 50%;" />
</p>

<p>핵심적인 아이디어는 이전 시점의 state 정보를 이후 state에 얼마나 반영할지를 결정하는 계산을 추가해주는 것이다. 이것을 위해 <strong>forget gate, input gate, output gate</strong>의 3가지 Gate와 <strong>memory cell</strong>이 추가 되었다.</p>

<p>LSTM의 계산 방식과 비교하기 위해 RNN의 계산식을 되짚어보면 다음과 같다.</p>

\[\begin{align*}
h_t&amp;=\tau(Wh^{t-1}+Ux^t) \\
\hat{y}_t&amp;=softmax(Vh^t) \\
\end{align*}\]

<p>이전 시점($t-1$)의 hidden state와 현재 시점($t$)의 input(둘 다 weighted)을 더하여 tanh를 통과시키면 현재 시점의 hidden state가 된다. 이것에 softmax를 취하면 output이 된다.</p>

<p><br /></p>

<h2 id="lstm-계산식">LSTM 계산식</h2>

<p>LSTM은 RNN의 방식에 residual connection 구조에서 착안한 memory 기능을 더하여 long-term dependency 문제를 해결하고자 했다. Memory 기능을 위해 추가된 계산들은 다음과 같다.</p>

<h3 id="forget-gate">Forget gate</h3>

<p>Forget gate $f$는 이전 시점의 정보를 얼마나 잊을지 결정하는 gate이다.</p>

\[f_t=\sigma(W_fh_{t-1}+U_fx_t)\]

<p>이전 시점의 hidden state와 현재 시점의 input을 더한 뒤 sigmoid를 취한다. 이것이 이전 시점의 memory cell state에 곱해진다. sigmoid의 특성에 의해 1에 가까울수록 이전 정보가 이후 많이 반영된다.</p>

<h3 id="input-gate">Input gate</h3>

<p>Input gate $i$는 현재 시점의 input을 다음 시점에 얼마나 반영할지 결정하는 gate이다. 여기서 candidate $\hat(c)$라는 개념이 등장하는데, candidate는 이전 시점의 hidden state와 현재 시점의 input을 고려했을 때 현재의 정보가 어떠한지를 나타내는 cell state의 후보 격인 값이다. 계산 방식이 RNN의 hidden state와 동일하다. input gate 값과 candidate를 곱해 현재의 정보 상 input이 얼마나 반영되면 좋은지를 구하고, 이것을 최종적으로 cell state에 더한다.</p>

\[\begin{align}
i_t&amp;=\sigma(W_{in}h_{_t-1}+U_{in}x_{t})\\
\hat{C}_t&amp;=\tau(W_{c}h_{t-1}+U_{c}x_t)
\end{align}\]

<h3 id="memory-cell">Memory cell</h3>

<p>Memory cell (cell state)은 세 가지 gate와 함께 LSTM의 구현 목적을 위해 추가된 개념이다. 현재 시점의 cell state는 이전 시점의 cell state 및 현재 시점의 forget gate와 현재 시점의 input gate 및 candidate로 계산한다.</p>

\[C_t=f_t*C_{t-1}+i_t*\hat{C}_t\]

<p>$*$는 pointwise operation</p>

<p>이전 정보인 cell state와 현재 input을 얼마나 반영할지가 합해져 현재 시점의 cell state가 구해진다.</p>

<h3 id="output-gate">Output gate</h3>

<p>Output gate는 memory cell을 현재 시점의 hidden state에 얼마나 반영할지 결정한다.</p>

\[\begin{align}
o_t&amp;=\sigma(W_oh_{t-1}+U_ox_t) \\
h_t&amp;=o_t\tau(C_t) \\
&amp;=o_t\tau(f_t*C_{t-1}+i_t*\hat{C}_t) \\
\end{align}\]

<p>현재 시점의 hidden state는 이전 시점의 정보와 현재 시점의 input이 반영된 현재 시점의 cell state와 output gate의 결과값과 곱해져 최종 결정된다.</p>

<h3 id="output">Output</h3>

<p>최종 출력 $\hat{y}_t$은 RNN과 같이 계산된다.</p>

\[\hat{y}_t=softmax(Vh_t)\]

<p><br /></p>

<h2 id="lstm의-한계">LSTM의 한계</h2>

<p>LSTM은 cell state 도입을 통해 gradient vanishing 문제를 해결하고자 하였다. 하지만 RNN 구조를 기반으로 하고 있는 한 이 문제를 완벽하게 해결하기에 한계가 있다. 오히려 gate를 여러 개 사용하여 계산량이 증가하는 문제가 있다.</p>


    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/papers/2023/05/15/alpaca/">
        [Paper] Alpaca: A Strong, Replicable Instruction-Following Model
      </a>
    </h1>
    <!--<span class="post-date">15 May 2023</span>-->
    <p class="post-date">15 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/ko/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="llm">
              <a href="https://alatteaday.github.io/ko/tags/?tag=llm">
                #llm
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Paper Link</a></p>

<h1 id="points">Points</h1>

<ul>
  <li>Alpaca aims to support academic research on instruction-following large language models (LLMs), addressing deficiencies like hallucinations, toxicity, and biases.</li>
  <li>Uses the self-instruct approach to create an instruction-following dataset with text-davinci-003, costing under $500.</li>
  <li>The LLaMA 7B model is fine-tuned using efficient techniques.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>LLMs trained through instruction-following, such as ChatGPT, have significantly impacted daily life. However, these models still face issues like generating misinformation, toxic content, and exhibiting social biases. To address these problems, academic research is essential. Closed-source models hinder this research, making it difficult to study instruction-following models.</p>

<p>Alpaca is a model designed for academic research, fine-tuned from the LLaMA 7B model using 52k instruction-following data generated from OpenAI’s text-davinci-003. Commercial use of Alpaca is prohibitied by following reasons:</p>

<ul>
  <li>Non-commercial license: LLaMA</li>
  <li>Data restrictions: Based on text-davinci-003 prohibiting competition with OpenAI</li>
  <li>Deployment caution: Not designed with adequate safety mesuares for general use.</li>
</ul>

<p><br /></p>

<h1 id="training-recipe">Training Recipe</h1>

<p>To train a high-quality instruction-following model under an academic budget, two key challenges are addressed:</p>

<ol>
  <li>Strong pre-trained language model: LLaMA models</li>
  <li>High-quality instruction-following data: Self-instruct method</li>
</ol>

<h2 id="self-instruct-method">Self-instruct method</h2>

<ul>
  <li>Seed set: 175 human-written instruction-following output pairs from self-instruct seed set.</li>
  <li>Data generation: Prompting text-davinci-003 to generate more instructions using the seed set as examples.</li>
  <li>Efficiency: Improved the self-instruct method, generating 52k unique instructions and outputs for less than $500 using the OpenAI API.</li>
</ul>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-15-alpaca/fig1.png?raw=true" alt="fig1" style="zoom: 50%;" />
</p>

<h2 id="fine-tuning-the-model">Fine-tuning the model</h2>

<ul>
  <li>Process: LLaMA models are fine-tuned with the generated instruction-following dataset using fully shared data parallel (FSDP) and mixed precision trianing.</li>
  <li>Cost and time: Fine-tuning a 7B LLaMA model took 3 hours on eight 80GB A100s, costing less than $100 on most cloud compute providers.</li>
</ul>

<p><br /></p>

<h1 id="preliminary-evaluation">Preliminary Evaluation</h1>

<p>Human evaluation was conducted on inputs from the self-instruct evaluation set. Key findings include:</p>
<ul>
  <li>Comparison: Alpaca 7B vs. text-davinci-003</li>
  <li>Performance: Alpaca wins 90 to 89 comparisons.
    <ul>
      <li>Given Alpaca’s smaller size and limited data, it performed similarly to text-davinci-003.</li>
    </ul>
  </li>
  <li>Generation style: Alpaca’s outputs tend to be similar with text-davinci-003, and reflect the general style of the training dataset.</li>
  <li>Evaluation limitation: The evaluation data’s limitations should be noted.</li>
  <li>An interactive demo was released to gather further feedback.</li>
</ul>

<p><br /></p>

<h1 id="known-limitiations">Known Limitiations</h1>

<p>Alpaca shares common deficiencies with LLMs, such as hallucinations, toxicity, and stereotypes. It struggles particularly with hallucination, sometimes producing well-written misinformation. Despite these issues, Alpaca provides a lightweight model for studying these deficiencies, aiding academic research.</p>

<p><br /></p>

<h1 id="release">Release</h1>

<p>Released assets:</p>
<ul>
  <li>Demo: Interactive demo for evaluation</li>
  <li>Data: 52k demonstrations used to fine-tune Alpaca</li>
  <li>Data generation process: Code for generating the data</li>
  <li>Training code: Fine-tuning code using Hugging Face API</li>
</ul>

<p>Future release:</p>
<ul>
  <li>Model weights: Pending guidance from Meta</li>
</ul>

<p>The release aims to support academic studies on instruction-following LMs and developing new technique to address the existing deficiencies.</p>

<p><br /></p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/papers/2023/05/10/llama/">
        [Paper] Llama: Open and efficient foundation language models (2023)
      </a>
    </h1>
    <!--<span class="post-date">10 May 2023</span>-->
    <p class="post-date">10 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/ko/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="llm">
              <a href="https://alatteaday.github.io/ko/tags/?tag=llm">
                #llm
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.” <em>arXiv preprint arXiv:2302.13971</em> (2023).</p>

<p><a href="https://arxiv.org/abs/2302.13971">Paper Link</a></p>

<h1 id="points">Points</h1>

<ul>
  <li>효율적 inference를 위한 smaller model: LLaMA 모델은 효율성을 위해 대규모 데이터셋으로 학습된 작은 사이즈의 모델을 사용한다. 특히 inference 시 비용 면에서 효율적일 뿐 아니라 state-of-the-art (SOTA)의 성능을 달성했다.</li>
  <li>Publicly available data: 기존의 많은 모델은 공개되지 않는 독점 데이터를 사용해 학습되었다. 이와 달리 LLaMA는 공개된 데이터만으로 학습되어 투명성 및 호환성, 오픈 소스 원칙을 보장한다.</li>
  <li>다양한 Benchmark Performance: LLaMA 모델은 common sense reasoning, question answering, reading comprehension 등 다양한 task에서 경쟁력 있는 성능을 보여주었고, 더 큰 사이즈의 모델을 능가하기도 한다.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>Large language model (LLM)은 최소한의 지시(instruction)나 예제로도 새로운 task를 수행할 수 있는 능력을 보여주었다. 그러나 최근 연구에 따르면 작은 모델을 큰 데이터셋으로 학습하면, 큰 사이즈 모델 이상의 성능을 달성할 수 있다는 것이 보고되었다. 한편 모델을 실시간으로 서빙해야 하는 관점에서 보면 학습 도중의 효율성보다는 inference 시 비용 절감과 효율성 확대가 더 중요하다.</p>

<p><br /></p>

<h1 id="approach">Approach</h1>

<p>LLaMA는 다양한 inference 비용에 맞춰 최적화된 성능을 내도록 설계된 언어 모델 (LM) 시리즈로, 7B부터 65B 파라미터를 갖는다. 모든 모델은 공개된 데이터만을 사용하여 학습되었다.</p>

<h2 id="pre-training-data">Pre-training data</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table1.png?raw=true" style="zoom: 30%;" />
</p>
<p>오픈 소스 원칙을 보장하는, 다양한 도메인의 공개 데이터셋을 사용했다:</p>

<ol>
  <li>English CommonCrawl [67%]: 2017-2020년의 다섯 개 CommonCrawl dump에서 전처리한 데이터를 사용했고, 영어가 아니거나 품질이 낮은 콘텐츠는 필터링했다.</li>
  <li>C4 [15%]: CommonCrawl과 유사하게 전처리되었다. 이 전처리 방식이 성능 향상에 도움이 되는 것으로 보인다.</li>
  <li>Github [4.5%]: Google BigQuery에서 line 수와 알파벳 문자 비율을 기준으로 필터링하여 구성하였다.</li>
  <li>Wikipedia [4.5%]: 2022년 중반의 덤프 데이터로 여러 언어를 포함한다.</li>
  <li>Gutenberg and Books3 [4.5%]: 공개적으로 사용 가능한 서적 데이터로, 중복 콘텐츠를 제거했다.</li>
  <li>ArXiv [2.5%]: 과학 관련 내용을 포함하는 데이터로, 필수적이지 않은 내용은 제거했다.</li>
  <li>Stack Exchange [2%]: 점수에 따라 정렬하여 퀄리티가 좋은 것을 골라낸 Q&amp;A 콘텐츠 데이터이다.</li>
</ol>

<h3 id="tokenization">Tokenization</h3>

<ul>
  <li>Byte Pair Encoding (BPE) tokenizer를 사용했다.</li>
  <li>수를 개별 숫자 단위로 나누고, 알 수 없는 UTF-8 문자는 분해했다.</li>
  <li>
    <p>학습 데이터는 중복을 최소화하여 약 1.4T 토큰을 포함한다 (fig 1).</p>

    <p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig1.png?raw=true" alt="fig1" style="zoom: 30%;" />
</p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<p>LLaMA 모델은 Transformer 구조를 기반으로 하는데, 몇 가지 수정된 사항이 있다:</p>

<ol>
  <li>Pre-normalization [GPT3]: 각 Transformer 하위 레이어의 입력을 RMSNorm을 사용해 정규화하여 학습 안정성을 강화했다.</li>
  <li>SwiGLU activation function [PaLM]: ReLU 대신 SwiGLU를 사용해 성능을 향상시켰다. PaLM에서 사용된 $4d$ 대신 $2\over3 4d$ 차원을 사용한다.</li>
  <li>Rotary Embeddings [GPTNeo]: 각 레이어에서 absolute positional embedding 대신 Rotary embedding (RoPE)을 사용했다.</li>
</ol>

<h2 id="optimizer">Optimizer</h2>

<p>AdamW optimizer를 사용해 학습한다. 최적화 옵션은 다음과 같다:</p>

<ul>
  <li>$\beta_1=0.9, \beta_2=0.95$.</li>
  <li>최대 학습률의 10%로 끝나는 Cosine learning rate schedule.</li>
  <li>Weight decay 0.1과 gradient clipping 1.0.</li>
  <li>
    <p>모델 크기에 따라 다양한 leanring rate와 batch size로 2,000 warmup-steps (table 2).</p>

    <p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table2.png?raw=true" alt="table2" style="zoom: 30%;" />
</p>
  </li>
</ul>

<h2 id="efficient-implementation">Efficient implementation</h2>

<ol>
  <li>Causal multi-head attention: xformer library를 사용해 메모리와 실행 시간을 효율적으로 줄이고자 하였다.</li>
  <li>Activation reductions: 체크포인팅을 사용해 backward pass 동안, 특히 계산 비용이 많이 드는 레이어에 대해 activation을 다시 계산한다.</li>
</ol>

<p><br /></p>

<h1 id="main-results">Main Results</h1>

<p>20개의 벤치마크에서 zero-shot 및 few-shot task로 평가했고, GPT-3, Gopher, Chinchilla, PaLM 등 비공개 모델 및 OPT, GPT-J, GPT-Neo 등의 오픈 소스 모델과 결과를 비교했다.</p>

<h2 id="common-sense-reasonging">Common sense reasonging</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table3.png?raw=true" alt="table3" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA의 8개 표준 벤치마크에 대해 평가했다. 이 데이터셋들에는 Cloze 및 Winograd style task와 multiple choice question answering (QA)이 포함된다.</li>
  <li>Results
    <ul>
      <li>LLaMA-65B는 대부분의 벤치마크에서 Chinchilla 70B와  PaLM-540B를 능가했다.</li>
      <li>LLaMA-13B는 훨씬 작은 사이즈의 모델임에도 대부분의 벤치마크에서 GPT-3보다 좋은 성능을 보였다.</li>
    </ul>
  </li>
</ul>

<h2 id="close-book-question-answering">Close-book question answering</h2>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table4.png?raw=true" alt="table4" style="zoom: 30%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table5.png?raw=true" alt="table5" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: Natural Questions과 TriviaQA. 모델은 질문에 대한 답에 관련된 단서를 참조하지 않고도 답을 얼마나 잘 맞추는지를 평가 받는다.</li>
  <li>Results:
    <ul>
      <li>LLaMA-65B zero-shot과 few-shot 세팅 모두에서 state-of-the-art (SOTA) 성능을 달성했다.</li>
      <li>LLaMA-13B은 더 큰 모델인 GPT-3와 Chinchilla에 뒤쳐지지 않는 성능을 보였다.</li>
    </ul>
  </li>
</ul>

<h2 id="reading-comprehension">Reading comprehension</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table6.png?raw=true" alt="table6" style="zoom: 30%;" />
</p>

<ul>
  <li>Benchmark: RACE reading comprehension 벤치마크를 사용했다. 중국 중고등학교 영어 독해 시험에서 수집되었다.</li>
  <li>Results: LLaMA-65B는 PaLM-540B와 유사한 성능을 보였고, LLaMA-13B는 GPT-3을 능가했다.</li>
</ul>

<h2 id="mathematical-reasoning">Mathematical reasoning</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table7.png?raw=true" alt="table7" style="zoom: 30%;" />
</p>

<ul>
  <li>Benchmarks: MATH와 GSM8k 벤치마크를 사용했다. MATH는 12,000개 중고등학교 수학문제, GSM8k는 중학교 수준 수학 문제로 구성된다.</li>
  <li>Results: LLaMA-65B는 GSM8k에서 Minerva-62B를 뛰어 넘은 성능을 보였다.
    <ul>
      <li>Minerva는 ArXiv와 Math Web Pages에서 추출한 38.5B개 토큰으로 fine-tune된 PaLM model 시리즈이다. 한편 PaLM과 LLaMA는 수학 문제 데이터에 finetune 되지 않았다.</li>
    </ul>
  </li>
</ul>

<h2 id="code-generation">Code generation</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table8.png?raw=true" alt="table8" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: HumanEval과 MBPP 벤치마크를 사용했다. 모델은 자연어로 묘사한 내용을 보고 코드를 작성하는 능력에 대해 평가된다.</li>
  <li>Results:
    <ul>
      <li>LLaMA 모델은 LaMDA와 PaLM을 포함한 다른 모델을 능가한다. LLaMA-13B는 LaMDA-137B를, LLaMA 65B는 PaLM-62B보다 좋은 성능을 보였다.</li>
      <li>코드에 특화된 데이터로 fine-tune하면 성능이 더욱 향상되는 것을 볼 수 있었다.</li>
    </ul>
  </li>
</ul>

<h2 id="massive-multitask-language-understanding">Massive multitask language understanding</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table9.png?raw=true" alt="table9" style="zoom: 30%;" />
</p>
<ul>
  <li>Massive multitask language understanding (MMLU): MMLU는 인문학, STEM, 사회과학 등 다양한 영역의 지식을 포괄하는 다중 선택 질문으로 구성된다.</li>
  <li>Results: LLaMA-65B는 Chinchilla-70B와 PaLM-540B에 비해 낮은 성능을 보였는데, 이것은 학술적인 데이터를 다른 모델 만큼 충분히 학습하지 않았기 때문인 것으로 추측된다.</li>
</ul>

<h2 id="evolution-of-performance-during-training">Evolution of performance during training</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig2.png?raw=true" alt="fig2" style="zoom: 30%;" />
</p>
<ul>
  <li>Result: 성능은 학습 중 지속적으로 향상된다. 또한 성능과 모델의 복잡성 간 상관관계가 나타났다.</li>
  <li>SIQA와 WinoGrande에 대해서는 예외적인 결과를 보였다: SIQA의 경우 성능의 변동이 나타나는 것으로 보아 신뢰하기 어려운 벤치마크일 가능성이 있다. WinoGrande에서는 모델 복잡성과 성능 간 관계성이 나타나지 않았다.</li>
</ul>

<p><br /></p>

<h1 id="instruction-fine-tuning">Instruction Fine-tuning</h1>

<p>Fine-tuning은 성능을 향상시키고 instruction을 따르는 능력을 개선한다. LLaMA-I는 MMLU에 대해 instruction과 함께 fine-tune한 모델이다. 이를 비슷한 사이즈를 가지는 finetuned 모델인 OPT-IML 및 Flan-PaLM 시리즈와 비교했다.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table10.png?raw=true" alt="table10" style="zoom: 30%;" />
</p>
<ul>
  <li>65B 파라미터의 LLaMA-I는 기존 instruction fine-tuned 모델보다 좋은 성능을 보였다. 그러나 GPT ‘code-davinci-002’에는 미치지 못했다.</li>
</ul>

<p><br /></p>

<h1 id="bias-toxicity-and-misinformation">Bias, Toxicity and Misinformation</h1>

<p>LLM은 학습 데이터의 내용에 따라 편향을 가질 수 있으며, 공격적인(toxic/offensive) 콘텐츠를 생성할 수 있다. LLaMA 모델은 웹에서 수집한 데이터를 많이 학습했기 때문에 이러한 콘텐츠 생성의 가능성을 확인할 필요가 있다. toxic content generation 및 stereotypes detection을 평가하기 위해 다양한 벤치마크를 사용했다.</p>

<h2 id="realtoxicityprompts">RealToxicityPrompts</h2>

<p>모델이 toxic 콘텐츠를 얼마나 생성하는지를 평가하는 벤치마크이다. 모델은 약 10만개의 프롬프트를 완성하고, 점수는 PerspectiveAPI에 의해 0(non-toxic)부터 1(toxic)로 자동 평가된다.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table11.png?raw=true" alt="table11" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA는 다른 모델과 비슷한 toxic score를 얻었다. 예를 들어 Chinchilla의 경우 0.087의 toxicity score를 보였다.</li>
  <li>모델이 클수록 toxicity가 강하게 나타났다. 이전 연구에서도 유사한 결과가 있었다. 한편 Gopher와 Chinchilla의 경우 Gopher가 사이즈가 더 작음에도 Chinchilla보다 더 toxic하다고 평가된 바가 있어 예외적이었다. 이를 고려할 때 toxicity와 모델 크기 간 관계가 같은 모델 시리즈 내에서만 적용된다고 짐작할 수 있다.</li>
</ul>

<h2 id="crows-pairs">CrowS-Pairs</h2>

<p>모델의 bias를 9개 카테고리에 따라 평가한다: gender, religion, race, sexual orientation, age, nationality, disability, physical appearance 및 socioenconomic status.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table12.png?raw=true" alt="table12" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA는 특히 religion, age 및 gender 카테고리에서 약간의 bias를 보였다. 모델이 학습한CommonCrawl 데이터에서 비롯된 것일 수 있다.</li>
</ul>

<h2 id="winogender">WinoGender</h2>

<p>모델의 gender 카테고리에 대한 bias를 체크하기 위한 벤치마크이다. 모델의 co-reference resolution 성능이 성별 관련 대명사에 영향을 받는지를 평가한다. 특히 직업과 관련된 사회적 편견을 모델이 학습했는지를 볼 수 있다.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table13.png?raw=true" alt="table13" style="zoom: 30%;" />
</p>
<ul>
  <li>성능은 성별 대명사의 종류에 따라 다양하게 나타났다: “her/her/she”와 “his/him/he” 대명사에 관한 성능보다 “their/them/someone” 대명사에서 관한 성능이 더 좋다.</li>
  <li>큰 모델이 더 큰 gender bias를 가졌다: “gotcha” 사례의 경우, LLaMA-65B가 더 큰 에러를 보이며 gender에 대해 더 편향되어 있음을 보였다.
    <ul>
      <li>“gotcha”는 해당 사례 내 대명사가 보편적이라고 인식되는 직업의 대명사와 일치하지 않는데, 그것이 옳은 답변인 경우를 말한다.</li>
    </ul>
  </li>
</ul>

<h2 id="truthfulqa">TruthfulQA</h2>

<p>모델이 내용의 진위 여부를 판단할 수 있는지를 평가하고, 잘못된 정보를 생성할 위험을 얼마나 갖는지 측정한다. 모델 답변의 truthfulness를 평가한다.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table14.png?raw=true" alt="table14" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA 모델은 GPT-3보다 나은 결과를 보였다. 하지만 여전히 정답률이 낮기 때문에 잘못된 정보를 생성할 가능성이 있다.</li>
</ul>

<p><br /></p>

<h1 id="carbon-footprint">Carbon footprint</h1>

<p>모델 훈련과 배포에 있어 환경에 미치는 영향을 설명한다.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table15.png?raw=true" alt="table15" style="zoom: 30%;" />
</p>

<p><br /></p>


    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/ko/papers/2023/04/30/rlhf/">
        [Paper] Training language models to follow instructions with human feedback (2022)
      </a>
    </h1>
    <!--<span class="post-date">30 Apr 2023</span>-->
    <p class="post-date">30 Apr 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/ko/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="llm">
              <a href="https://alatteaday.github.io/ko/tags/?tag=llm">
                #llm
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <style>
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: 2.5em;
   margin-bottom: -0.5em;
   margin-left: 0em;
   margin-right: 0em;
}
</style>

<p>Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” <em>Advances in neural information processing systems</em> 35 (2022): 27730-27744.</p>

<p><a href="https://arxiv.org/abs/2203.02155">Paper Link</a></p>

<h1 id="point">Point</h1>

<ul>
  <li>Employs <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> to fine-tune GPT-3 models, aligning them with human intentions while reducing unintended behaviors like hallucinations and toxicity.</li>
  <li><strong>InstructGPT</strong> models outperforms GPT-3 in truthfulness and reliability, generalizing well to new tasks like non-English and coding instructions.</li>
  <li>Highlights the need for diverse stakeholder input and suggest combining RLHF with other methods to improve model alignment and safety.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>Language models (LMs) often generate misinsformation, toxic or biased content and this issue cannot be resolved simply by increasing the model size. Understanding user intent is crucial for these models. Fine-tuning with human feedback can align the models with user intentions across various tasks.</p>

<p>Large language models (LLMs) frequently exhibit uninteded behaviors, such as hallucinations, toxic text generation, failing to follow user instructions. These are influenced by the model’s objective, which typically involves predicting the next token based on web data, differing from the goal of “following the user instructions helpfully and safely”.</p>

<p>To align LMs, this paper employs <strong>Reinforcement Learning from Human Feedbak (RLHF)</strong> to fine-tune GPT-3 to follow instructions. Human preferences serve as a reward signal for this fine-tuning process.</p>

<p><br /></p>

<h1 id="methods-and-experimental-details">Methods and experimental details</h1>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig2.png?raw=true" alt="fig2" style="zoom: 50%;" />
</p>

<h2 id="high-level-methology">High-level methology</h2>

<ol>
  <li>Preparation: Utilize pre-trained language models (GPT-3), prepare a distribution of prompts for alignment, and train human labelers.</li>
  <li>Collect demonstration data and train a supervised policy: Labelers provide input prompts as desired behavior responses. The model is fine-tuned on this data using supervised learning.</li>
  <li>Collect comparison data and train a reward model: Labelers compare model outputs and indicate their preferences. A reward model (RM) is trained using these comparisons to predict human-preferred outputs.</li>
  <li>Optimize a policy aganst the RM using PPO: The RM’s output serves as a scalar reward. The supervised policy (trained GPT-3) is fine-tuned using the PPO algorithm to optimize this reward.</li>
</ol>

<p>Step 2 and 3 can be iterative: More comparison data is collected on the current best policy, used to train a new RM and subsequently a new policy.</p>

<p><br /></p>

<h2 id="dataset">Dataset</h2>

<p>Source of prompts:</p>
<ul>
  <li>Consists of text prompts submitted to the OpenAI API, specifically those using an earlier version of InstructGPT models on the Playground interface.</li>
  <li>The paper does not include data from customers using the API in production.</li>
</ul>

<p>Deduplication and filtering:</p>
<ul>
  <li>Heuristically deduplicated by checking for prompts that share a long common prefix.</li>
  <li>The number of prompts is limited to 200 per user ID.</li>
  <li>Validation and test sets contain no data from users whose data is in the training set.</li>
  <li>All prompts in the training split were filtered for personally indentifiable information (PII).</li>
</ul>

<p>Initial source of prompts: Human-written prompts were used as an initial source of instruction to bootstrap the process.</p>

<p>Datasets for fine-tuning:</p>
<ul>
  <li>SFT dataset: Labelers’ demonstrations (13k prompts, from the API and labeler-written examples).</li>
  <li>RM dataset: Labeler rankings of model outputs (33k, from the API and labeler-written examples).</li>
  <li>PPO dataset: Inputs for RLHF fine-tuning. Human labels were not used (31k, only from the API).</li>
</ul>

<p>Use cases: Most of the use-cases have are generative, rather than classification of prompts submitted to InstructGPT models</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true" alt="fig1" style="zoom: 35%;" />
</p>

<p><br /></p>

<h2 id="tasks">Tasks</h2>

<p>Datasets for training tasks</p>
<ul>
  <li>Sources: The datasets are sourced from prompts written by labelers and those submitted to early versions of InstructGPT models via API.</li>
  <li>Labeler Instructions: Labelers are trained and instructed to write prompts with specific intents or implicit goals in mind to ensure the model aligns with desired behaviors.</li>
  <li>Language: The datasets are predominately in English (95%). However, the paper also reports the models’ performance in other languages.</li>
</ul>

<p><br /></p>

<h2 id="human-data-collection">Human data collection</h2>

<p>Selection of Labelers: A diverse group of labelers was selected to ensure a broad demographic representation. It aims to generate inputs with a wide range of perspectives and to identify potentially harmful outputs.</p>

<p>Training and Evaluation: Labelers underwent tests designed to measure their performance in labeling according to the set standards. This included their ability to generate diverse prompts and accurately identify harmful content.</p>

<p><br /></p>

<h2 id="models">Models</h2>

<p>Pre-trained GPT models are utilized as basis. These models are trianed on a broad distribution of Internet data and can be used for various tasks but initially exhibit poorly characterized behavior. The GPT-3 models are then further trained using three different techniques:</p>

<p><br /></p>

<h3 id="supervised-fine-tuning-sft">Supervised fine-tuning (SFT)</h3>

<p>This method fine-tunes GPT-3 on labeler demonstrations using supervised learning.</p>

<ul>
  <li>Training details: 16 epochs using a cosine learing rate decay and a residual dropout of 0.2.</li>
  <li>Model selection: Based on the model’s RM score on the validation set.</li>
  <li>Finding: Training for more epochs improves both the RM score and human preference ratings, depite some overfitting.</li>
</ul>

<p><br /></p>

<h3 id="reward-modeling-rm">Reward modeling (RM)</h3>

<ul>
  <li>Base model: Starts with a pre-trained SFT model but the final unembedding layer is removed. This layer maps the model’s representations to the vocabulary space for generating output tokens.</li>
  <li>Input and output: The model takes a prompt and a response are as input and outputs a scalr reward, representing theh quality of the response for the given prompt.</li>
  <li>Model size: Utilizes 6B reward model (RM) for efficiency. A larger 175B RM was found to be unstable and unsuitable for use as the value function in RL.</li>
  <li>Data: Uses comparisons between two model outputs for the same input to determine which output is preferred by human labelers.</li>
  <li>Loss: Trained with cross-entropy loss, using the comparisons as labels. The reward difference reflect the log odds of one response being preferred over the other by a labeler.</li>
  <li>Speed-up comparison collection: Labelers are presented with $K$ responses to rank for each prompt, where $K$ ranges from 4 to 9. This results in $K(K-1) \over 2$ comparisons for each prompt.</li>
  <li>Training efficiency and overfitting:
    <ul>
      <li>Comparisons within each labeling task are very correlated. If all comparisons are shuffled into one dataset and processed in a single pass, the model tends to overfit.</li>
      <li>To address this, the training treats all $K(K-1) \over 2$ comparisons from each prompt as a single batch element, offering several benefits:
        <ul>
          <li>Requires only one forward pass for each set of $K$ responses, instead of $K(K-1) \over 2$ forward passes.</li>
          <li>Prevents overfitting by avoiding isolated highly correlated comparisons.</li>
          <li>Improves computational efficiency, and achieves better validation accuracy and log loss.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Loss function:</p>

\[loss(\theta)=-{1\over \binom{K}{2}} E_{(x,y_w,y_l)~D}[\log(\sigma(r_\theta(x,y_w)-r_\theta(x,y_l)))]\]

    <ul>
      <li>$r_\theta(x,y)$ is the scalar output of the RM for promt $x$ and completion $y$ with parameters $\theta$.</li>
      <li>$y_w$ is preferred completion out of the pair of $y_w$ and $y_l$.</li>
      <li>$D$ is the dataset of human comparisons.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="reinforcement-learning-rl">Reinforcement learning (RL)</h3>

<ul>
  <li>Base model: The SFT model is fine-tuned using Proximal Policy Optimization (PPO) in an environment.</li>
  <li>Training environment: A bandit environment. It this context, a bandit environment presents a random customer prompt, expects a response, produces a reward determined by the RM, and ends the episode.</li>
  <li>Input and output: The model takes the prompt and response as input and outputs a reward determined by the RM.</li>
  <li>KL penalty: A per-token Kullback-Leibler (KL) penalty is added from the SFT model at each token.
    <ul>
      <li>This penalty mitigates over-optimization of the RM and prevents the model from deviating too far from the behavior learned during supervised fine-tuning.</li>
      <li>The value funciton used in PPO is initialized from the RM.</li>
    </ul>
  </li>
  <li>PPO and PPO-ptx models:
    <ul>
      <li>PPO models: Fine-tuned with PPO.</li>
      <li>PPO-ptx models: Involve an additional experiment where pre-training gradients are mixed into PPO gradients to address performance regressions on public NLP datasets.</li>
      <li>
        <p>The objective function for PPO-ptx:</p>

\[\begin{aligned}
\text{objective}(\phi) = &amp; \ \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{RL}}} \left[ r_\theta(x, y) - \beta \log \left( \frac{\pi_\phi^{RL}(y | x)}{\pi^{SFT}(y | x)} \right) \right] \\
&amp; + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log(\pi_\phi^{RL}(x)) \right]
\end{aligned}\]

        <p>where:</p>

        <ul>
          <li>$\pi_\phi^{RL}$ is the learned RL policy and $\pi^{SFT}$ is the supervised fine-tuned model.</li>
          <li>$D_{\pi^{RL}}$ is the distribution of data under the RL policy, and $D_{pretrain}$ is the pre-training distribution.</li>
          <li>$\beta$ is the KL reward coefficient, controlling the strength of the KL penalty.</li>
          <li>$\gamma$ is the pre-training loss coefficient, controlling the influence of pre-training gradients. For PPO models $\gamma$ is set to 0.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In this paper, InstructGPT refers to the PPO-ptx models.</li>
</ul>

<p><br /></p>

<h3 id="baselines">Baselines</h3>

<p>The performance of PPO models is compared against several baselines:</p>
<ul>
  <li>SFT models: Fine-tuned using supervised learing.</li>
  <li>GPT-3: The standard GPT-3 model without additional fine-tuning.</li>
  <li>GPT-3 Prompted: Provided with a few-shot previx to prompt it into an instruction-following mode, where the prefix is prepended to the user-specified instruction.</li>
  <li>InstructGPT is compared to 175B GPT-3 models fine-tuned on FLAN and T0 datasets. These datasets include various NLP tasks combined with natural language instructions.</li>
</ul>

<p><br /></p>

<h2 id="evaluation">Evaluation</h2>

<p>The definition of “alignment” to evaluate models is based on their ability to act in accordance with user intentions. The practical evaluation framework checks if the model is helpful, honest and harmless.</p>

<ul>
  <li>Helpfulness: The model should follow instructions and infer intentions from prompts or a patterns.
    <ul>
      <li>Since the intention could be unclear, labeler preference ratings are considered mainly for evaluation.</li>
      <li>There may be divergence between actual user intentions and labeler interpretations.</li>
    </ul>
  </li>
  <li>Honesty: Truthfulness is measured instead of comparing the model’s output to its actual belief.
    <ul>
      <li>Two metrics are used:
        <ul>
          <li>The model’s tendency to fabricate information on closed domain tasks</li>
          <li>Performance on the TruthfulQA dataset.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Harm: Harmfulness depends on the context in which the model is used, and assessing potential harm requires significatn speculation.
    <ul>
      <li>More specific proxy criteria are used:
        <ul>
          <li>Whether a deployed model could be harmful.</li>
          <li>Labelers evaluate if an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content.</li>
          <li>Benchmarks like RealToxicityPrompts and CrowS-pairs are used to measure bias and toxicity.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="evaluation-on-api-distiribution">Evaluation on API distiribution</h3>

<p>When using prompts from the API for evaluting human preference ratings, only prompts not included in training are selected.</p>

<p>Since prompts for InsturctGPT models are not suitable for the GPT-3 baselines, prompts submitted to the GPT-3 API are also used for evaluation.</p>
<ul>
  <li>The GPT-3 prompts are not in an instruction-following style.</li>
  <li>The 175B SFT model is chosen as the baseline due to its average performance.</li>
</ul>

<p>Each model is evaluated based on how often its outputs are preferred, and labelers judge the overall quality of each response on a 1-7 Likert scale.</p>

<p><br /></p>

<h3 id="evaluation-on-public-nlp-datasets">Evaluation on public NLP datasets</h3>

<p>Two types of public datasets are used:</p>
<ul>
  <li>Safety evaluation: Focuses on truthfulness, toxicity, and bias. Includes evaluations of toxicity using the RealToxicityPrompts dataset.</li>
  <li>Zero-shot performance: Assesses performance on traditional NLP tasks such as question anwering (QA), reading comprehension, and summarization.</li>
</ul>

<p><br /></p>

<h1 id="results">Results</h1>

<p>The experimental results are organized into three parts: results on the API prompt distribution, results on public NLP datasets, and qualitative results.</p>

<h2 id="results-on-the-api-distribution">Results on the API distribution</h2>

<h3 id="1-labelers-significantly-prefer-instructgpt-outputs-over-outputs-from-gpt-3">1. Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true" alt="fig1" style="zoom: 35%;" />
</p>

<ul>
  <li>175B InstructGPT outputs are preferred to GPT-3 outputs around 85% of the time and around 71% compared to few-shot GPT-3.</li>
  <li>The preference order is GPT-3 &lt; GPT-3 Prompted &lt; SFT &lt; PPO.</li>
  <li>Adding updates on the pre-training mix during PPO does not lead to significant changes in labeler preference.</li>
</ul>

<p class="a" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig3.png?raw=true" alt="fig3" style="zoom: 35%;" />
</p>

<ul>
  <li>This preference trend remains consistent when evaluating models on prompts submitted to GPT-3 models on the API, though PPO-ptx models perform slightly worse at larger sizes.</li>
</ul>

<p class="a" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig4.png?raw=true" alt="fig4" style="zoom: 35%;" />
</p>

<ul>
  <li>InstructGPT outputs are rated favorably on more concrete axes: They follow constraints and instruction better and hallucinate less.</li>
  <li>This suggests that InstructGPT models are more reliable and easier to control than GPT-3.</li>
</ul>

<p><br /></p>

<h3 id="2-instructgpt-models-generalize-to-the-preferences-of-held-out-labelers-that-did-not-produce-any-training-data">2. InstructGPT models generalize to the preferences of “held-out” labelers that did not produce any training data.</h3>

<ul>
  <li>InstructGPT models’ outputs are rated better than GPT-3 baselines by held-out labelers, indicating InstructGPT models are not simiply overfitting to the preferences of training labelers.</li>
  <li>RMs also demonstrate generlization capabilties with cross-validation results: 69.6% accuracy in predicting the preferences of held-out labelers, which is slightly lower than 72.4% accuracy in the predicting preferences within the training set.</li>
</ul>

<p><br /></p>

<h3 id="3-public-nlp-datasets-are-not-reflective-of-how-the-lms-are-used">3. Public NLP datasets are not reflective of how the LMs are used.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig5.png?raw=true" alt="fig5" style="zoom: 35%;" />
</p>

<ul>
  <li>When comparing InstructGPT to 175B GPT-3 baseline fine-tuned on FLAN and T0, these models perform better than GPT-3 with a good prompt but worse than the SFT baseline. This suggests the datasets are not sufficiently diverse to improve API prompt distribution.</li>
  <li>InstructGPT may outperform FLAN and T0 because:
    <ul>
      <li>Public NLP datasets are desinged to capture typical tasks that are easy to evaluate (e.g., classification, QA). However, open-ended generation and brainstorming constitute most (57%) of tasks the API users want.</li>
      <li>Public NLP datasets may lack the high diversity of inputs that real-world users are interested in.</li>
    </ul>
  </li>
</ul>

<h2 id="results-on-public-nlp-datasets">Results on public NLP datasets</h2>

<h3 id="1-instructgpt-models-show-improvements-in-truthfulness-over-gpt-3">1. InstructGPT models show improvements in truthfulness over GPT-3.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig6.png?raw=true" alt="fig6" style="zoom: 35%;" />
</p>

<ul>
  <li>PPO models demonstrate significant improvements on the TruthfulQA dataset.</li>
  <li>The 1.3B PPO-ptx model performs slightly worse than GPT-3 of the same size.</li>
  <li>Training with an “Instruction+QA” prompt helps the model avoid generating false information.
    <ul>
      <li>Instruction+QA: Instructs the model to respond with “I have no comment” when it’s uncertain of the correct answer.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="2-instructgpt-shows-small-improvements-in-toxicity-over-gpt-3-but-not-bias">2. InstructGPT shows small improvements in toxicity over GPT-3, but not bias.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig7.png?raw=true" alt="fig7" style="zoom: 35%;" />
</p>

<ul>
  <li>Toxicity: Evaluated using the RealToxicityPrompts benchmark.
    <ul>
      <li>Evaluation method: Toxicity scores are obtained through the Perspective API with model samples and labelers rate the samples.</li>
      <li>InstructGPT outputs are less toxic than those of GPT-3 when instructed to generate respectful outputs. Without any prompt, the models are similar, and InstructGPT can be more toxic when prompted to produce toxic content.</li>
    </ul>
  </li>
  <li>Bias: Evaluated using the Winogender and CrowS-Pairs benchmarks.
    <ul>
      <li>Evaluation method: Calculates the relative probabilities of producing sentences in each pair and the entropy of the associated binary probability distributions.
        <ul>
          <li>Unbiased models will show no preference, thus having maximum entropy.</li>
        </ul>
      </li>
      <li>InstructGPT and GPT-3 show similar levels of bias. The PPO-ptx model shows higher bias when instructed to act respectfully, with unclear patterns.</li>
      <li>Instructed models tend to be more certain of their outputs, regardlessly with stereotypes.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="3-modifying-rlhf-fine-tuning-procedures-can-minimize-performance-regressions-on-public-nlp-datasets">3. Modifying RLHF fine-tuning procedures can minimize performance regressions on public NLP datasets.</h3>

<ul>
  <li>Alignment tax: PPO model experience a decrease in performance on public NLP datasets, referred to as “alignment tax”.</li>
</ul>

<p style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig28.png?raw=true" alt="fig28" style="width: 49%; vertical-align:text-top;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig29.png?raw=true" alt="fig29" style="width: 49%; vertical-align:text-top;" />
</p>

<ul>
  <li>Mitigation strategies: Mixing pre-training updates to the PPO fine-tuning (PPO-ptx) reduces performance regressions across all datasets.</li>
</ul>

<p class="a" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig33.png?raw=true" alt="fig33" style="zoom: 35%;" />
</p>

<ul>
  <li>PPO-ptx performs better than merely increasing the KL coefficient. Changing the KL model from the PPO initialization to GPT-3 yields similar improvements.</li>
</ul>

<p><br /></p>

<h2 id="qualitative-results">Qualitative results</h2>

<h3 id="1-instructgpt-models-show-promising-generlization-to-instructions-outside-of-the-rlhf-fine-tuning-distribution">1. InstructGPT models show promising generlization to instructions outside of the RLHF fine-tuning distribution.</h3>

<ul>
  <li>InstructGPT models can follow non-English instructions, and perform coding tasks, despite limited training data in these formats.</li>
  <li>Alignment methods can generalize to produce desired behaviors on inputs not directly supervised.</li>
</ul>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig8.png?raw=true" alt="fig8" style="zoom: 35%;" />
</p>

<ul>
  <li>175B PPO-ptx model can answer questions about code and non-English instructions, but often responds in English to questions in other languages.</li>
</ul>

<p><br /></p>

<h3 id="2-instructgpt-still-makes-simple-mistakes">2. InstructGPT still makes simple mistakes.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig9.png?raw=true" alt="fig9" style="zoom: 35%;" />
</p>

<ul>
  <li>The model sometimes incorrectly assumes a false premise in an instruction is true.</li>
  <li>It can overly hedge even when the answer is clear.</li>
  <li>It struggles with generating responses when there’re multiple or challenging constraints in an instruction.</li>
</ul>

<p><br /></p>

<h1 id="discussion">Discussion</h1>

<h2 id="implications-for-alignment-research">Implications for alignment research</h2>

<p>Improving the alignment of current AI systems provides a clear empirical feedback loop, esssential for refining alignment techniques.</p>

<p>Moreover, RLHF is an important building block for aligning superhuman systems, especially for tasks difficult to evaluate.</p>

<p>General lessons for alignment research:</p>

<ul>
  <li><strong>The cost of increasing model alignment is modest relative to pre-training</strong>: The significant costs lie in data collection and computation. With RLHF, larger LMs become more helpful, suggesting investing in aligning existing LMs is more efficient than training new, larger models.</li>
  <li><strong>There is evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in</strong>: E.g., non-English and code tasks. This is important as creating supervised models for each task is expensive.</li>
  <li><strong>The proposed fine-tuning can mitigate most of the performance degradations</strong>: Low alignment tax techniques are needed for future AI systems capable of understanding human intents, and RLHF is effective in this regard.</li>
  <li><strong>Alignment techniques are validated in the real world</strong>: This work grounds alignment research in real-world applications, providing valuable insights for AI systems used by actual users.</li>
</ul>

<p><br /></p>

<h2 id="who-are-we-aligning-to">Who are we aligning to?</h2>

<p>Factors influencing the fine-tuning data and key sources of alignment preferences:</p>

<ul>
  <li>Labelers’ preferences: The models are aligned to the preferences of hired labelers who generate the training data. They are mostly English speakers, with around 73% agreement among them.</li>
  <li>Researchers’ preferences: Researchers design the study, write instructions, and guide labelers on edge cases, thereby influencing the alignment. More research is needed to understand the impact of different instructions and interfaces on the collected data and model behavior.</li>
  <li>Customer prompts: Training data includes prompts from OpenAI customers using the API. There is potential misalignment between customer goals and end-user well-being.</li>
  <li>Customer representation: The customers are not representative of all potential or current LM users. The initial user base was biased towards OpenAI’s networks.</li>
</ul>

<p>Challenges and future directions:</p>

<ul>
  <li>Designing a fair and transparent alignment process is complex.</li>
  <li>This paper demonstrates that the alignment method can work for a specific human reference group but doesn’t claim these group preferences are ideal.</li>
  <li>Multiple stakeholders need consideration, including model trainers, developers, end-users, and the broader impacted population.</li>
  <li>Aligning a system to everyone’s preferences simultaneously is impossible, and not all trade-offs will be universally endorsed.</li>
  <li>One potential approach is to train models for different group preferences so that it can reflect diverse values. However, this may still impact broader society, raising decisions about prioritizing preferences.</li>
</ul>

<p><br /></p>

<h2 id="limitations">Limitations</h2>

<p>Methodology:</p>

<ul>
  <li>Contractor influence: InstructGPT is influenced by the human feedback from about 40 contractors.
    <ul>
      <li>Contractors’ identity, beliefs, cultural backgrounds, and personal history may affect their judgments.</li>
      <li>They were selected based on their performance with sensitive prompts and labeling tasks.</li>
      <li>The small team size allowed for better communication but is not representative of the broader population will use the models.</li>
      <li>They are mostly  English-speaking, and the data is almost entirely in English.</li>
    </ul>
  </li>
  <li>Data collection improvements: Most comparisons are labeled by only one contractor to reduce costs.
    <ul>
      <li>Multiple labelings could help identify disagreement areas, indicating where a single model may not align with all labelers.</li>
      <li>Averaging labeler preferences for disagreements might not be ideal, especially for minority groups, whose preferences should be weighted more heavily.</li>
    </ul>
  </li>
</ul>

<p>Models:</p>
<ul>
  <li>Imcomplete alignment and safety: InstructGPT is not fully aligned or safe.
    <ul>
      <li>It still generates toxic or biased outputs, misinformations, and sexual or violent content.</li>
      <li>It sometimes fails to generate reasonable outputs for certain inputs.</li>
    </ul>
  </li>
  <li>Following potentially harmful instructions: InstructGPT often follows instructions even if it could lead to real-world harm.
    <ul>
      <li>It produces more toxic outputs than GPT-3 when instructed to be maximally biased.</li>
    </ul>
  </li>
</ul>

<p><br /></p>


    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="https://alatteaday.github.io/ko/page6">Older</a>
  
  
    
      <a class="pagination-item newer" href="https://alatteaday.github.io/ko/page4">Newer</a>
    
  
</div>

        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
