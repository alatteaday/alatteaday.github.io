<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      [Paper] Deep learning for image super-resolution: A survey (2020) &middot; Coffee Chat
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/papers/2020/12/22/srsurvey/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/papers/2020/12/22/srsurvey/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">[Paper] Deep learning for image super-resolution: A survey (2020)</h1>
  <p class="post-date">22 Dec 2020&nbsp;&nbsp;&nbsp;&nbsp;
    <!--<span class="post-date">22 Dec 2020</span>-->
    
      
        
          <span class="tag" data-tag="cv">
            <a href="https://alatteaday.github.io/tags/?tag=cv">
              #cv
            </a>
          </span>
        
      
    
  </p>
  <p>Wang, Zhihao, Jian Chen, and Steven CH Hoi. “Deep learning for image super-resolution: A survey.” <em>IEEE transactions on pattern analysis and machine intelligence</em> 43.10 (2020): 3365-3387.</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/9044873?casa_token=ZvibT-s3inQAAAAA:7z3uDjyf2cDsJhnY-NLadsaG1exlVS3qQAPck6JXaj6awV7I5Gcc8XXbjjw5uugCWXfE6tXJNB4">Paper Link</a></p>

<h1 id="introduction">Introduction</h1>

<ul>
  <li>Super-resolution (SR) is the process of enhancing the resolution of images, transforming low-resolution (LR) images to high-resolution (HR) images.</li>
  <li>SR is an ill-posed problem due to the existence of multiple HR images for a single LR image.</li>
  <li>Deep learning has significantly advanced SR, with approaches like CNNs (SRCNN) and GANs (SRGAN).</li>
</ul>

<p><br /></p>

<h1 id="problem-setting-and-terminology">Problem Setting and Terminology</h1>

<ul>
  <li>Problem Definition: Developing a super-resolution model to approximate HR images from LR inputs.</li>
  <li>
    <p>Image Quality Assessment (IQA): Methods include subjective human perception and objective computational techniques, classified into full-reference, reduced-reference, and no-reference methods.</p>

    <p><br /></p>
  </li>
</ul>

<h1 id="supervised-super-resolution">Supervised Super-Resolution</h1>

<h2 id="sr-framework">SR Framework</h2>

<ul>
  <li>Pre-Upsampling Framework: Uses traditional upsampling followed by deep neural networks (e.g., SRCNN).</li>
  <li>Post-Upsampling Framework: Employs end-to-end deep learning models for upsampling.</li>
  <li>Progressive Upsampling Framework: Utilizes cascades of CNNs for step-by-step refinement of images.</li>
  <li>Iterative Up-and-Down Sampling: Incorporates methods like DBPN and SRFBN for capturing LR-HR dependencies.</li>
</ul>

<h2 id="upsampling-methods">Upsampling Methods</h2>

<h3 id="interpolation-based">Interpolation-Based</h3>

<p>Includes nearest-neighbor, bilinear, and bicubic interpolation. These are traditional techniques used to resize images before the advent of deep learning-based methods.</p>

<h3 id="learning-based">Learning-Based</h3>

<p>Utilizes transposed convolution layers and sub-pixel layers for end-to-end learning.</p>

<ol>
  <li><strong>Transposed Convolution Layer (Deconvolution Layer)</strong>: Predicts the possible input based on feature maps sized like the convolution output for resolution, expanding the image by inserting zeros and performing convolution.
    <p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/deconv.jpg?raw=true" alt="deconv" style="zoom: 70%;" />
</p>

    <ul>
      <li>This method enlarges the image size while maintaining a connectivity pattern, but it can cause uneven overlapping on each axis, leading to checkerboard-like artifacts that can affect SR performance.</li>
    </ul>
  </li>
  <li><strong>Sub-Pixel Layer (Pixelshuffle)</strong>: Generates plurality of channels by convolution and the reshape them.
    <p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/subpixel.png?raw=true" alt="subpixel" style="zoom: 70%;" />
</p>

    <ul>
      <li>Given an input size $(h \times w \times c)$, it generates $s^2$ times channels, where $s$ is a scaling factor. The output size becomes $(h \times w \times s^2c)$, which is then reshaped(shuffled) to $(sh \times sw \times c)$.</li>
      <li>This method maintains a larger receptive field than the transposed convolution layer, providing more contextual and realistic details. However, the distribution of the receptive field can be uneven, leading to artifacts near the boundaries of different blocks.</li>
    </ul>
  </li>
</ol>

<h2 id="network-design">Network Design</h2>

<p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/networks.png?raw=true" alt="networks" style="zoom: 100%;" />
</p>

<h3 id="residual-learning">Residual Learning</h3>

<ul>
  <li>Simplifies learning by focusing on the residuals between LR and HR images instead of learning a direct mapping. This approach reduces the complexity of the transformation task.</li>
  <li>By learning only the difference (residuals) between the input and the target image, the model can focus on fine details, resulting in better performance and faster convergence.</li>
  <li>Example: The ResNet architecture uses residual blocks to enhance the ability of very deep networks to learn effectively without vanishing gradients.</li>
</ul>

<h3 id="recursive-learning">Recursive Learning</h3>

<ul>
  <li>Repeatedly applies the same modules to capture higher-level features while maintaining a manageable number of parameters.</li>
  <li>Allows the network to refine features iteratively, leading to more detailed and accurate image reconstructions.</li>
  <li>Example: Deep Recursive Convolutional Network (DRCN) utilizes a single convolutional layer applied multiple times to expand the receptive field without increasing the number of parameters significantly.</li>
</ul>

<h3 id="multi-path-learning">Multi-Path Learning</h3>

<ol>
  <li><strong>Local Multi-Path Learning</strong>
    <ul>
      <li>Extracts features through multiple parallel paths which then get fused to provide better modeling capabilities. This approach helps in capturing different aspects of the image simultaneously.</li>
      <li>Different paths can focus on various scales or types of features, which are then combined to improve the overall representation.</li>
      <li>Example: Multi-scale Residual Network (MSRN) uses multiple convolutional layers with different kernel sizes to capture multi-scale features.</li>
    </ul>
  </li>
  <li><strong>Scale-Specific Multi-Path Learning</strong>
    <ul>
      <li>Involves having separate paths for different scaling factors within a single network, allowing the network to handle multiple scales more effectively.</li>
      <li>Example: MDSR (Multi-Scale Deep Super-Resolution) shares most network parameters but has scale-specific layers to handle different upscaling factors.</li>
    </ul>
  </li>
</ol>

<h3 id="dense-connections">Dense Connections</h3>

<ul>
  <li>Enhances gradient flow and feature reuse by connecting each layer to every other layer in a feed-forward fashion. This ensures that gradients can flow directly to earlier layers, improving learning efficiency.</li>
  <li>Promotes feature reuse, leading to more efficient and compact networks.</li>
  <li>Example: DenseNet connects each layer to every other layer, facilitating better feature propagation and reducing the risk of gradient vanishing.</li>
</ul>

<h3 id="group-convolution">Group Convolution</h3>

<ul>
  <li>Splits the input channels into groups and performs convolutions within each group. This reduces the computational complexity and number of parameters.</li>
  <li>Often used in lightweight models to balance performance and efficiency.</li>
  <li>Example: Xception and MobileNet architectures use depthwise separable convolutions, a type of group convolution, to reduce the number of parameters and computation.</li>
</ul>

<h3 id="pyramid-pooling">Pyramid Pooling</h3>

<ul>
  <li>Uses pooling operations at multiple scales to capture both global and local context information. This helps in understanding the image at different resolutions.</li>
  <li>Example: PSPNet (Pyramid Scene Parsing Network) uses pyramid pooling to aggregate contextual information from different scales, which is then combined to enhance the feature representation.</li>
</ul>

<h3 id="attention-mechanisms">Attention Mechanisms</h3>

<ol>
  <li><strong>Channel Attention</strong>
    <ul>
      <li>Focuses on the interdependencies between feature channels. It assigns different weights to different channels, enhancing important features and suppressing less useful ones.</li>
      <li>Example: Squeeze-and-Excitation Networks (SENet) uses a squeeze operation to aggregate feature maps across spatial dimensions, followed by an excitation operation that recalibrates channel-wise feature responses.</li>
    </ul>
  </li>
  <li><strong>Spatial Attention</strong>
    <ul>
      <li>Focuses on the spatial location of important features. It assigns weights to different spatial regions, allowing the model to focus on relevant areas of the image.</li>
      <li>Example: Convolutional Block Attention Module (CBAM) combines channel and spatial attention to improve representation by focusing on meaningful parts of the image.</li>
    </ul>
  </li>
  <li><strong>Non-Local Attention</strong>
    <ul>
      <li>Captures long-range dependencies between distant pixels. This is particularly useful for super-resolution tasks where global context is important.</li>
      <li>Example: Non-local Neural Networks use a self-attention mechanism to compute relationships between all pairs of positions in the feature map, allowing the model to capture global context and dependencies.</li>
    </ul>
  </li>
  <li><strong>Combined Attention</strong>
    <ul>
      <li>Some networks combine multiple types of attention mechanisms to leverage the strengths of each. For instance, combining channel and spatial attention can provide a more comprehensive attention mechanism.</li>
      <li>Example: The Residual Channel Attention Network (RCAN) uses channel attention modules within a residual network structure to enhance the network’s ability to capture important features for image super-resolution</li>
    </ul>
  </li>
</ol>

<h2 id="learning-strategies">Learning Strategies</h2>

<ul>
  <li>Loss Functions: Early methods used pixel-wise L2 loss, while newer approaches incorporate more complex losses like content loss, adversarial loss, and perceptual loss to improve the quality of the reconstructed images.</li>
  <li>Training Techniques: Techniques such as curriculum learning, multi-supervision, and progressive learning are used to enhance the training process and improve model performance.</li>
</ul>

<p><br /></p>

<h1 id="unsupervised-super-resolution">Unsupervised Super-Resolution</h1>

<p>Unsupervised methods do not rely on paired LR-HR datasets. Instead, they use generative models and adversarial training to learn the mapping from LR to HR images. Techniques include CycleGAN, which learns the transformation by mapping LR images to HR images and vice versa.</p>

<h1 id="domain-specific-super-resolution">Domain-Specific Super-Resolution</h1>

<p>Domain-specific methods focus on specific applications such as face SR, text SR, and medical image SR. These methods leverage domain knowledge to improve the quality of SR in specific contexts.</p>

<h1 id="benchmark-datasets-and-performance-evaluation">Benchmark Datasets and Performance Evaluation</h1>

<p>Several benchmark datasets are used for evaluating SR models, including Set5, Set14, BSD100, and Urban100. Common evaluation metrics include Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM).</p>

<h3 id="metrics">Metrics</h3>

<p>While PSNR is widely used, it does not always correlate well with human perception of image quality. SSIM addresses this by considering luminance, contrast, and structure.</p>
<ul>
  <li>
    <p>PSNR is one of the most popular reconstruction quality measurements of lossy transformation. In the context of SR, it’s defined via the maximum pixel value ($L$) and the mean squared error (MSE) between the images.</p>

\[PSNR=10\cdot\log_{10}\big({L^2\over{1\over N}\sum_{i=1}^N(I(i)-\hat{I}(i))^2}\big)\]

    <ul>
      <li>$I(i)$ and $\hat{I}(i)$ represent the pixel values of the original and reconstructed images, respectively. and $N$ is the total number of pixels.</li>
    </ul>
  </li>
  <li>
    <p>SSIM measures the structural similarity between images based on independent comparisons in terms of luminance, contrast, and structures. It considers the human visual system (HVS) is highly adapted to extract image structures.</p>

\[SSIM(I,\hat{I})={(2\mu_I\mu_{\hat{I}}+C_1)(2\sigma_{I\hat{I}}+C_2)\over(\mu_I^2+\mu_{\hat{I}}^2+C_1)(\sigma_I^2+\sigma_\hat{I}^2+C_2)}\]

    <ul>
      <li>$\mu_I$ and $\mu_\hat{I}$ are the mean pixel values of the original and reconstructed images, respectively. $\sigma_I^2$ and $\sigma_\hat{I}^2$ are the variances, and $\sigma_{I\hat{I}}$ is the covariance of $I$ and $\hat{I}$. $C_1$ and $C_2$ are constants to stabilize the division when the denominators are small.</li>
    </ul>
  </li>
</ul>

<h1 id="challenges-and-future-directions">Challenges and Future Directions</h1>

<ul>
  <li>Scalability: Developing SR models that can handle varying scales and resolutions efficiently.</li>
  <li>Real-World Applications: Enhancing SR models to perform well on real-world images with diverse degradation.</li>
  <li>Efficiency: Reducing computational complexity and memory usage while maintaining high performance.</li>
  <li>Generality: Creating SR models that generalize well across different types of images and domains.</li>
  <li>Perceptual Quality: Improving the perceptual quality of SR images, ensuring that they are visually appealing and free from artifacts.</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>The survey paper provides an in-depth review of deep learning-based super-resolution techniques, categorizing them into supervised, unsupervised, and domain-specific methods. It discusses various network architectures, upsampling techniques, and learning strategies, highlighting the advancements and challenges in the field. The paper also covers benchmark datasets and performance evaluation metrics, providing a comprehensive overview of the current state of image super-resolution research.</p>

</div>


<div class="related">
  <h2 class="related-title">Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/paper/2024/06/14/mriqcsurvey/">
            Summaries of papers on MRI Quality Assessment and Control&nbsp;
            <small>14 Jun 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/2024/05/28/mriqc_report/">
            [MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)&nbsp;
            <small>28 May 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/">
            [MRIQC 3-1] Opening an HTML file using Flask&nbsp;
            <small>21 May 2024</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


        
          <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
