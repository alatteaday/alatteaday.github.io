<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      [Paper] Tabtransformer: Tabular data modeling using contextual embeddings (2020) &middot; Coffee Chat
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/papers/2024/04/11/tabtf/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/papers/2024/04/11/tabtf/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">[Paper] Tabtransformer: Tabular data modeling using contextual embeddings (2020)</h1>
  <p class="post-date">11 Apr 2024&nbsp;&nbsp;&nbsp;&nbsp;
    <!--<span class="post-date">11 Apr 2024</span>-->
    
      
        
          <span class="tag" data-tag="tabular">
            <a href="https://alatteaday.github.io/tags/?tag=tabular">
              #tabular
            </a>
          </span>
        
      
        
          <span class="tag" data-tag="transformer">
            <a href="https://alatteaday.github.io/tags/?tag=transformer">
              #transformer
            </a>
          </span>
        
      
    
  </p>
  <p>Huang, Xin, et al. “Tabtransformer: Tabular data modeling using contextual embeddings.” <em>arXiv preprint arXiv:2012.06678</em> (2020).</p>

<p><a href="https://arxiv.org/abs/2012.06678">Paper Link</a></p>

<p><br /></p>

<h1 id="points">Points</h1>

<ul>
  <li><strong>TabTransformer</strong>: A cutting-edge tabular data model leveraging contextual embeddings.</li>
  <li>Pre-trained by innovative two-phase approach for robust feature representation.</li>
  <li>Showed SOTA performance in both supervised and semi-supervised learning.</li>
  <li>Handles missing and noisy data robustly, ensuring reliable performance.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>The current state-of-the-art (SOTA) mdoels for tabular data primarily consist of tree-based ensemble methods, notably gradient boosted decision trees (GBDT). However, these models exhibit several limitations in comparison to deep learning models:</p>

<ul>
  <li>Not suitable for continual learning from streaming data.</li>
  <li>Ineffective for end-to-end learning of multi-modality of tabular data, such as incorporating image or text features.</li>
  <li>Not suitable for semi-supervised learning.</li>
</ul>

<p>On the other had, while multi-layer perceptrons (MLPs) offer the potential for end-to-end learning of image or text encoders, they are constrained by several drawbacks:</p>

<ul>
  <li>Lack of interpretability.</li>
  <li>Vulnerability to missing and noisy data.</li>
  <li>Limited performance in semi-supervised learning scenarios.</li>
  <li>Inability to match the performance of tree-based models.</li>
</ul>

<p><br /></p>

<h1 id="method">Method</h1>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/archi.png?raw=true" alt="archi" style="zoom: 70%;" />
</p>

<ul>
  <li>The Transformer layers receive only categorical inputs $x_{cat}$.</li>
  <li>Continuous inputs $x_{cont}$ are concatenated with the outputs of the Transformer modules of the categorical inputs.</li>
  <li>During the pre-training phase, the Transformer layers undergo training on two different tasks using unlabeled data
    <ul>
      <li>Only the categorical inputs are utilized for pre-training, with the exclusion of the continuous inputs.</li>
    </ul>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code1.png?raw=true" alt="code1" style="zoom: 100%;" />
</p>
  </li>
  <li>The pre-trained model is fine-tuned alongsidethe MLP head, utilizing labeled data to predict a target $y$.</li>
  <li>
    <p>Continuous values are incorporated during the fine-tuning phase by concatenating them with the categorical values.</p>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code2.png?raw=true" alt="code2" style="zoom: 100%;" />
</p>
  </li>
</ul>

<p><br /></p>

<h2 id="model-architecture">Model Architecture</h2>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig1.png?raw=true" alt="fig1" style="zoom: 60%;" />
</p>

<ul>
  <li>Each instance $x\equiv \lbrace x_{cat}, x_{cont}\rbrace$ is paired with its corresponding label $y$: $(x, y)$.</li>
  <li>$x_{cat} \equiv \lbrace x_1, x_2, …, x_m\rbrace$ represents categorical features, with each $x_i$ being a categorical feature $i \in {1, …, m}$.</li>
  <li>
    <p>$x_{cat}$ undergoes transformation into column embedding $E_\phi$:</p>

\[E_\phi(x_{cat}) \equiv \lbrace e_{\phi_1}(x_1), ..., e_{\phi_m}(x_m) \rbrace, \ e_{\phi_i}(x_i) \in \mathbb{R}^d\]
  </li>
  <li>
    <p>The embeddings are fed into the multiple Transformer layers $f_\theta$, producing contextual embeddings:</p>

\[\{h_1, ..., h_m\}=f_\theta(E_\phi(x_{cat})), \ h\in \mathbb{R}^d\]
  </li>
  <li>Contextual embeddings of $x_{cat}$ are concatenated with the $x_{cont} \in \mathbb{R}^c $ to form a vector of dimension $(d\times m+c)$.</li>
  <li>
    <p>The vector is passed through an MLP layer $g_\psi$ and a cross-entropy loss $H$ is computed between the predicted output and the target $y$:</p>

\[L(x, y) \equiv H(g_\psi(f_\theta(E_\phi(x_{cat})), x_{cont}), y)\]
  </li>
</ul>

<p><br /></p>

<p><strong>Column Embedding</strong></p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/colemb.png?raw=true" alt="colemb" style="zoom: 60%;" />
</p>

<ul>
  <li>Each categorical feature $x_i$ has its own embedding lookup table $e_{\phi_i}(.)$.</li>
  <li>For the $i$th feature with $d_i$ classes, the embedding table $e_{\phi_i}(.)$ contains $(d_1+1)$ embeddings. The additional $d_1+1$th embedding is reserved for representing the missing(masked) values.</li>
  <li>Each embedding $e_{\phi_i}(j)$ is represented as $[c_{\phi_i}, w_{\phi_{ij}}]$, where:
    <ul>
      <li>$c_{\phi_i}$ helps distinguish the classes in column $i$ from those in the other columns.</li>
      <li>$w_{\phi_{ij}}$ distinguishes the class of the feature $j$ within the $i$th column from the other classes within the same column.</li>
    </ul>
  </li>
  <li>
    <p>*The dimension $d$ likely is set to be the same as the hidden dimension $h$ according to the codes.</p>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code3.png?raw=true" alt="code3" style="zoom: 100%;" />
</p>
  </li>
</ul>

<p><br /></p>

<h2 id="pre-training">Pre-training</h2>

<p>The Transformer layers are trained using inputs consisting of categorical values $x_{cat}=\lbrace x_1, x_2, …, x_m\rbrace$ on two pre-training tasks:</p>

<ol>
  <li><strong>Masked language modeling (MLM)</strong>
    <ul>
      <li>Randomly masks $k\%$ features of the input, where $k$ is set to 30 in experiments.</li>
      <li>Minimizes the cross-entropy loss of a multi-class classifier $g_\psi$, which predicts the original features of the masked features.</li>
    </ul>
  </li>
  <li><strong>Replaced token detection (RTD)</strong>
    <ul>
      <li>Replaces the original feature by a random value of that feature.</li>
      <li>Minimizes the loss of a binary classifier predicting whether the feature has been replaced.</li>
      <li>Each column has its own embedding lookup table, necessitating the definition of a separate binary classifier for each column.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h1 id="experiments">Experiments</h1>

<h2 id="settings">Settings</h2>

<p><strong>Data</strong></p>

<ul>
  <li>Models were evaluated on 15 publicly available binary classification datasets sourced from UCI repository, AutoML Challenge, and Kaggle.</li>
  <li>Each dataset was divided into 5 cross-validation splits.</li>
  <li>Training:Validation:Testing proportion was set to 65:15:20 (%).</li>
  <li>The number of categorical features ranged from 2 to136.</li>
  <li>Semi-supervised and supervised experiments
    <ul>
      <li>Semi-supervised: Training data consisted of $p$ labeled data points + the remaining unlabeled data, with $p\in (50, 200, 500)$ for 3 different scenarios.</li>
      <li>Supervised: Fully labeled training data was used.</li>
    </ul>
  </li>
</ul>

<p><strong>Setup</strong></p>
<ul>
  <li>Hidden dimension: 32</li>
  <li>The num of layers: 6</li>
  <li>The num of attention heads: 8</li>
  <li>MLP layer architecture: $\lbrace 4\times l, \ 2\times l \rbrace$ (where $l$ represents the size of its input).</li>
  <li>Hyperparamter optimization (HPO) conducted with 20 rounds for each cross-validation split.</li>
  <li>Metrics: Area under the curve (AUC).</li>
  <li>Pre-training was exclusively applied in the semi-supervised scenario.
    <ul>
      <li>It was not found to be significantly beneficial when the entire dataset was labeled.</li>
      <li>Its benefits were more apparent when there is a large number of unlabeled examples and a few labeled examples, as pre-training provided representations of the data that could not be learned solely from the labeled examples.</li>
    </ul>
  </li>
</ul>

<p><strong>Baseline model</strong>: An MLP model without Transformers was employed to evaluate the effectiveness of Transformers in comparison.</p>

<p><br /></p>

<h2 id="the-effectiveness-of-the-transformer-layers">The effectiveness of the Transformer Layers</h2>

<ol>
  <li>
    <p><strong>Performance comparison</strong></p>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table1.png?raw=true" alt="table1" style="zoom: 60%;" />
</p>

    <ul>
      <li>Conducted in a supervised learning scenario, comparing TabTransformer to MLP.</li>
      <li>TabTransformer outperforms the baseline MLP on 14 datasets, achieving an average 1.0% gain in AUC.</li>
    </ul>
  </li>
  <li>
    <p><strong>t-SNE visualization of contextual embeddings</strong></p>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig2.png?raw=true" alt="fig2" style="zoom: 100%;" />
</p>

    <ul>
      <li>Each marker in the plot represents an average of 2D points over the test data points for a certain class.</li>
      <li>In the t-SNE plot of the last layer of TabTransformer (Left), semantically similar classes are closely grouped, forming clusters in the embedding space.</li>
      <li>Before passing into the Transformer (Center), the embeddings start to distinguish features with different characteristics.</li>
      <li>The embeddings of MLP (Right) do not reveal any discernible pattern.</li>
    </ul>
  </li>
  <li>
    <p><strong>Prediction performance of linear models using the embeddings from different Transformer layers</strong></p>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig3.png?raw=true" alt="fig2" style="zoom: 60%;" />
</p>

    <ul>
      <li>Logistic regression models are employed to evaluate the quality of learned embeddings.</li>
      <li>Each model predicts $y$ using embedding features along with continuous values.</li>
      <li>Metrics: Cross-validation score in AUC on the test data.</li>
      <li>Normalization: Each prediction score is normalized by the best score from an end-to-end trained TabTransformer for the corresponding dataset.</li>
      <li>Features: Embeddings are averaged and processed using maximum pooling instead of concatenation.</li>
      <li>The effectiveness of the embeddings improves as the Transformer layers progress.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h2 id="the-robustness-of-tabtransformer">The robustness of TabTransformer</h2>

<p>The robustness of TabTransformer was evaluated by assessing its performance on datasets containing noisy data and data with missing values.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig4_5.png?raw=true" alt="fig4_5" style="zoom: 100%;" />
</p>

<ol>
  <li><strong>Noisy data</strong>
    <ul>
      <li>Method: Values were replaced with randomly generated ones from corresponding columns, introducing noise into datasets.</li>
      <li>Findings: As the noise increases, TabTransformer demonstrated significantly significantly superior compared to the MLP (see fig. 4).</li>
      <li>The contextual property of embeddings likely contributes to TabTransformer’s robustness in noisy environments.</li>
    </ul>
  </li>
  <li><strong>Data with missing values</strong>
    <ul>
      <li>Method: Some values artificially made missing, and models were evaluated on these modified datasets.
        <ul>
          <li>The average learned embeddings over all classes in the corresponding columns were used to handle the embeddings of missing values.</li>
        </ul>
      </li>
      <li>Findings: TabTransformer exhibited better stability than MLP in handling missing values (see fig. 5).</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h2 id="supervised-learning">Supervised learning</h2>

<p>TabTransformer’s performance was compared against four categories of methods:</p>
<ul>
  <li>Logistic Regression and GBDT</li>
  <li>MLP and sparse MLP</li>
  <li>TabNet model</li>
  <li>Variational Information Bottleneck (VIB) model</li>
</ul>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table2.png?raw=true" alt="table2" style="zoom: 70%;" />
</p>

<p>Findings:</p>
<ul>
  <li>TabTransformer demonstrated comparable performance with GBDT.</li>
  <li>It significantly outperformed than recent deep learning models designed for tabular data, including TabNet and VIB.</li>
</ul>

<p><br /></p>

<h2 id="semi-supervised-learning">Semi-supervised learning</h2>

<p>TabTransformer was evaluated under the semi-supervised learning scenario and compared against other semi-supervised models, including baseline models:</p>
<ul>
  <li>Entropy Regularization (ER)</li>
  <li>Pseudo Labeling (PL) combined with MLP, TabTransformer, and GBDT</li>
  <li>MLP (DAE): An unsupervised pre-training method designed for deep models on tabular data, specifically the swap noise Denoising AutoEncoder</li>
</ul>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table3_4.png?raw=true" alt="table3_4" style="zoom: 70%;" />
</p>

<p>Method:</p>
<ul>
  <li>Pre-trained models (TabTransformer-RTD/MLM and MLP): pre-trained on the unlabeled data and then fine-tuned on labeled data.</li>
  <li>Semi-supervised learning methods (ER and PL): trained on the mix of labeled and unlabeled training data.</li>
</ul>

<p>Findings:</p>
<ul>
  <li>TabTransformer-RTD/MLM are outperformed all the other models.</li>
  <li>TabTransformer (ER), TabTransformer (PL) and GBDT (PL) performed worse than the average of all the models.</li>
  <li>TabTransformer-RTD consistently showed better results when the number of unlabeled data decreased, surpassing TabTransformer-MLM.
    <ul>
      <li>This could be attributed to the easier pre-training task of a binary classification compared to the multi-class classification of MLM.</li>
    </ul>
  </li>
  <li>With only 50 data points, MLM (ER) and MLM (PL) outperformed TabTransformer models.
    <ul>
      <li>The suggests that the proposed approach allows for informative embeddings but does not enable the weights of the classifier itself to be trained with unlabeled data.</li>
    </ul>
  </li>
  <li>Overall, TabTransformer models are promise in extracting useful information from unlabeled data to aid supervised training, and are particularly useful when the size of unlabeled data is large.</li>
</ul>


</div>


<div class="related">
  <h2 class="related-title">Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/paper/2024/06/14/mriqcsurvey/">
            Summaries of papers on MRI Quality Assessment and Control&nbsp;
            <small>14 Jun 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/2024/05/28/mriqc_report/">
            [MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)&nbsp;
            <small>28 May 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/">
            [MRIQC 3-1] Opening an HTML file using Flask&nbsp;
            <small>21 May 2024</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


        
          <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
