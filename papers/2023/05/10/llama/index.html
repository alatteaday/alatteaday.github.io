<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      [Paper] Llama: Open and efficient foundation language models (2023) &middot; Coffee Chat
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/papers/2023/05/10/llama/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/papers/2023/05/10/llama/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">[Paper] Llama: Open and efficient foundation language models (2023)</h1>
  <p class="post-date">10 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
    <!--<span class="post-date">10 May 2023</span>-->
    
      
        
          <span class="tag" data-tag="nlp">
            <a href="https://alatteaday.github.io/tags/?tag=nlp">
              #nlp
            </a>
          </span>
        
      
        
          <span class="tag" data-tag="llm">
            <a href="https://alatteaday.github.io/tags/?tag=llm">
              #llm
            </a>
          </span>
        
      
    
  </p>
  <p>Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.” <em>arXiv preprint arXiv:2302.13971</em> (2023).</p>

<p><a href="https://arxiv.org/abs/2302.13971">Paper Link</a></p>

<h1 id="points">Points</h1>

<ul>
  <li>Efficient inference with smaller models: LLaMA models prioritize inference efficiency by using smaller models trained on large datasets, achieving state-of-the-art (SOTA) performance across benchmarks while being cost-effective during inference.</li>
  <li>Publicly available data: Unlikely many existing models that rely on proprietary data, LLaMA models are exclusively trained on publicly available datasets, ensuring transparency and compatibility with open-source principles.</li>
  <li>Broad Benchmark Performance: LLaMA models demonstrate competitive performance on a wide range of tasks, including common sense reasoning, question answering, reading comprehension, etc,.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>Large language models (LLMs) have demonstrated remarkable capabilities in performing new tasks with minimal instruction or examples, thanks to their vast size. However, recent research suggests that smaller models trained on larger datasets can achieve superior performance, highlighting the importance of efficiency during inference rather than training.</p>

<p><br /></p>

<h1 id="approach">Approach</h1>

<p>LLaMA is a series of language models (LMs) designed to optimize performance across various inference budgets, ranging from 7B to 65B parameters, using only publicly available data.</p>

<h2 id="pre-training-data">Pre-training data</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table1.png?raw=true" style="zoom: 30%;" />
</p>
<p>The dataset mixture used cover diverse domains and is entirely publicly avaliable, ensuring compatibility with open-source principles:</p>
<ol>
  <li>English CommonCrawl [67%]: Preprocessed from five CommonCrawl dumps (2017-2020)., filtered for non-English and low-quality content.</li>
  <li>C4 [15%]: Similarly preprocessed to CommonCrawl, to enhance performance.</li>
  <li>Github [4.5%]: Filtered for line length and alphanumeric content from Google BigQuery.</li>
  <li>Wikipedia [4.5%]: Dumps from mid-2022, covering multiple languages.</li>
  <li>Gutenberg and Books3 [4.5%]: Publicly available books with redundant content removed.</li>
  <li>ArXiv [2.5%]: Includes scientific data, with non-essential content removed.</li>
  <li>Stack Exchange [2%]: High-quality Q&amp;A content sorted by score.</li>
</ol>

<h3 id="tokenization">Tokenization</h3>

<ul>
  <li>Byte Pair Encoding (BPE) tokenizer used.</li>
  <li>Splits numbers into digits and decomposes unknown UTF-8 characters.</li>
  <li>The training dataset contains approximately 1.4T tokens, with minimal repetition (fig 1).
    <p align="center">
 <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig1.png?raw=true" alt="fig1" style="zoom: 30%;" />
 </p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<p>LLaMA models are based on transformer architecture with key modifications:</p>
<ol>
  <li>Pre-normalization [GPT3]: Normalizes the input of each transformer sub-layer, enhancing training stability using RMSNorm.</li>
  <li>SwiGLU activation function [PaLM]: Uses SwiGLU instead of ReLU, improving performance with a dimension of $2\over3 4d$ instead of $4d$ as in PaLM.</li>
  <li>Rotary Embeddings [GPTNeo]: Employs Rotary embeddings (RoPE) instead of absolute positional embeddings at each layer of the network.</li>
</ol>

<h2 id="optimizer">Optimizer</h2>

<p>Trained using the AdamW optimizer with:</p>
<ul>
  <li>$\beta_1=0.9, \beta_2=0.95$.</li>
  <li>Cosine learning rate schedule, ending at 10% of the maximal rate.</li>
  <li>Weight decay of 0.1 and gradient clipping of 1.0.</li>
  <li>2,000 warmup-steps, with varying learning rates and batch size with the size of the model (table 2).
    <p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table2.png?raw=true" alt="table2" style="zoom: 30%;" />
</p>
  </li>
</ul>

<h2 id="efficient-implementation">Efficient implementation</h2>
<ol>
  <li>Causal multi-head attention: Efficient implementation using xformer library to reduce memory and runtime.</li>
  <li>Activation reductions: Uses checkpointing to recompute activations during the backward pass, especially for computationally expensive layers.</li>
</ol>

<p><br /></p>

<h1 id="main-results">Main Results</h1>

<p>Evaluated on 20 benchmarks for zero-shot and few-shot tasks, compared to non-public models (GPT-3, Gopher, Chinchilla, PaLM) and open-sourced models (OPT, GPT-J, GPT-Neo).</p>

<h2 id="common-sense-reasonging">Common sense reasonging</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table3.png?raw=true" alt="table3" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: Eight standard benchmarks such as BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA. Theses datasets include Cloze and Winograd style tasks and multiple choice question answering (QA).</li>
  <li>Results
    <ul>
      <li>LLaMA-65B outperforms Chinchilla 70B and PaLM-540B on most benchmarks except BoolQ.</li>
      <li>LLaMA-13B outperforms GPT-3 on most benchmarks despite being significantly smaller.</li>
    </ul>
  </li>
</ul>

<h2 id="close-book-question-answering">Close-book question answering</h2>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table4.png?raw=true" alt="table4" style="zoom: 30%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table5.png?raw=true" alt="table5" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: Natural Questions and TriviaQA. The models report exact match performance where the models do not have access to documents that contain avidence to answer the question.</li>
  <li>Results:
    <ul>
      <li>LLaMA-65B achieve state-of-the-art (SOTA) performance in zero-shot and few-shot settings.</li>
      <li>LLaMA-13B is competitive with GPT-3 and Chinchilla which are larger models.</li>
    </ul>
  </li>
</ul>

<h2 id="reading-comprehension">Reading comprehension</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table6.png?raw=true" alt="table6" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmark: RACE reading comprehension, collected from English reading comprehension exams in middle and high school Chinese students.</li>
  <li>Results: LLaMA-65B is competitive with PaLM-540B, and LLaMA-13B outperforms GPT-3.</li>
</ul>

<h2 id="mathematical-reasoning">Mathematical reasoning</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table7.png?raw=true" alt="table7" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: MATH and GSM8k. MATH contains 12K math problems of middle and high school. GSM8k is a set of middle school math problems.</li>
  <li>Results: LLaMA-65B outperforms Minerva-62B on GSM8k.
    <ul>
      <li>Minerva is a series of PaLM models fine-tuned on 38.5B tokens extracted from ArXiv and Math Web Pages. Both PaLM and LLaMA, however, are not finetuned on math data.</li>
    </ul>
  </li>
</ul>

<h2 id="code-generation">Code generation</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table8.png?raw=true" alt="table8" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: HumanEval and MBPP. The models are evaluated about their ability to write code from a natural language description.</li>
  <li>Results:
    <ul>
      <li>LLaMA models outperform other models, including LaMDA and PaLM. LLaMA-13B outperforms LaMDA-137B. LLaMA 65B outperforms PaLM-62B.</li>
      <li>Fine-tuning on code-specific tokens further improves performance.</li>
    </ul>
  </li>
</ul>

<h2 id="massive-multitask-language-understanding">Massive multitask language understanding</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table9.png?raw=true" alt="table9" style="zoom: 30%;" />
</p>
<ul>
  <li>Massive multitask language understanding (MMLU) consists of multiple choice questions covering various domains of knowledge, like humanities, STEM and social sciences.</li>
  <li>Results: LLaMA-65B underperforms compared to Chinchilla-70B and PaLM-540B, possibly due to limited academic data.</li>
</ul>

<h2 id="evolution-of-performance-during-training">Evolution of performance during training</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig2.png?raw=true" alt="fig2" style="zoom: 30%;" />
</p>
<ul>
  <li>Performance improves steadily, and correlates with the training perplexity of the model.</li>
  <li>SIQA and WinoGrande are the exceptions: SIQA may not be reliable as performance is varied, and performance doesn’t correlate with training perplexity on WinoGrande.</li>
</ul>

<p><br /></p>

<h1 id="instruction-fine-tuning">Instruction Fine-tuning</h1>

<p>Fine-tuning improves performance and futher the ability to follow instructions. LLaMA-I is trained on MMLU with instructions and compared with OPT-IML and Flan-PaLM series which fine-tuned with moderate sizes.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table10.png?raw=true" alt="table10" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA-I with 65B parameter size outperforms existing instruction fine-tuned models, but remains behind GPT ‘code-davinci-002’.</li>
</ul>

<p><br /></p>

<h1 id="bias-toxicity-and-misinformation">Bias, Toxicity and Misinformation</h1>

<p>LLMs have been showed to be biased to content of training data, and to generate toxic content. Evaluated using benchmarks for toxic content generation and stereotypes detection.</p>

<h2 id="realtoxicityprompts">RealToxicityPrompts</h2>

<p>Indicates how toxic is a model. The toxicity score is automatically evaluated by making a request to PerspectiveAPI, ranging from 0 (non-toxic) to 1 (toxic).</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table11.png?raw=true" alt="table11" style="zoom: 30%;" />
</p>
<ul>
  <li>Comparable to other models, with larger models exhibiting more toxicity, especially for “Respectiful” prompts.</li>
  <li>It can be suggested that the relation between toxicity and model size may only apply within a model family.</li>
</ul>

<h2 id="crows-pairs">CrowS-Pairs</h2>

<p>Evaluates the biases in a model with 9 categories: gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socioenconomic status.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table12.png?raw=true" alt="table12" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA shows slight biases, particularly in the religion, age, and gender categories. This may be come from CommonCrawl dataset.</li>
</ul>

<h2 id="winogender">WinoGender</h2>

<p>Used to investigate the bias of a model on the gender category. It evaluates if the model’s co-reference resolution performance is impacted by the gender of the pronoun.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table13.png?raw=true" alt="table13" style="zoom: 30%;" />
</p>
<ul>
  <li>Performance varies by pronoun type: The models have better performance “their/them/someone” pronouns than for the “her/her/she” and “his/him/he” pronouns.</li>
  <li>Larger models showing more gender bias: For “gotcha” cases, LLaMA-65B makes more errors, showing that it capture biases on gender.
    <ul>
      <li>“gotcha” cases are in which the pronoun does not match the majority gender of the occupation, and the occupation is the correct answer.</li>
    </ul>
  </li>
</ul>

<h2 id="truthfulqa">TruthfulQA</h2>

<p>Evaluates the a model’s ability to identify true claims and measures the risk of generating misinformation or false claims. This assesses the truthfulness of a model’s responses.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table14.png?raw=true" alt="table14" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA models show better truthfulness compared to GPT-3. However the correct answer rate remains low, indicating a potential for misinformation.</li>
</ul>

<p><br /></p>

<h1 id="carbon-footprint">Carbon footprint</h1>

<p>Details the environmental impact of training and deploying these models.</p>
<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table15.png?raw=true" alt="table15" style="zoom: 30%;" />
</p>

<p><br /></p>


</div>


<div class="related">
  <h2 class="related-title">Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/paper/2024/06/14/mriqcsurvey/">
            Summaries of papers on MRI Quality Assessment and Control&nbsp;
            <small>14 Jun 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/2024/05/28/mriqc_report/">
            [MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)&nbsp;
            <small>28 May 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/">
            [MRIQC 3-1] Opening an HTML file using Flask&nbsp;
            <small>21 May 2024</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


        
          <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
