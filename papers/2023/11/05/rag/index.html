<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      [Paper] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (NIPS 2020) &middot; Coffee Chat
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/papers/2023/11/05/rag/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/papers/2023/11/05/rag/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">[Paper] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (NIPS 2020)</h1>
  <p class="post-date">05 Nov 2023&nbsp;&nbsp;&nbsp;&nbsp;
    <!--<span class="post-date">05 Nov 2023</span>-->
    
      
        
          <span class="tag" data-tag="llm">
            <a href="https://alatteaday.github.io/tags/?tag=llm">
              #llm
            </a>
          </span>
        
      
        
          <span class="tag" data-tag="transformer">
            <a href="https://alatteaday.github.io/tags/?tag=transformer">
              #transformer
            </a>
          </span>
        
      
        
          <span class="tag" data-tag="nlp">
            <a href="https://alatteaday.github.io/tags/?tag=nlp">
              #nlp
            </a>
          </span>
        
      
    
  </p>
  <p>Lewis, Patrick, et al. “Retrieval-augmented generation for knowledge-intensive nlp tasks.” <em>Advances in Neural Information Processing Systems</em> 33 (2020): 9459-9474.</p>

<p><a href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html">Paper Link</a></p>

<h1 id="points">Points</h1>

<ul>
  <li><strong>Retrieval Augmented Generation (RAG)</strong> model combines a retriever and a generator for enhanced knowledge-intense tasks.</li>
  <li>RAG Variants: RAG-Sequence uses a single document for output; RAG-Token integrates multiple documents per token.</li>
  <li>RAG models outperform baselines in open-domain QA, abstractive QA, Jeopardy question generation, and fact verification.</li>
  <li>RAG models demonstrate practical benefits with easy updates to the non-parametric memory.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<ul>
  <li>Large pre-trained Language models (LLMs) store factual knowledge in their parameters, functioning as implicit knowledge base.</li>
  <li>LLMs, however, have limitations: they cannot expand their memory, provide insight into their predictions, and may produce ‘hallucinations’.</li>
  <li>Recently, hybrid models, such as REALM and ORQA, address these issues by using a differentiable retriever to revised and expanded knowledge, showing promising results, primarily in open-domain question answering (QA).</li>
</ul>

<p><br /></p>

<h1 id="method">Method</h1>

<p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig1.png?raw=true" style="zoom: 100%;" />
</p>

<p>Retrieval-augmented generation (RAG) fine-tunes pre-trained generation models with a non-parametric memory for a general-purpose tasks.</p>
<ul>
  <li>Parametric memory: a pre-trained seq2seq transformer</li>
  <li>Non-parametric memory: a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.</li>
  <li>Dense passage retriever (DPR): retrieves latent documents conditioned on the input.</li>
  <li>BART: the generator conditions on the latent documents together with the input to generate the output. Other seq2seq models like T5 can also be used and fine-tuned with the retriever.</li>
  <li>Latent documents: marginalized using a top-K approximation, either on a per-output basis or a per-token basis.
    <ul>
      <li>RAG-Sequence Model: assumes the same document is responsible for all tokens.</li>
      <li>RAG-Token Model: considers different documents for different tokens.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="models">Models</h2>

<p>RAG models use the input sequence $x$ to retrieve text documents $z$ and use them as additional context when generating the target sequence $y$. RAG has two components:</p>
<ul>
  <li>Retriever $p_\eta(z\mid x)$: returns distributions over text passages given a query $x$ with parameters $\eta$.
    <ul>
      <li>Truncated as top-K assumtion.</li>
    </ul>
  </li>
  <li>Generator $p_\theta(y_i\mid x,z,y_{1:i-1})$: generates a current token based on the previous $i-1$ tokens $y_{1:i-1}$, the input $x$, and a retrieved passage $z$ with parameters $\theta$.</li>
</ul>

<p>The retriever and the generator are trained end-to-end, treating the retrieved document as a latent variable. To marginalize over the latent documents, two methods are proposed, RAG-Sequence and RAG-Token.</p>

<p><br /></p>

<h2 id="rag-sequence-and-rag-token">RAG-Sequence and RAG-Token</h2>

<p><strong>RAG-Sequence Model</strong> uses the same retrieved document to generate the complete sequence.</p>
<ul>
  <li>The retrieved document is a single latent variable to get the seq2seq probability $p(y\mid x)$ via a top-K approximation.</li>
  <li>The top-K documents are retrieved using the retriever, and generator produces the output sequence probability for each document.</li>
</ul>

\[p_{RAG-Sequence}(y\mid x) \approx \sum_{z\in top-k(p(\cdot|x))}{p_\eta(z|x)p_\theta(y_i|x,z)} \\ = \sum_{z\in top-k(p(\cdot|x))}{p_\eta(z|x)}\prod_i^N p_\theta(y_i|x,z,y_{1:i-1})\]

<ul>
  <li>Use cases: Better suited for tasks where the context of entire documents is crucial, like summarization tasks.</li>
</ul>

<p><strong>RAG-Token Model</strong> uses different latent documents for each target token.</p>
<ul>
  <li>The generator chooses content from several documents for the answer.</li>
  <li>The top-K documents are retrieved using the retriever, and the generator produces a distribution for the next output token for each document before marginalizing.</li>
</ul>

\[p_{RAG-Token}(y|x)\approx \prod_i^N \sum_{z\in top-k(p(\cdot\mid x))}p_\eta(z\mid x)p_\theta(y_i\mid x,z_i,y_{1:i-1})\]

<ul>
  <li>Use cases: More suitable for tasks that benefit from integrating detailed information from multiple sources, like open-domain QA.</li>
</ul>

<p><br /></p>

<h2 id="retriever-and-generator">Retriever and Generator</h2>

<p><strong>Retriever</strong> $p_\mu(z\mid x)$ is based on DPR, which follows a bi-encoder architecture:</p>

\[p_\mu(z|x)\propto \exp(\bf d \rm (z)^\top \bf q \rm (x)) \\
\bf d \rm (z)=\rm BERT_d(z), \ \bf q \rm (x)=\rm BERT_q(x)\]

<ul>
  <li>$\bf d \rm (z)$: a dense representation of a document produced by a document encoder based on $\rm BERT_{BASE}$.</li>
  <li>$\bf q \rm (x)$: a query representation produced by a query encoder based on $\rm BERT_{BASE}$.</li>
  <li><span style="background-color:#fff5b1">Maximum inner product search (MIPS)</span>: caculates top-k $p_\eta(\cdot\mid x)$ approximately in sub-linear time.</li>
  <li><span style="background-color:#fff5b1">Non-parametric memory</span>: the index of the document. The retriever is trained to retrieve documents containing answers to TriviaQA questions and Natural Questions.</li>
</ul>

<p><strong>Generator</strong> $p_\theta(y_i\mid x,z,y_{1:i-1})$ can be any encoder-decoder model, based on BART in the paper.</p>
<ul>
  <li>$\rm BART_{large}$ is used: a pre-trained seq2seq transformer with 400M parameters, pre-trained using a denoising objective with various noising functions.</li>
  <li>The input $x$ and the retrieved document $z$ are concatenated and then inputted into $\rm BART$ model to generate the output.</li>
  <li><span style="background-color:#fff5b1">Parametric memory</span>: $\rm BART$ generator parameters $\theta$.</li>
</ul>

<p><br /></p>

<h2 id="training">Training</h2>

<p>The retriever and generator are trained jointly without direct supervision on which document should be retrieved.</p>
<ul>
  <li>Objective: Minimize the negative marginal log-likelihood of each target with a corpus of input/output pairs $(x_j, y_j)$, $\sum_j-\log(p(y_j\mid x_j))$.
    <ul>
      <li>Adam optimizer.</li>
    </ul>
  </li>
  <li>Fine-tuning only the query encoder $\rm BERT_q$ and the generator $\rm BART$ during training.
    <ul>
      <li>Updating the document encoder $\rm BERT_d$ is costly and ineffective
        <ul>
          <li>Requires periodic updating of the document index (as REALM).</li>
          <li>Not necessary for strong performance.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="decoding">Decoding</h2>

<p>For testing, RAG-Sequence and RAG-Token require different methos to approximate $\arg \max_y{p(y\mid x)}$.</p>

<p><strong>RAG-Sequence model</strong> utilizes beam search for each document $z$. It can’t be solved with a single beam search, as the likelihood $p(y\mid x)$ does not break into a conventional per-token likelihood.</p>
<ul>
  <li>Each hypothesis of $z$ is scored by $p_\theta(y_i\mid x,z,y_{1:i-1})$.</li>
  <li>Some hypothesis $y$ included in the set of hypothesis $Y$ may not have appeared in the beams of all documents.</li>
  <li><span style="background-color:#fff5b1">Thorough Decoding</span>: To estimate the probability of $y$, (1) Run an additional forward pass for each $z$ where $y$ doesn’t appear in the beam, (2) multiply the generator probability with $p_\eta(z\mid x)$, and (3) sum the probabilities across beams.</li>
  <li><span style="background-color:#fff5b1">Fast Decoding</span>: For efficient decoding, Approximate $p_\theta(y\mid x,z_i) \approx 0$ where $y$ wasn’t generated during beam search from $x, z_i$, avoiding additional forward passes once the candidate set $Y$ is generated.</li>
  <li>For longer output sequences, $\left\vert Y \right\vert$ can be large with many forward passes.</li>
</ul>

<p><strong>RAG-Token model</strong> is a basic autoregressive seq2seq generator with transition probability:</p>

\[p'_\theta(y_i\mid x,y_{1,i-1})=\sum_{z\in top-k(p(\cdot \mid x))}p_\eta(z_i \mid x)p_\theta(y_i\mid x,z_i,y_{1:i-1})\]

<p><br /></p>

<h1 id="experiments">Experiments</h1>

<p>The experiments were conducted on several datasets to evaluate the model’s performance in knowledge-intensive NLP tasks.</p>
<ul>
  <li>Wikipedia December 2018 dump was used as the non-parametric knowledge source.</li>
  <li>Wikipedia articles were split into 100-word chunks, totaling 21M documents.</li>
  <li>An embedding for each document was calculated by the document encoder $\rm BERT_d$, and a single MIPS index was built with Hierarchical Navigable Small World approximation for fast retrieval.</li>
  <li>When retrieving the top $k$ documents for each query, $k\in {5,10}$ was considered for training, and set using dev data for test time.</li>
</ul>

<h2 id="tasks">Tasks</h2>

<ol>
  <li><strong>Open-domain Question Answering (QA)</strong>: an important real-world application and common testbed for knowledge-intensive tasks.
    <ul>
      <li>Text pairs $(x,y)$ are matched as questions and answers.</li>
      <li>RAG is trained to minimize the negative log-likelihood of answers.</li>
      <li>Close-book QA is also a compared task: generating answers without retrieving but purely with parametric knowledge.</li>
      <li>Datasets: Natural Questions, TriviaQA, WebQuestions, CuratedTREC</li>
    </ul>
  </li>
  <li><strong>Abstractrive Question Answering</strong>: tests natural language generation (NLG) ability with free-form and abstractive cases.
    <ul>
      <li>Use MSMARCO NLG Task v2.1: only the question and answers, not existing gold passages in the dataset, treated as an open-domain abstractive QA task.</li>
    </ul>
  </li>
  <li><strong>Jeopardy Question Generation</strong>: evaluates the generation ability in a non-QA setting.
    <ul>
      <li>Jeopardy: guessing an entity from a fact about that entity.
        <ul>
          <li>e.g., “In 1986 Mexico scored as the first contry to host this international sport competition twice.” where the answer is “The World Cup”.</li>
        </ul>
      </li>
      <li>Jeopardy questions are precise and factual, making it a challenging, knowledge-intensive task to generate them conditioned on the anser entities.</li>
    </ul>
  </li>
  <li><strong>Fact Verification</strong> (FEVER): a retrieval problem coupled with an challenging entailment reasoning task.
    <ul>
      <li>Requires classifying whether a text is supported or refuted by Wikipedia or whether there’s not enough information to decide.</li>
      <li>Provides an appropriate testved for exploring a model’s ability to handle classification rather than generation.</li>
      <li>Two varients: the 3-way classification (supports/refutes/not enough) and the 2-way (support/refutes).</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h1 id="results">Results</h1>

<p>The results demonstrated that both RAG-Sequence and RAG-Token models outperformed baseline models across various datasets and tasks.</p>

<h2 id="open-domain-qa">Open-Domain QA</h2>

<p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table1_2.png?raw=true" style="zoom: 100%;" />
</p>

<ul>
  <li>RAG models significantly outperformed the baselines, showing higher EM and F1 scores.</li>
  <li>The RAG-Token model, in particular, performed well due to its ability to integrate detailed information from multiple documents.</li>
</ul>

<p><br /></p>

<h2 id="abstractive-question-answering">Abstractive Question Answering</h2>

<p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table3.png?raw=true" style="zoom: 100%;" />
</p>

<ul>
  <li>RAG models achieved SOTA performance, even though many questions are unanswerable without the gold passages.</li>
  <li>RAG models hallucinated less and generated more factually correct and diverse text compared to BART (Table 3).</li>
</ul>

<p><br /></p>

<h2 id="jeopardy-question-generation">Jeopardy Question Generation</h2>

<p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table4_5.png?raw=true" style="zoom: 100%;" />
</p>
<p>
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig2.png?raw=true" style="zoom: 100%;" />
</p>

<ul>
  <li>Both of RAG models outperformed BART on Q-BLEU-1 (Table 2).</li>
  <li>Human evaluators indicate that RAG-generated content was more factual in 42.7% of cases, demostrating the effectiveness of RAG over the SOTA generation model (Table 4).</li>
  <li>RAG-Token model performed better than RAG-Sequence, combining content from several documents effectively (Fig 2).</li>
  <li>The generator’s the parametric knowledge sufficed to complete the generation after initially referencing the document (Fig 2).</li>
</ul>

<p><br /></p>

<h2 id="fact-verification">Fact Verification</h2>

<ul>
  <li>For 3-way classification, RAG achieved scores within 4.3% of SOTA models trained with intermediate retrieval supervision for a specific domain.</li>
  <li>For 2-way classification, RAG achieved performance within 2.7% of the base model, SotA, which were trained to classify true of false given the gold evidences.</li>
  <li>The documents retrieved by RAG are overlapped significantly with FEVER’s gold evidence.</li>
</ul>

<p><br /></p>

<h2 id="additional-results">Additional Results</h2>

<ol>
  <li>
    <p><strong>Generation Diversity</strong>: When investigating generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models, RAG models generated more diverse outputs compared to BART. RAG-Sequence produced slightly more diverse outputs than RAG-Token (Table 5).</p>
  </li>
  <li><strong>Retrieval Ablations</strong>: Freezing the retriever during training resulted in lower performance compared to the original RAG models. Replacing the retriever with a BM25 system showed that learned retrieval improved performance for all task (table 6).
    <p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table6.png?raw=true" style="zoom: 100%;" />
</p>
  </li>
  <li>
    <p><strong>Index hot-swapping</strong>: Demonstrated the advantage of non-parametric memory by using an index from Wikipedia dump from December 2016. RAG models still answered 70% of questions correctly, showing that knowledge can be updated simply by replacing the non-parametric memory.</p>
  </li>
  <li><strong>Effect of Retrieving more documents</strong>: Adjusting the number of retrieved documents at test time showed improved performance up to a certain point, demonstrating the benefits of retrieveing more relevant documents (fig 3).
    <p align="center">
    <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table6.png?raw=true" style="zoom: 100%;" />
</p>
  </li>
</ol>

<p><br /></p>

</div>


<div class="related">
  <h2 class="related-title">Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/paper/2024/06/14/mriqcsurvey/">
            Summaries of papers on MRI Quality Assessment and Control&nbsp;
            <small>14 Jun 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/2024/05/28/mriqc_report/">
            [MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)&nbsp;
            <small>28 May 2024</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/">
            [MRIQC 3-1] Opening an HTML file using Flask&nbsp;
            <small>21 May 2024</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


        
          <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
