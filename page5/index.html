<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      Coffee Chat &middot; Brewing AI Knowledge
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/page5/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item active" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/page5/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/study/2023/05/20/rnn/">
        Recurrent Neural Networks (RNNs)
      </a>
    </h1>
    <!--<span class="post-date">20 May 2023</span>-->
    <p class="post-date">20 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Sequence data를 분석하기 위한 딥러닝 모델 구조로, Rumelhart et al., 1986에 근간을 둔다. Deep neural networks (DNNs)와는 달리 hidden state node 간 연결을 통해 이전 시점의 정보를 현재 시점에서 사용할 수 있게 디자인되었다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn.jpg?raw=true" style="zoom: 50%;" />
</p>

<p>현재 시점 node인 $s_t$에 전 시점의 $s_{t-1}$ node에서 정보가 들어온다. 이 정보를 현재 시점의 입력인 $x_t$와 함께 받아 다음 node $s_{t+1}$로 전송될 값을 계산한다. 이 작업을 회귀적으로(recurrently) 진행한다.</p>

<p><br /></p>

<h2 id="weight-sharing">Weight sharing</h2>

<p>Weight $U$, $W$, $V$는 모든 시점에서 동일하다. 이것으로</p>
<ul>
  <li>학습에 필요한 weight 수를 줄일 수 있다.</li>
  <li>데이터의 sequence 길이에 유연하다: 하나의 모델을 다른 길이의 sequence에 적용할 수 있다.
    <ul>
      <li>다른 길이의 sequence에 같은 weight값을 계속 사용함으로써 next token generation이 가능하다</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="rnn-계산">RNN 계산</h2>

<p>위 그림을 보면 hidden state $s_t$와 output $o_t$의 계산은 다음과 같다.</p>

\[\begin{align*}
s_t&amp;=\tau(Ws^{t-1})+Ux^t \\
o_t&amp;=softmax(Vs^t) \\
\end{align*}\]

<p>여기서 node 수가 $D$, $J$, $K$인 경우 각 변수의 차원은 아래와 같다.</p>

\[x\in\mathbb{R}^D, s\in\mathbb{R}^J, o\in\mathbb{R}^K, U\in\mathbb{R}^{J\times D}, W\in\mathbb{R}^{J\times J}, U\in\mathbb{R}^{K\times J}\]

<p><br /></p>

<h2 id="long-term-dependency-problem">Long-term dependency problem</h2>

<p>hidden state 연산은 다음과 같이 표현할 수 있다.</p>

\[s^t=\tau(Ux^t+W\tau(Ux^{t-1}+Ws^{t-2}))\]

<p>$s$가 tanh activation function 내에서 중첩되는 것을 볼 수 있다. 이렇게 되면</p>
<ol>
  <li>feed forward 시 앞 단에서 입력된 정보가 점점 소실된다: tanh의 output은 $\tau(\cdot)\in(-1,1)$인데, 즉 tanh 연산의 중첩은 1보다 작은 값을 계속해서 곱하는 것과 같다. 이렇게 되면 앞에서 곱해진 값은 점점 작아진다.</li>
  <li>back-propagation 시 기울기 소실(gradient vanishing) 혹은 폭발(explosion)의 문제가 생길 수 있다: tanh 함수에 의해 기울기가 0에 가깝게 되거나 너무 커지는 경우가 생긴다. 작은 gradient는 더 작아지고, 큰 gradient는 더 커진다.</li>
</ol>

<p>*Gradient vanishing: back-propagation 시 반영되는 gradient 값이 layer를 지날 수록 소실되는 문제</p>

<p>*Gradient explosion: gradient가 실제 값보다 증폭되어 loss 계산 시 정답과의 차이가 너무 커져, 업데이트에 과도하게 반영되는 문제</p>

<p><br /></p>

<h2 id="다양한-rnn-구조">다양한 RNN 구조</h2>

<p>입출력 형태에 따라 다양하게 RNN을 구성할 수 있다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn_types.png?raw=true" style="zoom: 50%;" />
</p>

<ul>
  <li>One-to-One: hidden state가 1개인 모형, 기본적인 Neural network 구조</li>
  <li>One-to-Many: 하나의 입력값을 받아 순차적으로 여러 개의 값(한 sequence)을 생성</li>
  <li>Many-to-One: 한 sequence를 입력 받아 마지막에 하나의 값을 생성
    <ul>
      <li>e.g. sentence classification</li>
    </ul>
  </li>
  <li>Many-to-Many: 한 sequence를 입력 받아 latent represenation을 구한 후 이것을 통해 sequence를 출력
    <ul>
      <li>e.g. machine translation</li>
    </ul>
  </li>
  <li>
    <p>Many-to-Many: 한 sequence의 매 token을 입력 받는대로 대응하는 token을 출력하여 한 seqeunce를 생성</p>

    <p><br /></p>
  </li>
</ul>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/study/2023/05/20/lstm/">
        Long Short-term Memory (LSTM)
      </a>
    </h1>
    <!--<span class="post-date">20 May 2023</span>-->
    <p class="post-date">20 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Recurrent neural network (RNN)의 Long-term dependency 문제를 해결하고자 만들어진 프레임워크이다.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-lstm/lstm.png?raw=true" style="zoom: 50%;" />
</p>

<p>핵심적인 아이디어는 이전 시점의 state 정보를 이후 state에 얼마나 반영할지를 결정하는 계산을 추가해주는 것이다. 이것을 위해 <strong>forget gate, input gate, output gate</strong>의 3가지 Gate와 <strong>memory cell</strong>이 추가 되었다.</p>

<p>LSTM의 계산 방식과 비교하기 위해 RNN의 계산식을 되짚어보면 다음과 같다.</p>

\[\begin{align*}
h_t&amp;=\tau(Wh^{t-1}+Ux^t) \\
\hat{y}_t&amp;=softmax(Vh^t) \\
\end{align*}\]

<p>이전 시점($t-1$)의 hidden state와 현재 시점($t$)의 input(둘 다 weighted)을 더하여 tanh를 통과시키면 현재 시점의 hidden state가 된다. 이것에 softmax를 취하면 output이 된다.</p>

<p><br /></p>

<h2 id="lstm-계산식">LSTM 계산식</h2>

<p>LSTM은 RNN의 방식에 residual connection 구조에서 착안한 memory 기능을 더하여 long-term dependency 문제를 해결하고자 했다. Memory 기능을 위해 추가된 계산들은 다음과 같다.</p>

<h3 id="forget-gate">Forget gate</h3>

<p>Forget gate $f$는 이전 시점의 정보를 얼마나 잊을지 결정하는 gate이다.</p>

\[f_t=\sigma(W_fh_{t-1}+U_fx_t)\]

<p>이전 시점의 hidden state와 현재 시점의 input을 더한 뒤 sigmoid를 취한다. 이것이 이전 시점의 memory cell state에 곱해진다. sigmoid의 특성에 의해 1에 가까울수록 이전 정보가 이후 많이 반영된다.</p>

<h3 id="input-gate">Input gate</h3>

<p>Input gate $i$는 현재 시점의 input을 다음 시점에 얼마나 반영할지 결정하는 gate이다. 여기서 candidate $\hat(c)$라는 개념이 등장하는데, candidate는 이전 시점의 hidden state와 현재 시점의 input을 고려했을 때 현재의 정보가 어떠한지를 나타내는 cell state의 후보 격인 값이다. 계산 방식이 RNN의 hidden state와 동일하다. input gate 값과 candidate를 곱해 현재의 정보 상 input이 얼마나 반영되면 좋은지를 구하고, 이것을 최종적으로 cell state에 더한다.</p>

\[\begin{align}
i_t&amp;=\sigma(W_{in}h_{_t-1}+U_{in}x_{t})\\
\hat{C}_t&amp;=\tau(W_{c}h_{t-1}+U_{c}x_t)
\end{align}\]

<h3 id="memory-cell">Memory cell</h3>

<p>Memory cell (cell state)은 세 가지 gate와 함께 LSTM의 구현 목적을 위해 추가된 개념이다. 현재 시점의 cell state는 이전 시점의 cell state 및 현재 시점의 forget gate와 현재 시점의 input gate 및 candidate로 계산한다.</p>

\[C_t=f_t*C_{t-1}+i_t*\hat{C}_t\]

<p>$*$는 pointwise operation</p>

<p>이전 정보인 cell state와 현재 input을 얼마나 반영할지가 합해져 현재 시점의 cell state가 구해진다.</p>

<h3 id="output-gate">Output gate</h3>

<p>Output gate는 memory cell을 현재 시점의 hidden state에 얼마나 반영할지 결정한다.</p>

\[\begin{align}
o_t&amp;=\sigma(W_oh_{t-1}+U_ox_t) \\
h_t&amp;=o_t\tau(C_t) \\
&amp;=o_t\tau(f_t*C_{t-1}+i_t*\hat{C}_t) \\
\end{align}\]

<p>현재 시점의 hidden state는 이전 시점의 정보와 현재 시점의 input이 반영된 현재 시점의 cell state와 output gate의 결과값과 곱해져 최종 결정된다.</p>

<h3 id="output">Output</h3>

<p>최종 출력 $\hat{y}_t$은 RNN과 같이 계산된다.</p>

\[\hat{y}_t=softmax(Vh_t)\]

<p><br /></p>

<h2 id="lstm의-한계">LSTM의 한계</h2>

<p>LSTM은 cell state 도입을 통해 gradient vanishing 문제를 해결하고자 하였다. 하지만 RNN 구조를 기반으로 하고 있는 한 이 문제를 완벽하게 해결하기에 한계가 있다. 오히려 gate를 여러 개 사용하여 계산량이 증가하는 문제가 있다.</p>


    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/papers/2023/05/15/alpaca/">
        [Paper] Alpaca: A Strong, Replicable Instruction-Following Model
      </a>
    </h1>
    <!--<span class="post-date">15 May 2023</span>-->
    <p class="post-date">15 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="llm">
              <a href="https://alatteaday.github.io/tags/?tag=llm">
                #llm
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Paper Link</a></p>

<h1 id="points">Points</h1>

<ul>
  <li>Alpaca aims to support academic research on instruction-following large language models (LLMs), addressing deficiencies like hallucinations, toxicity, and biases.</li>
  <li>Uses the self-instruct approach to create an instruction-following dataset with text-davinci-003, costing under $500.</li>
  <li>The LLaMA 7B model is fine-tuned using efficient techniques.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>LLMs trained through instruction-following, such as ChatGPT, have significantly impacted daily life. However, these models still face issues like generating misinformation, toxic content, and exhibiting social biases. To address these problems, academic research is essential. Closed-source models hinder this research, making it difficult to study instruction-following models.</p>

<p>Alpaca is a model designed for academic research, fine-tuned from the LLaMA 7B model using 52k instruction-following data generated from OpenAI’s text-davinci-003. Commercial use of Alpaca is prohibitied by following reasons:</p>

<ul>
  <li>Non-commercial license: LLaMA</li>
  <li>Data restrictions: Based on text-davinci-003 prohibiting competition with OpenAI</li>
  <li>Deployment caution: Not designed with adequate safety mesuares for general use.</li>
</ul>

<p><br /></p>

<h1 id="training-recipe">Training Recipe</h1>

<p>To train a high-quality instruction-following model under an academic budget, two key challenges are addressed:</p>

<ol>
  <li>Strong pre-trained language model: LLaMA models</li>
  <li>High-quality instruction-following data: Self-instruct method</li>
</ol>

<h2 id="self-instruct-method">Self-instruct method</h2>

<ul>
  <li>Seed set: 175 human-written instruction-following output pairs from self-instruct seed set.</li>
  <li>Data generation: Prompting text-davinci-003 to generate more instructions using the seed set as examples.</li>
  <li>Efficiency: Improved the self-instruct method, generating 52k unique instructions and outputs for less than $500 using the OpenAI API.</li>
</ul>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-15-alpaca/fig1.png?raw=true" alt="fig1" style="zoom: 50%;" />
</p>

<h2 id="fine-tuning-the-model">Fine-tuning the model</h2>

<ul>
  <li>Process: LLaMA models are fine-tuned with the generated instruction-following dataset using fully shared data parallel (FSDP) and mixed precision trianing.</li>
  <li>Cost and time: Fine-tuning a 7B LLaMA model took 3 hours on eight 80GB A100s, costing less than $100 on most cloud compute providers.</li>
</ul>

<p><br /></p>

<h1 id="preliminary-evaluation">Preliminary Evaluation</h1>

<p>Human evaluation was conducted on inputs from the self-instruct evaluation set. Key findings include:</p>
<ul>
  <li>Comparison: Alpaca 7B vs. text-davinci-003</li>
  <li>Performance: Alpaca wins 90 to 89 comparisons.
    <ul>
      <li>Given Alpaca’s smaller size and limited data, it performed similarly to text-davinci-003.</li>
    </ul>
  </li>
  <li>Generation style: Alpaca’s outputs tend to be similar with text-davinci-003, and reflect the general style of the training dataset.</li>
  <li>Evaluation limitation: The evaluation data’s limitations should be noted.</li>
  <li>An interactive demo was released to gather further feedback.</li>
</ul>

<p><br /></p>

<h1 id="known-limitiations">Known Limitiations</h1>

<p>Alpaca shares common deficiencies with LLMs, such as hallucinations, toxicity, and stereotypes. It struggles particularly with hallucination, sometimes producing well-written misinformation. Despite these issues, Alpaca provides a lightweight model for studying these deficiencies, aiding academic research.</p>

<p><br /></p>

<h1 id="release">Release</h1>

<p>Released assets:</p>
<ul>
  <li>Demo: Interactive demo for evaluation</li>
  <li>Data: 52k demonstrations used to fine-tune Alpaca</li>
  <li>Data generation process: Code for generating the data</li>
  <li>Training code: Fine-tuning code using Hugging Face API</li>
</ul>

<p>Future release:</p>
<ul>
  <li>Model weights: Pending guidance from Meta</li>
</ul>

<p>The release aims to support academic studies on instruction-following LMs and developing new technique to address the existing deficiencies.</p>

<p><br /></p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/papers/2023/05/10/llama/">
        [Paper] Llama: Open and efficient foundation language models (2023)
      </a>
    </h1>
    <!--<span class="post-date">10 May 2023</span>-->
    <p class="post-date">10 May 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="llm">
              <a href="https://alatteaday.github.io/tags/?tag=llm">
                #llm
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.” <em>arXiv preprint arXiv:2302.13971</em> (2023).</p>

<p><a href="https://arxiv.org/abs/2302.13971">Paper Link</a></p>

<h1 id="points">Points</h1>

<ul>
  <li>Efficient inference with smaller models: LLaMA models prioritize inference efficiency by using smaller models trained on large datasets, achieving state-of-the-art (SOTA) performance across benchmarks while being cost-effective during inference.</li>
  <li>Publicly available data: Unlikely many existing models that rely on proprietary data, LLaMA models are exclusively trained on publicly available datasets, ensuring transparency and compatibility with open-source principles.</li>
  <li>Broad Benchmark Performance: LLaMA models demonstrate competitive performance on a wide range of tasks, including common sense reasoning, question answering, reading comprehension, etc,.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>Large language models (LLMs) have demonstrated remarkable capabilities in performing new tasks with minimal instruction or examples, thanks to their vast size. However, recent research suggests that smaller models trained on larger datasets can achieve superior performance, highlighting the importance of efficiency during inference rather than training.</p>

<p><br /></p>

<h1 id="approach">Approach</h1>

<p>LLaMA is a series of language models (LMs) designed to optimize performance across various inference budgets, ranging from 7B to 65B parameters, using only publicly available data.</p>

<h2 id="pre-training-data">Pre-training data</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table1.png?raw=true" style="zoom: 30%;" />
</p>
<p>The dataset mixture used cover diverse domains and is entirely publicly avaliable, ensuring compatibility with open-source principles:</p>
<ol>
  <li>English CommonCrawl [67%]: Preprocessed from five CommonCrawl dumps (2017-2020)., filtered for non-English and low-quality content.</li>
  <li>C4 [15%]: Similarly preprocessed to CommonCrawl, to enhance performance.</li>
  <li>Github [4.5%]: Filtered for line length and alphanumeric content from Google BigQuery.</li>
  <li>Wikipedia [4.5%]: Dumps from mid-2022, covering multiple languages.</li>
  <li>Gutenberg and Books3 [4.5%]: Publicly available books with redundant content removed.</li>
  <li>ArXiv [2.5%]: Includes scientific data, with non-essential content removed.</li>
  <li>Stack Exchange [2%]: High-quality Q&amp;A content sorted by score.</li>
</ol>

<h3 id="tokenization">Tokenization</h3>

<ul>
  <li>Byte Pair Encoding (BPE) tokenizer used.</li>
  <li>Splits numbers into digits and decomposes unknown UTF-8 characters.</li>
  <li>The training dataset contains approximately 1.4T tokens, with minimal repetition (fig 1).
    <p align="center">
 <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig1.png?raw=true" alt="fig1" style="zoom: 30%;" />
 </p>
  </li>
</ul>

<h2 id="architecture">Architecture</h2>

<p>LLaMA models are based on transformer architecture with key modifications:</p>
<ol>
  <li>Pre-normalization [GPT3]: Normalizes the input of each transformer sub-layer, enhancing training stability using RMSNorm.</li>
  <li>SwiGLU activation function [PaLM]: Uses SwiGLU instead of ReLU, improving performance with a dimension of $2\over3 4d$ instead of $4d$ as in PaLM.</li>
  <li>Rotary Embeddings [GPTNeo]: Employs Rotary embeddings (RoPE) instead of absolute positional embeddings at each layer of the network.</li>
</ol>

<h2 id="optimizer">Optimizer</h2>

<p>Trained using the AdamW optimizer with:</p>
<ul>
  <li>$\beta_1=0.9, \beta_2=0.95$.</li>
  <li>Cosine learning rate schedule, ending at 10% of the maximal rate.</li>
  <li>Weight decay of 0.1 and gradient clipping of 1.0.</li>
  <li>2,000 warmup-steps, with varying learning rates and batch size with the size of the model (table 2).
    <p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table2.png?raw=true" alt="table2" style="zoom: 30%;" />
</p>
  </li>
</ul>

<h2 id="efficient-implementation">Efficient implementation</h2>
<ol>
  <li>Causal multi-head attention: Efficient implementation using xformer library to reduce memory and runtime.</li>
  <li>Activation reductions: Uses checkpointing to recompute activations during the backward pass, especially for computationally expensive layers.</li>
</ol>

<p><br /></p>

<h1 id="main-results">Main Results</h1>

<p>Evaluated on 20 benchmarks for zero-shot and few-shot tasks, compared to non-public models (GPT-3, Gopher, Chinchilla, PaLM) and open-sourced models (OPT, GPT-J, GPT-Neo).</p>

<h2 id="common-sense-reasonging">Common sense reasonging</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table3.png?raw=true" alt="table3" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: Eight standard benchmarks such as BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA. Theses datasets include Cloze and Winograd style tasks and multiple choice question answering (QA).</li>
  <li>Results
    <ul>
      <li>LLaMA-65B outperforms Chinchilla 70B and PaLM-540B on most benchmarks except BoolQ.</li>
      <li>LLaMA-13B outperforms GPT-3 on most benchmarks despite being significantly smaller.</li>
    </ul>
  </li>
</ul>

<h2 id="close-book-question-answering">Close-book question answering</h2>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table4.png?raw=true" alt="table4" style="zoom: 30%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table5.png?raw=true" alt="table5" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: Natural Questions and TriviaQA. The models report exact match performance where the models do not have access to documents that contain avidence to answer the question.</li>
  <li>Results:
    <ul>
      <li>LLaMA-65B achieve state-of-the-art (SOTA) performance in zero-shot and few-shot settings.</li>
      <li>LLaMA-13B is competitive with GPT-3 and Chinchilla which are larger models.</li>
    </ul>
  </li>
</ul>

<h2 id="reading-comprehension">Reading comprehension</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table6.png?raw=true" alt="table6" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmark: RACE reading comprehension, collected from English reading comprehension exams in middle and high school Chinese students.</li>
  <li>Results: LLaMA-65B is competitive with PaLM-540B, and LLaMA-13B outperforms GPT-3.</li>
</ul>

<h2 id="mathematical-reasoning">Mathematical reasoning</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table7.png?raw=true" alt="table7" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: MATH and GSM8k. MATH contains 12K math problems of middle and high school. GSM8k is a set of middle school math problems.</li>
  <li>Results: LLaMA-65B outperforms Minerva-62B on GSM8k.
    <ul>
      <li>Minerva is a series of PaLM models fine-tuned on 38.5B tokens extracted from ArXiv and Math Web Pages. Both PaLM and LLaMA, however, are not finetuned on math data.</li>
    </ul>
  </li>
</ul>

<h2 id="code-generation">Code generation</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table8.png?raw=true" alt="table8" style="zoom: 30%;" />
</p>
<ul>
  <li>Benchmarks: HumanEval and MBPP. The models are evaluated about their ability to write code from a natural language description.</li>
  <li>Results:
    <ul>
      <li>LLaMA models outperform other models, including LaMDA and PaLM. LLaMA-13B outperforms LaMDA-137B. LLaMA 65B outperforms PaLM-62B.</li>
      <li>Fine-tuning on code-specific tokens further improves performance.</li>
    </ul>
  </li>
</ul>

<h2 id="massive-multitask-language-understanding">Massive multitask language understanding</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table9.png?raw=true" alt="table9" style="zoom: 30%;" />
</p>
<ul>
  <li>Massive multitask language understanding (MMLU) consists of multiple choice questions covering various domains of knowledge, like humanities, STEM and social sciences.</li>
  <li>Results: LLaMA-65B underperforms compared to Chinchilla-70B and PaLM-540B, possibly due to limited academic data.</li>
</ul>

<h2 id="evolution-of-performance-during-training">Evolution of performance during training</h2>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig2.png?raw=true" alt="fig2" style="zoom: 30%;" />
</p>
<ul>
  <li>Performance improves steadily, and correlates with the training perplexity of the model.</li>
  <li>SIQA and WinoGrande are the exceptions: SIQA may not be reliable as performance is varied, and performance doesn’t correlate with training perplexity on WinoGrande.</li>
</ul>

<p><br /></p>

<h1 id="instruction-fine-tuning">Instruction Fine-tuning</h1>

<p>Fine-tuning improves performance and futher the ability to follow instructions. LLaMA-I is trained on MMLU with instructions and compared with OPT-IML and Flan-PaLM series which fine-tuned with moderate sizes.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table10.png?raw=true" alt="table10" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA-I with 65B parameter size outperforms existing instruction fine-tuned models, but remains behind GPT ‘code-davinci-002’.</li>
</ul>

<p><br /></p>

<h1 id="bias-toxicity-and-misinformation">Bias, Toxicity and Misinformation</h1>

<p>LLMs have been showed to be biased to content of training data, and to generate toxic content. Evaluated using benchmarks for toxic content generation and stereotypes detection.</p>

<h2 id="realtoxicityprompts">RealToxicityPrompts</h2>

<p>Indicates how toxic is a model. The toxicity score is automatically evaluated by making a request to PerspectiveAPI, ranging from 0 (non-toxic) to 1 (toxic).</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table11.png?raw=true" alt="table11" style="zoom: 30%;" />
</p>
<ul>
  <li>Comparable to other models, with larger models exhibiting more toxicity, especially for “Respectiful” prompts.</li>
  <li>It can be suggested that the relation between toxicity and model size may only apply within a model family.</li>
</ul>

<h2 id="crows-pairs">CrowS-Pairs</h2>

<p>Evaluates the biases in a model with 9 categories: gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socioenconomic status.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table12.png?raw=true" alt="table12" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA shows slight biases, particularly in the religion, age, and gender categories. This may be come from CommonCrawl dataset.</li>
</ul>

<h2 id="winogender">WinoGender</h2>

<p>Used to investigate the bias of a model on the gender category. It evaluates if the model’s co-reference resolution performance is impacted by the gender of the pronoun.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table13.png?raw=true" alt="table13" style="zoom: 30%;" />
</p>
<ul>
  <li>Performance varies by pronoun type: The models have better performance “their/them/someone” pronouns than for the “her/her/she” and “his/him/he” pronouns.</li>
  <li>Larger models showing more gender bias: For “gotcha” cases, LLaMA-65B makes more errors, showing that it capture biases on gender.
    <ul>
      <li>“gotcha” cases are in which the pronoun does not match the majority gender of the occupation, and the occupation is the correct answer.</li>
    </ul>
  </li>
</ul>

<h2 id="truthfulqa">TruthfulQA</h2>

<p>Evaluates the a model’s ability to identify true claims and measures the risk of generating misinformation or false claims. This assesses the truthfulness of a model’s responses.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table14.png?raw=true" alt="table14" style="zoom: 30%;" />
</p>
<ul>
  <li>LLaMA models show better truthfulness compared to GPT-3. However the correct answer rate remains low, indicating a potential for misinformation.</li>
</ul>

<p><br /></p>

<h1 id="carbon-footprint">Carbon footprint</h1>

<p>Details the environmental impact of training and deploying these models.</p>
<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table15.png?raw=true" alt="table15" style="zoom: 30%;" />
</p>

<p><br /></p>


    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/papers/2023/04/30/rlhf/">
        [Paper] Training language models to follow instructions with human feedback (2022)
      </a>
    </h1>
    <!--<span class="post-date">30 Apr 2023</span>-->
    <p class="post-date">30 Apr 2023&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="nlp">
              <a href="https://alatteaday.github.io/tags/?tag=nlp">
                #nlp
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="llm">
              <a href="https://alatteaday.github.io/tags/?tag=llm">
                #llm
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <style>
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: 2.5em;
   margin-bottom: -0.5em;
   margin-left: 0em;
   margin-right: 0em;
}
</style>

<p>Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” <em>Advances in neural information processing systems</em> 35 (2022): 27730-27744.</p>

<p><a href="https://arxiv.org/abs/2203.02155">Paper Link</a></p>

<h1 id="point">Point</h1>

<ul>
  <li>Employs <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> to fine-tune GPT-3 models, aligning them with human intentions while reducing unintended behaviors like hallucinations and toxicity.</li>
  <li><strong>InstructGPT</strong> models outperforms GPT-3 in truthfulness and reliability, generalizing well to new tasks like non-English and coding instructions.</li>
  <li>Highlights the need for diverse stakeholder input and suggest combining RLHF with other methods to improve model alignment and safety.</li>
</ul>

<p><br /></p>

<h1 id="background">Background</h1>

<p>Language models (LMs) often generate misinsformation, toxic or biased content and this issue cannot be resolved simply by increasing the model size. Understanding user intent is crucial for these models. Fine-tuning with human feedback can align the models with user intentions across various tasks.</p>

<p>Large language models (LLMs) frequently exhibit uninteded behaviors, such as hallucinations, toxic text generation, failing to follow user instructions. These are influenced by the model’s objective, which typically involves predicting the next token based on web data, differing from the goal of “following the user instructions helpfully and safely”.</p>

<p>To align LMs, this paper employs <strong>Reinforcement Learning from Human Feedbak (RLHF)</strong> to fine-tune GPT-3 to follow instructions. Human preferences serve as a reward signal for this fine-tuning process.</p>

<p><br /></p>

<h1 id="methods-and-experimental-details">Methods and experimental details</h1>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig2.png?raw=true" alt="fig2" style="zoom: 50%;" />
</p>

<h2 id="high-level-methology">High-level methology</h2>

<ol>
  <li>Preparation: Utilize pre-trained language models (GPT-3), prepare a distribution of prompts for alignment, and train human labelers.</li>
  <li>Collect demonstration data and train a supervised policy: Labelers provide input prompts as desired behavior responses. The model is fine-tuned on this data using supervised learning.</li>
  <li>Collect comparison data and train a reward model: Labelers compare model outputs and indicate their preferences. A reward model (RM) is trained using these comparisons to predict human-preferred outputs.</li>
  <li>Optimize a policy aganst the RM using PPO: The RM’s output serves as a scalar reward. The supervised policy (trained GPT-3) is fine-tuned using the PPO algorithm to optimize this reward.</li>
</ol>

<p>Step 2 and 3 can be iterative: More comparison data is collected on the current best policy, used to train a new RM and subsequently a new policy.</p>

<p><br /></p>

<h2 id="dataset">Dataset</h2>

<p>Source of prompts:</p>
<ul>
  <li>Consists of text prompts submitted to the OpenAI API, specifically those using an earlier version of InstructGPT models on the Playground interface.</li>
  <li>The paper does not include data from customers using the API in production.</li>
</ul>

<p>Deduplication and filtering:</p>
<ul>
  <li>Heuristically deduplicated by checking for prompts that share a long common prefix.</li>
  <li>The number of prompts is limited to 200 per user ID.</li>
  <li>Validation and test sets contain no data from users whose data is in the training set.</li>
  <li>All prompts in the training split were filtered for personally indentifiable information (PII).</li>
</ul>

<p>Initial source of prompts: Human-written prompts were used as an initial source of instruction to bootstrap the process.</p>

<p>Datasets for fine-tuning:</p>
<ul>
  <li>SFT dataset: Labelers’ demonstrations (13k prompts, from the API and labeler-written examples).</li>
  <li>RM dataset: Labeler rankings of model outputs (33k, from the API and labeler-written examples).</li>
  <li>PPO dataset: Inputs for RLHF fine-tuning. Human labels were not used (31k, only from the API).</li>
</ul>

<p>Use cases: Most of the use-cases have are generative, rather than classification of prompts submitted to InstructGPT models</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true" alt="fig1" style="zoom: 35%;" />
</p>

<p><br /></p>

<h2 id="tasks">Tasks</h2>

<p>Datasets for training tasks</p>
<ul>
  <li>Sources: The datasets are sourced from prompts written by labelers and those submitted to early versions of InstructGPT models via API.</li>
  <li>Labeler Instructions: Labelers are trained and instructed to write prompts with specific intents or implicit goals in mind to ensure the model aligns with desired behaviors.</li>
  <li>Language: The datasets are predominately in English (95%). However, the paper also reports the models’ performance in other languages.</li>
</ul>

<p><br /></p>

<h2 id="human-data-collection">Human data collection</h2>

<p>Selection of Labelers: A diverse group of labelers was selected to ensure a broad demographic representation. It aims to generate inputs with a wide range of perspectives and to identify potentially harmful outputs.</p>

<p>Training and Evaluation: Labelers underwent tests designed to measure their performance in labeling according to the set standards. This included their ability to generate diverse prompts and accurately identify harmful content.</p>

<p><br /></p>

<h2 id="models">Models</h2>

<p>Pre-trained GPT models are utilized as basis. These models are trianed on a broad distribution of Internet data and can be used for various tasks but initially exhibit poorly characterized behavior. The GPT-3 models are then further trained using three different techniques:</p>

<p><br /></p>

<h3 id="supervised-fine-tuning-sft">Supervised fine-tuning (SFT)</h3>

<p>This method fine-tunes GPT-3 on labeler demonstrations using supervised learning.</p>

<ul>
  <li>Training details: 16 epochs using a cosine learing rate decay and a residual dropout of 0.2.</li>
  <li>Model selection: Based on the model’s RM score on the validation set.</li>
  <li>Finding: Training for more epochs improves both the RM score and human preference ratings, depite some overfitting.</li>
</ul>

<p><br /></p>

<h3 id="reward-modeling-rm">Reward modeling (RM)</h3>

<ul>
  <li>Base model: Starts with a pre-trained SFT model but the final unembedding layer is removed. This layer maps the model’s representations to the vocabulary space for generating output tokens.</li>
  <li>Input and output: The model takes a prompt and a response are as input and outputs a scalr reward, representing theh quality of the response for the given prompt.</li>
  <li>Model size: Utilizes 6B reward model (RM) for efficiency. A larger 175B RM was found to be unstable and unsuitable for use as the value function in RL.</li>
  <li>Data: Uses comparisons between two model outputs for the same input to determine which output is preferred by human labelers.</li>
  <li>Loss: Trained with cross-entropy loss, using the comparisons as labels. The reward difference reflect the log odds of one response being preferred over the other by a labeler.</li>
  <li>Speed-up comparison collection: Labelers are presented with $K$ responses to rank for each prompt, where $K$ ranges from 4 to 9. This results in $K(K-1) \over 2$ comparisons for each prompt.</li>
  <li>Training efficiency and overfitting:
    <ul>
      <li>Comparisons within each labeling task are very correlated. If all comparisons are shuffled into one dataset and processed in a single pass, the model tends to overfit.</li>
      <li>To address this, the training treats all $K(K-1) \over 2$ comparisons from each prompt as a single batch element, offering several benefits:
        <ul>
          <li>Requires only one forward pass for each set of $K$ responses, instead of $K(K-1) \over 2$ forward passes.</li>
          <li>Prevents overfitting by avoiding isolated highly correlated comparisons.</li>
          <li>Improves computational efficiency, and achieves better validation accuracy and log loss.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Loss function:</p>

\[loss(\theta)=-{1\over \binom{K}{2}} E_{(x,y_w,y_l)~D}[\log(\sigma(r_\theta(x,y_w)-r_\theta(x,y_l)))]\]

    <ul>
      <li>$r_\theta(x,y)$ is the scalar output of the RM for promt $x$ and completion $y$ with parameters $\theta$.</li>
      <li>$y_w$ is preferred completion out of the pair of $y_w$ and $y_l$.</li>
      <li>$D$ is the dataset of human comparisons.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="reinforcement-learning-rl">Reinforcement learning (RL)</h3>

<ul>
  <li>Base model: The SFT model is fine-tuned using Proximal Policy Optimization (PPO) in an environment.</li>
  <li>Training environment: A bandit environment. It this context, a bandit environment presents a random customer prompt, expects a response, produces a reward determined by the RM, and ends the episode.</li>
  <li>Input and output: The model takes the prompt and response as input and outputs a reward determined by the RM.</li>
  <li>KL penalty: A per-token Kullback-Leibler (KL) penalty is added from the SFT model at each token.
    <ul>
      <li>This penalty mitigates over-optimization of the RM and prevents the model from deviating too far from the behavior learned during supervised fine-tuning.</li>
      <li>The value funciton used in PPO is initialized from the RM.</li>
    </ul>
  </li>
  <li>PPO and PPO-ptx models:
    <ul>
      <li>PPO models: Fine-tuned with PPO.</li>
      <li>PPO-ptx models: Involve an additional experiment where pre-training gradients are mixed into PPO gradients to address performance regressions on public NLP datasets.</li>
      <li>
        <p>The objective function for PPO-ptx:</p>

\[\begin{aligned}
\text{objective}(\phi) = &amp; \ \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{RL}}} \left[ r_\theta(x, y) - \beta \log \left( \frac{\pi_\phi^{RL}(y | x)}{\pi^{SFT}(y | x)} \right) \right] \\
&amp; + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log(\pi_\phi^{RL}(x)) \right]
\end{aligned}\]

        <p>where:</p>

        <ul>
          <li>$\pi_\phi^{RL}$ is the learned RL policy and $\pi^{SFT}$ is the supervised fine-tuned model.</li>
          <li>$D_{\pi^{RL}}$ is the distribution of data under the RL policy, and $D_{pretrain}$ is the pre-training distribution.</li>
          <li>$\beta$ is the KL reward coefficient, controlling the strength of the KL penalty.</li>
          <li>$\gamma$ is the pre-training loss coefficient, controlling the influence of pre-training gradients. For PPO models $\gamma$ is set to 0.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In this paper, InstructGPT refers to the PPO-ptx models.</li>
</ul>

<p><br /></p>

<h3 id="baselines">Baselines</h3>

<p>The performance of PPO models is compared against several baselines:</p>
<ul>
  <li>SFT models: Fine-tuned using supervised learing.</li>
  <li>GPT-3: The standard GPT-3 model without additional fine-tuning.</li>
  <li>GPT-3 Prompted: Provided with a few-shot previx to prompt it into an instruction-following mode, where the prefix is prepended to the user-specified instruction.</li>
  <li>InstructGPT is compared to 175B GPT-3 models fine-tuned on FLAN and T0 datasets. These datasets include various NLP tasks combined with natural language instructions.</li>
</ul>

<p><br /></p>

<h2 id="evaluation">Evaluation</h2>

<p>The definition of “alignment” to evaluate models is based on their ability to act in accordance with user intentions. The practical evaluation framework checks if the model is helpful, honest and harmless.</p>

<ul>
  <li>Helpfulness: The model should follow instructions and infer intentions from prompts or a patterns.
    <ul>
      <li>Since the intention could be unclear, labeler preference ratings are considered mainly for evaluation.</li>
      <li>There may be divergence between actual user intentions and labeler interpretations.</li>
    </ul>
  </li>
  <li>Honesty: Truthfulness is measured instead of comparing the model’s output to its actual belief.
    <ul>
      <li>Two metrics are used:
        <ul>
          <li>The model’s tendency to fabricate information on closed domain tasks</li>
          <li>Performance on the TruthfulQA dataset.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Harm: Harmfulness depends on the context in which the model is used, and assessing potential harm requires significatn speculation.
    <ul>
      <li>More specific proxy criteria are used:
        <ul>
          <li>Whether a deployed model could be harmful.</li>
          <li>Labelers evaluate if an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content.</li>
          <li>Benchmarks like RealToxicityPrompts and CrowS-pairs are used to measure bias and toxicity.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="evaluation-on-api-distiribution">Evaluation on API distiribution</h3>

<p>When using prompts from the API for evaluting human preference ratings, only prompts not included in training are selected.</p>

<p>Since prompts for InsturctGPT models are not suitable for the GPT-3 baselines, prompts submitted to the GPT-3 API are also used for evaluation.</p>
<ul>
  <li>The GPT-3 prompts are not in an instruction-following style.</li>
  <li>The 175B SFT model is chosen as the baseline due to its average performance.</li>
</ul>

<p>Each model is evaluated based on how often its outputs are preferred, and labelers judge the overall quality of each response on a 1-7 Likert scale.</p>

<p><br /></p>

<h3 id="evaluation-on-public-nlp-datasets">Evaluation on public NLP datasets</h3>

<p>Two types of public datasets are used:</p>
<ul>
  <li>Safety evaluation: Focuses on truthfulness, toxicity, and bias. Includes evaluations of toxicity using the RealToxicityPrompts dataset.</li>
  <li>Zero-shot performance: Assesses performance on traditional NLP tasks such as question anwering (QA), reading comprehension, and summarization.</li>
</ul>

<p><br /></p>

<h1 id="results">Results</h1>

<p>The experimental results are organized into three parts: results on the API prompt distribution, results on public NLP datasets, and qualitative results.</p>

<h2 id="results-on-the-api-distribution">Results on the API distribution</h2>

<h3 id="1-labelers-significantly-prefer-instructgpt-outputs-over-outputs-from-gpt-3">1. Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true" alt="fig1" style="zoom: 35%;" />
</p>

<ul>
  <li>175B InstructGPT outputs are preferred to GPT-3 outputs around 85% of the time and around 71% compared to few-shot GPT-3.</li>
  <li>The preference order is GPT-3 &lt; GPT-3 Prompted &lt; SFT &lt; PPO.</li>
  <li>Adding updates on the pre-training mix during PPO does not lead to significant changes in labeler preference.</li>
</ul>

<p class="a" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig3.png?raw=true" alt="fig3" style="zoom: 35%;" />
</p>

<ul>
  <li>This preference trend remains consistent when evaluating models on prompts submitted to GPT-3 models on the API, though PPO-ptx models perform slightly worse at larger sizes.</li>
</ul>

<p class="a" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig4.png?raw=true" alt="fig4" style="zoom: 35%;" />
</p>

<ul>
  <li>InstructGPT outputs are rated favorably on more concrete axes: They follow constraints and instruction better and hallucinate less.</li>
  <li>This suggests that InstructGPT models are more reliable and easier to control than GPT-3.</li>
</ul>

<p><br /></p>

<h3 id="2-instructgpt-models-generalize-to-the-preferences-of-held-out-labelers-that-did-not-produce-any-training-data">2. InstructGPT models generalize to the preferences of “held-out” labelers that did not produce any training data.</h3>

<ul>
  <li>InstructGPT models’ outputs are rated better than GPT-3 baselines by held-out labelers, indicating InstructGPT models are not simiply overfitting to the preferences of training labelers.</li>
  <li>RMs also demonstrate generlization capabilties with cross-validation results: 69.6% accuracy in predicting the preferences of held-out labelers, which is slightly lower than 72.4% accuracy in the predicting preferences within the training set.</li>
</ul>

<p><br /></p>

<h3 id="3-public-nlp-datasets-are-not-reflective-of-how-the-lms-are-used">3. Public NLP datasets are not reflective of how the LMs are used.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig5.png?raw=true" alt="fig5" style="zoom: 35%;" />
</p>

<ul>
  <li>When comparing InstructGPT to 175B GPT-3 baseline fine-tuned on FLAN and T0, these models perform better than GPT-3 with a good prompt but worse than the SFT baseline. This suggests the datasets are not sufficiently diverse to improve API prompt distribution.</li>
  <li>InstructGPT may outperform FLAN and T0 because:
    <ul>
      <li>Public NLP datasets are desinged to capture typical tasks that are easy to evaluate (e.g., classification, QA). However, open-ended generation and brainstorming constitute most (57%) of tasks the API users want.</li>
      <li>Public NLP datasets may lack the high diversity of inputs that real-world users are interested in.</li>
    </ul>
  </li>
</ul>

<h2 id="results-on-public-nlp-datasets">Results on public NLP datasets</h2>

<h3 id="1-instructgpt-models-show-improvements-in-truthfulness-over-gpt-3">1. InstructGPT models show improvements in truthfulness over GPT-3.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig6.png?raw=true" alt="fig6" style="zoom: 35%;" />
</p>

<ul>
  <li>PPO models demonstrate significant improvements on the TruthfulQA dataset.</li>
  <li>The 1.3B PPO-ptx model performs slightly worse than GPT-3 of the same size.</li>
  <li>Training with an “Instruction+QA” prompt helps the model avoid generating false information.
    <ul>
      <li>Instruction+QA: Instructs the model to respond with “I have no comment” when it’s uncertain of the correct answer.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="2-instructgpt-shows-small-improvements-in-toxicity-over-gpt-3-but-not-bias">2. InstructGPT shows small improvements in toxicity over GPT-3, but not bias.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig7.png?raw=true" alt="fig7" style="zoom: 35%;" />
</p>

<ul>
  <li>Toxicity: Evaluated using the RealToxicityPrompts benchmark.
    <ul>
      <li>Evaluation method: Toxicity scores are obtained through the Perspective API with model samples and labelers rate the samples.</li>
      <li>InstructGPT outputs are less toxic than those of GPT-3 when instructed to generate respectful outputs. Without any prompt, the models are similar, and InstructGPT can be more toxic when prompted to produce toxic content.</li>
    </ul>
  </li>
  <li>Bias: Evaluated using the Winogender and CrowS-Pairs benchmarks.
    <ul>
      <li>Evaluation method: Calculates the relative probabilities of producing sentences in each pair and the entropy of the associated binary probability distributions.
        <ul>
          <li>Unbiased models will show no preference, thus having maximum entropy.</li>
        </ul>
      </li>
      <li>InstructGPT and GPT-3 show similar levels of bias. The PPO-ptx model shows higher bias when instructed to act respectfully, with unclear patterns.</li>
      <li>Instructed models tend to be more certain of their outputs, regardlessly with stereotypes.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="3-modifying-rlhf-fine-tuning-procedures-can-minimize-performance-regressions-on-public-nlp-datasets">3. Modifying RLHF fine-tuning procedures can minimize performance regressions on public NLP datasets.</h3>

<ul>
  <li>Alignment tax: PPO model experience a decrease in performance on public NLP datasets, referred to as “alignment tax”.</li>
</ul>

<p style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig28.png?raw=true" alt="fig28" style="width: 49%; vertical-align:text-top;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig29.png?raw=true" alt="fig29" style="width: 49%; vertical-align:text-top;" />
</p>

<ul>
  <li>Mitigation strategies: Mixing pre-training updates to the PPO fine-tuning (PPO-ptx) reduces performance regressions across all datasets.</li>
</ul>

<p class="a" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig33.png?raw=true" alt="fig33" style="zoom: 35%;" />
</p>

<ul>
  <li>PPO-ptx performs better than merely increasing the KL coefficient. Changing the KL model from the PPO initialization to GPT-3 yields similar improvements.</li>
</ul>

<p><br /></p>

<h2 id="qualitative-results">Qualitative results</h2>

<h3 id="1-instructgpt-models-show-promising-generlization-to-instructions-outside-of-the-rlhf-fine-tuning-distribution">1. InstructGPT models show promising generlization to instructions outside of the RLHF fine-tuning distribution.</h3>

<ul>
  <li>InstructGPT models can follow non-English instructions, and perform coding tasks, despite limited training data in these formats.</li>
  <li>Alignment methods can generalize to produce desired behaviors on inputs not directly supervised.</li>
</ul>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig8.png?raw=true" alt="fig8" style="zoom: 35%;" />
</p>

<ul>
  <li>175B PPO-ptx model can answer questions about code and non-English instructions, but often responds in English to questions in other languages.</li>
</ul>

<p><br /></p>

<h3 id="2-instructgpt-still-makes-simple-mistakes">2. InstructGPT still makes simple mistakes.</h3>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig9.png?raw=true" alt="fig9" style="zoom: 35%;" />
</p>

<ul>
  <li>The model sometimes incorrectly assumes a false premise in an instruction is true.</li>
  <li>It can overly hedge even when the answer is clear.</li>
  <li>It struggles with generating responses when there’re multiple or challenging constraints in an instruction.</li>
</ul>

<p><br /></p>

<h1 id="discussion">Discussion</h1>

<h2 id="implications-for-alignment-research">Implications for alignment research</h2>

<p>Improving the alignment of current AI systems provides a clear empirical feedback loop, esssential for refining alignment techniques.</p>

<p>Moreover, RLHF is an important building block for aligning superhuman systems, especially for tasks difficult to evaluate.</p>

<p>General lessons for alignment research:</p>

<ul>
  <li><strong>The cost of increasing model alignment is modest relative to pre-training</strong>: The significant costs lie in data collection and computation. With RLHF, larger LMs become more helpful, suggesting investing in aligning existing LMs is more efficient than training new, larger models.</li>
  <li><strong>There is evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in</strong>: E.g., non-English and code tasks. This is important as creating supervised models for each task is expensive.</li>
  <li><strong>The proposed fine-tuning can mitigate most of the performance degradations</strong>: Low alignment tax techniques are needed for future AI systems capable of understanding human intents, and RLHF is effective in this regard.</li>
  <li><strong>Alignment techniques are validated in the real world</strong>: This work grounds alignment research in real-world applications, providing valuable insights for AI systems used by actual users.</li>
</ul>

<p><br /></p>

<h2 id="who-are-we-aligning-to">Who are we aligning to?</h2>

<p>Factors influencing the fine-tuning data and key sources of alignment preferences:</p>

<ul>
  <li>Labelers’ preferences: The models are aligned to the preferences of hired labelers who generate the training data. They are mostly English speakers, with around 73% agreement among them.</li>
  <li>Researchers’ preferences: Researchers design the study, write instructions, and guide labelers on edge cases, thereby influencing the alignment. More research is needed to understand the impact of different instructions and interfaces on the collected data and model behavior.</li>
  <li>Customer prompts: Training data includes prompts from OpenAI customers using the API. There is potential misalignment between customer goals and end-user well-being.</li>
  <li>Customer representation: The customers are not representative of all potential or current LM users. The initial user base was biased towards OpenAI’s networks.</li>
</ul>

<p>Challenges and future directions:</p>

<ul>
  <li>Designing a fair and transparent alignment process is complex.</li>
  <li>This paper demonstrates that the alignment method can work for a specific human reference group but doesn’t claim these group preferences are ideal.</li>
  <li>Multiple stakeholders need consideration, including model trainers, developers, end-users, and the broader impacted population.</li>
  <li>Aligning a system to everyone’s preferences simultaneously is impossible, and not all trade-offs will be universally endorsed.</li>
  <li>One potential approach is to train models for different group preferences so that it can reflect diverse values. However, this may still impact broader society, raising decisions about prioritizing preferences.</li>
</ul>

<p><br /></p>

<h2 id="limitations">Limitations</h2>

<p>Methodology:</p>

<ul>
  <li>Contractor influence: InstructGPT is influenced by the human feedback from about 40 contractors.
    <ul>
      <li>Contractors’ identity, beliefs, cultural backgrounds, and personal history may affect their judgments.</li>
      <li>They were selected based on their performance with sensitive prompts and labeling tasks.</li>
      <li>The small team size allowed for better communication but is not representative of the broader population will use the models.</li>
      <li>They are mostly  English-speaking, and the data is almost entirely in English.</li>
    </ul>
  </li>
  <li>Data collection improvements: Most comparisons are labeled by only one contractor to reduce costs.
    <ul>
      <li>Multiple labelings could help identify disagreement areas, indicating where a single model may not align with all labelers.</li>
      <li>Averaging labeler preferences for disagreements might not be ideal, especially for minority groups, whose preferences should be weighted more heavily.</li>
    </ul>
  </li>
</ul>

<p>Models:</p>
<ul>
  <li>Imcomplete alignment and safety: InstructGPT is not fully aligned or safe.
    <ul>
      <li>It still generates toxic or biased outputs, misinformations, and sexual or violent content.</li>
      <li>It sometimes fails to generate reasonable outputs for certain inputs.</li>
    </ul>
  </li>
  <li>Following potentially harmful instructions: InstructGPT often follows instructions even if it could lead to real-world harm.
    <ul>
      <li>It produces more toxic outputs than GPT-3 when instructed to be maximally biased.</li>
    </ul>
  </li>
</ul>

<p><br /></p>


    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="https://alatteaday.github.io/page6">Older</a>
  
  
    
      <a class="pagination-item newer" href="https://alatteaday.github.io/page4">Newer</a>
    
  
</div>

        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
