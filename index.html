<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--site.title site.tagline-->
  <title>
    
      Coffee Chat &middot; Brewing AI Knowledge
    
  </title>

  
  <link rel="canonical" href="https://alatteaday.github.io/">
  

  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/poole.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/syntax.css">
  <link rel="stylesheet" href="https://alatteaday.github.io/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://alatteaday.github.io/public/favicon.ico/apple-touch-icon.png">
  <link rel="shortcut icon" href="https://alatteaday.github.io/public/favicon.ico/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="https://alatteaday.github.io/atom.xml">

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>Curation of studies, techs, ideas and a journey as a maching learning engineer</p>
  </div>

  <nav class="sidebar-nav">

    <a class="sidebar-nav-item" href="https://alatteaday.github.io/about">About</a>
    <a class="sidebar-nav-item active" href="https://alatteaday.github.io/">Home</a>
    <a class="sidebar-nav-item" href="https://alatteaday.github.io/tags">Tags</a>

    

    
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
          <a class="sidebar-nav-item" 
          href="https://alatteaday.github.io/about/">About</a>
        
        -->
        
      
    
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/error/">Dev Tips & Fixes</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/papers/">Papers</a>
        
      
    
      
        <!--
        
        -->
        
          <a class="sidebar-nav-item "
          href="https://alatteaday.github.io/category/study/">Study</a>
        
      
    
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
      
        <!--
        
        -->
        
      
    
    <!--
    <a class="sidebar-nav-item" href="/archive/v1.1.0.zip">Download</a>
    <a class="sidebar-nav-item" href="">GitHub project</a>
    <span class="sidebar-nav-item">Currently v1.1.0</span> 
    -->
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2024. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <!--site.title site.tagline-->
    <div class="wrap">
      <div class="masthead">
        <div class="container" >
          <h3 class="masthead-title">
            <a href="/" title="Home">Coffee Chat</a>
            <small>Brewing AI Knowledge</small>
          </h3>
          <div class="lang-switcher">
    
    
        eng
    

    
    
        
            <a href="/ko/">kor</a>
        
    

</div>
        </div>
      </div>

      <div class="container content">
        <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/paper/2024/06/14/mriqcsurvey/">
        Summaries of papers on MRI Quality Assessment and Control
      </a>
    </h1>
    <!--<span class="post-date">14 Jun 2024</span>-->
    <p class="post-date">14 Jun 2024&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="brainImaging">
              <a href="https://alatteaday.github.io/tags/?tag=brainImaging">
                #brainImaging
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="mri">
              <a href="https://alatteaday.github.io/tags/?tag=mri">
                #mri
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>I summarized four papers related to MRI quality assessment and control. Below are the summaries:</p>

<h1 id="paper-list">Paper list</h1>

<ul>
  <li>Liao, Lufan, et al. “Joint image quality assessment and brain extraction of fetal MRI using deep learning.” <em>Medical Image Computing and Computer Assisted Intervention–MICCAI</em> <em>2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VI 23</em>. Springer International Publishing, 2020.</li>
  <li>Giganti, Francesco, et al. “Prostate Imaging Quality (PI-QUAL): a new quality control scoring system for multiparametric magnetic resonance imaging of the prostate from the PRECISION trial.” European urology oncology 3.5 (2020): 615-619.</li>
  <li>Esses, Steven J., et al. “Automated image quality evaluation of T2‐weighted liver MRI utilizing deep learning architecture.” <em>Journal</em> <em>of</em> <em>Magnetic</em> <em>Resonance</em> <em>Imaging</em> 47.3 (2018): 723-728.</li>
  <li>Monereo-Sánchez, Jennifer, et al. “Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations-insights from the Maastricht study.” <em>Neuroimage</em> 237 (2021): 118174.</li>
</ul>

<p><br /></p>

<h1 id="joint-image-quality-assessment-and-brain-extraction-of-fetal-mri-using-deep-learning-2020">Joint Image Quality Assessment and Brain Extraction of Fetal MRI Using Deep Learning (2020)</h1>

<h2 id="background">Background</h2>

<ul>
  <li>Quality Assessment (QA): Evaluates MRI image quality for analysis suitability.</li>
  <li>Brain Extraction (BE): Identifies and isolates the brain region from the MRI image.</li>
</ul>

<p>Traditionally handled separately, this paper proposes a joint deep learning model for simultaneous QA and BE, enhancing performance and efficiency, since both tasks focus on the brain region. Besides, dealing with fetal brain images are difficult, in that they can appear in different positions and angles within the MRI scans. And their shapes and appearances change as fetuses grow. To solve this difficulty, the study leverages deformable convolution method.</p>

<h2 id="main-contributions">Main Contributions</h2>

<ol>
  <li>
    <p>Joint Optimization: Combining QA and BE, allowing the network to learn shared features and reducing the risk of overfitting.</p>
  </li>
  <li>
    <p>Multi-Stage Deep Learning Model:</p>

    <ul>
      <li>Brain Detector: Locates the brain region within the MRI scan. This helps in focusing the subsequent analysis on the relevant part of the image.</li>
      <li>Deformable Convolution: Adapts the receptive field to the varying shapes and sizes of fetal brains. This is crucial because fetal brain shapes change significantly across different gestational ages.</li>
      <li>Task-Specific Module: Simultaneously performs QA and BE.</li>
    </ul>
  </li>
  <li>
    <p>Multi-Step Training Strategy: Progressive training enhances model learning.</p>
  </li>
</ol>

<h2 id="evaluation">Evaluation</h2>

<ul>
  <li>Datasets: Fetal MRI images, focusing on 2D slice quality.</li>
  <li>Metrics:
    <ul>
      <li>Dice Similarity Coefficient (DSC): The primary metric for evaluating the accuracy of brain extraction, measuring the overlap between the predicted and true brain regions.</li>
      <li>Quality Scores: For image quality assessment, the model was trained to classify images into different quality levels.</li>
    </ul>
  </li>
  <li>Results:
    <ul>
      <li>The model achieved a DSC score of 0.89, which is comparable to or better than existing methods, indicating high accuracy in brain extraction.</li>
      <li>The image quality assessment module successfully classified image slices into quality categories, with 85% accuracy in distinguishing between high and low-quality images.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>The study introduces a DL model for simultaneous QA and BE in fetal MRI scans, using deformable convolutions to handle variability in brain images. The multi-step training and diverse dataset validation demonstrate its effectiveness, making it a promising tool for fetal MRI analysis.</p>

<p><br /></p>

<h1 id="prostate-imaging-quality-pi-qual-a-new-quality-control-scoring-system-for-multiparametric-magnetic-resonance-imaging-of-the-prostate-from-the-precision-trial-2020">Prostate Imaging Quality (PI-QUAL): A New Quality Control Scoring System for Multiparametric Magnetic Resonance Imaging of the Prostate from the PRECISION trial (2020)</h1>

<h2 id="background-1">Background</h2>

<p>The PRECISION trial was a multicenter randomized study that demonstrated multiparametric magnetic resonance imaging (mpMRI)-targeted biopsy is superior to standard transrectal ultrasound-guided biopsy for detecting prostate cancer. The success of mpMRI-targeted biopsies relies heavily on the quality of the mpMRI scans, but there was no existing scoring system to evaluate this quality.</p>

<h2 id="prostate-imaging-quality-pi-qual">Prostate Imaging Quality (PI-QUAL)</h2>

<p>To address this gap, the researchers introduced a novel scoring system called the Prostate Imaging Quality (PI-QUAL) score. PI-QUAL is a Likert scale from 1 to 5:</p>

<ul>
  <li>1: No mpMRI sequences are of diagnostic quality.</li>
  <li>5: Each sequence is independently of optimal diagnostic quality.</li>
</ul>

<h2 id="method">Method</h2>

<ol>
  <li>Selection of MRI scans: From the PRECISION trial, 58 out of 252 mpMRI scans (23%) were randomly selected for evaluation. These scans were taken from 22 different centers involved in the trial.</li>
  <li>Radiologist assessment: Two experienced radiologists from the coordinating trial center independently assessed the selected MRI scans. The radiologists were blinded to the pathology results to ensure unbiased evaluation.</li>
  <li>Scoring system: The scans were scored using the newly developed PI-QUAL system.</li>
  <li>Quality Metrics
    <ul>
      <li>Overall quality: The overall diagnostic quality of the scans was evaluated.</li>
      <li>Specific sequence quality: The quality of individual sequences such as T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and dynamic contrast-enhanced imaging (DCE) was assessed separately.</li>
    </ul>
  </li>
  <li>Statistical Analysis
    <ul>
      <li>The percentage of scans with sufficient diagnostic quality (PI-QUAL score ≥3) was calculated.</li>
      <li>The percentage of scans with good or optimal diagnostic quality (PI-QUAL score ≥4) was determined.</li>
      <li>The diagnostic quality of the specific imaging sequences (T2WI, DWI, DCE) was also analyzed.</li>
    </ul>
  </li>
</ol>

<h2 id="results">Results</h2>

<ul>
  <li>Overall Diagnostic Quality:
    <ul>
      <li>55 out of 58 scans (95%) had sufficient diagnostic quality (PI-QUAL score ≥3).</li>
      <li>35 out of 58 scans (60%) had good or optimal diagnostic quality (PI-QUAL score ≥4).</li>
    </ul>
  </li>
  <li>Sequence-Specific Quality: 95% of T2WI scans, 79% of DWI scans, and 66% of DCE scans were of diagnostic quality.</li>
</ul>

<h2 id="conclusion-1">Conclusion</h2>

<p>The introduction of the PI-QUAL score provides a standardized method to assess the quality of mpMRI scans. Further validation of this scoring system is recommended to ensure its effectiveness in various clinical settings.</p>

<p><br /></p>

<h1 id="automated-image-quality-evaluation-of-t2-weighted-liver-mri-utilizing-deep-learning-architecture-2018">Automated image quality evaluation of T2-weighted liver MRI utilizing deep learning architecture (2018)</h1>

<h2 id="background-2">Background</h2>

<p>Accurate screening of T2-weighted (T2WI) liver MRI scans is essential for effective diagnosis, but manual evaluation by radiologists is time-consuming and subject to variability. Automated methods, specifically using deep learning (DL) approaches like Convolutional Neural Networks (CNNs), offer a promising solution for consistent and efficient image quality assessment. This study aimed to develop and evaluate a CNN for automated screening to identify non-diagnostic images and compare its performance to radiologists’ evaluations.</p>

<h2 id="method-1">Method</h2>

<ul>
  <li>Data Collection: The study utilized 522 liver MRI exams performed at 1.5T and 3T between November 2014 and May 2016 for training and validation of the CNN.</li>
  <li>CNN Architecture: The CNN consisted of several layers, including an input layer, convolutional layer, fully connected layer, and output layer.</li>
  <li>Training Data: 351 T2WI images were anonymized and labeled as diagnostic or non-diagnostic based on their ability to detect lesions and assess liver morphology. These were used to train CNN.</li>
  <li>Validation Data: An independent set of 171 T2WI images was used for blind testing. Two radiologists independently evaluated these images, labeling them as diagnostic or non-diagnostic.</li>
  <li>Comparison: The image quality (IQ) output from the CNN was compared to the evaluations made by the two radiologists.</li>
</ul>

<h2 id="results-1">Results</h2>

<ul>
  <li>The agreement between the CNN and the radiologists was: 79% with Reader 1, and 73% with Reader 2</li>
  <li>Sensitivity and Specificity of the CNN in identifying non-diagnostic images:
    <ul>
      <li>Sensitivity: 67% with respect to Reader 1 and 47% with respect to Reader 2</li>
      <li>Specificity: 81% with respect to Reader 1 and 80% with respect to Reader 2</li>
    </ul>
  </li>
  <li>Negative predictive value: 94% with respect to Reader 1 and 86% with respect to Reader 2</li>
</ul>

<h2 id="conclusion-2">Conclusion</h2>

<p>This research shows the potential of using deep learning, specifically a CNN, for automated quality evaluation of T2-weighted liver MRI images. The CNN’s performance was compared to radiologists’ assessments, showing a high negative predictive value, which indicates its reliability in identifying diagnostic images. This automated approach could be assist radiologists in clinical settings by quickly and accurately determining the quality of MRI scans.</p>

<p><br /></p>

<h1 id="quality-control-strategies-for-brain-mri-segmentation-and-parcellation-practical-approaches-and-recommendations---insights-from-the-maastricht-study-2021">Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations - insights from the Maastricht study (2021)</h1>

<h2 id="background-3">Background</h2>

<p>Quality control (QC) in brain MRI segmentation is crucial for ensuring accurate data. Manual QC, although considered the gold standard, is impractical for large datasets due to its time-consuming nature. Automated methods offer faster and reproducible alternatives but lack a consensus on the best approach. This study aims to highlight the impact of manual edits on brain segmentation accuracy and compare various QC strategies to reduce measurement errors effectively.</p>

<h2 id="method-2">Method</h2>

<ul>
  <li>Data: Structural brain MRI from 259 participants of The Maastricht Study.</li>
  <li>Segmentation Tool: FreeSurfer 6.0 was used to automatically extract morphological estimates.</li>
  <li>Manual Editing: Segmentations with inaccuracies were manually edited, and the differences in morphological estimates before and after editing were compared.</li>
  <li>Quality Control Strategies:
    <ul>
      <li>Manual Strategies: Visual inspection to exclude or manually edit images.</li>
      <li>Automated strategies: Exclusion of outliers using MRIQC and Qoala-T, and metrics such as morphological global measures, Euler numbers, and Contrast-to-Noise ratio.</li>
      <li>Semi-Automated Strategies: Visual inspection and manual editing of outliers detected by tools and metrics without excluding them.</li>
    </ul>
  </li>
  <li>Evaluation: Measuring the proportion of unexplained variance relative to the total variance after applying each strategy.</li>
</ul>

<h2 id="results-2">Results</h2>

<ul>
  <li>Manual Editing: Significant changes in subcortical brain volumes and moderate changes in cortical surface area, thickness, and hippocampal volumes.</li>
  <li>Strategy Performance: Depended on the morphological measure of interest.
    <ul>
      <li>Manual Strategies: Provided the largest reduction in unexplained variance.</li>
      <li>Automated Alternatives: Based on Euler numbers and MRIQC scores.</li>
      <li>Global Morphological Measures: Excluding outliers increased unexplained variance.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion-3">Conclusion</h2>

<p>The study underscores the importance of QC in brain MRI segmentation, advocating for manual methods as the most reliable, though impractical for large datasets. Automated methods, especially those using Euler numbers and MRIQC, provide effective alternatives. Excluding outliers based on global measures may increase errors, guiding practical QC recommendations for neuroimaging research to ensure data accuracy and reliability.</p>

<p><br /></p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/study/2024/05/28/mriqc_report/">
        [MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)
      </a>
    </h1>
    <!--<span class="post-date">28 May 2024</span>-->
    <p class="post-date">28 May 2024&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="bio">
              <a href="https://alatteaday.github.io/tags/?tag=bio">
                #bio
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="brainImaging">
              <a href="https://alatteaday.github.io/tags/?tag=brainImaging">
                #brainImaging
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <style>
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: -0.5em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
p.b{
   margin-top: 1em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
</style>

<h1 id="mriqc-results">MRIQC Results</h1>

<p class="b" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex1.png?raw=true" alt="ex1" style="width: 32%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex2.png?raw=true" alt="ex2" style="width: 32%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex3.png?raw=true" alt="ex3" style="width: 32%;" />
</p>

<p>Using MRIQC to analyze magnetic resonance imaging (MRI) images yields a report in HTML format. The report is divided into two main sections:</p>

<ol>
  <li><strong>Basic visual report</strong>: View of the background of the anatomical image, Zoomed-in mosaic view of the brain</li>
  <li><strong>About</strong>: Errors, Reproducibility and provenance information</li>
</ol>

<p><br /></p>

<h2 id="view-of-the-background-of-the-anatomical-image">View of the background of the anatomical image</h2>

<p>The extent of artifacts in the background surrounding the brain region on MRI scans is visualized. Here, the background outside the brain is referred to as air. Typically, there is no signal in the air surrounding the head. Any signal detected in this air mask can be considered noise or unusual patterns, known as artifacts, generated during the imaging process. Let’s compare an MRIQC report of a well-acquired <a href="http://localhost:4000/ko/study/2023/12/26/mri2/">T1 weighted image (T1WI)</a> with that of a T1WI with artificially added noise. The noise was introduced using the torchio library to create a <a href="https://mriquestions.com/ghosting.html">ghosting effect</a>.</p>

<p style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_normal1.png?raw=true" alt="mosaic_bg_normal1" style="width: 49%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_normal2.png?raw=true" alt="mosaic_bg_normal2" style="width: 49%;" />
</p>

<p class="a" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_abnormal1.png?raw=true" alt="mosaic_bg_abnormal1" style="width: 49%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_abnormal2.png?raw=true" alt="mosaic_bg_abnormal2" style="width: 49%;" />
</p>
<p>The top result is from the well-acquired image (1), and the bottom is from the noise-added image (2). Signal intensity within the slices is indicated by brightness; the stronger the signal, the darker the color. In the first image, the head mask is generally dark, and the air mask is bright, making a clear distinction. In contrast, the second image shows less difference in brightness between the head and air masks, and some head regions appear weaker than the air. A closer look reveals wave-like patterns, indicating the artificially induced ghosting effect. Through this background artifact check, it is possible to qualitatively assess whether the brain region was well-captured without noise interference, ensuring the background is excluded.</p>

<p><br /></p>

<h2 id="zoomed-in-mosaic-view-of-brain">Zoomed-in mosaic view of brain</h2>

<p>The MRI slices are arranged in order and displayed in a mosaic view. To examine the brain area in detail, the background is mostly excluded, and the images are zoomed in to fit the size of the head mask. Using the mosaic view, we can assess the quality by checking for head motion during the MRI scan, uniformity of image intensity (intensity inhomogeneities), and the presence of global or local noise. Let’s compare the MRIQC report results of the two images used earlier.</p>

<p style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_normal1.png?raw=true" alt="mosaic_bg_normal1" style="width: 48.5%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_normal2.png?raw=true" alt="mosaic_bg_normal2" style="width: 50.5%;" />
</p>

<p class="a" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_abnormal1.png?raw=true" alt="mosaic_bg_abnormal1" style="width: 48.5%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_abnormal2.png?raw=true" alt="mosaic_bg_abnormal2" style="width: 50.5%;" />
</p>
<p>The top result is from image 1, and the bottom is from image 2. Overall, image 1 appears sharper based on the image quality and the distinction between different structures. Regarding head motion, neither image shows significant related issues when reviewing all slices in the mosaic view. However, in image 2, the artificially added ghosting noise is observed within the slices. Wave patterns within the head mask degrade the image quality. By directly examining the images through the mosaic view, we can identify and assess such issues.</p>

<p><br /></p>

<h2 id="reproducibility-and-provenance-information">Reproducibility and provenance information</h2>

<p>To ensure the reproducibility and transparency of the MRIQC report results, provenance information related to quality checks is provided.</p>

<h3 id="provenance-information">Provenance Information</h3>

<p>Provenance and reproducibility metadata are provided. This includes information such as the analysis environment (Execution environment), the path of the data used (Input filename), the versions of the packages used (Versions), and the MD5 checksum for file integrity verification (MD5sum).</p>

<p class="b" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/prov_info.png?raw=true" alt="prov_info" style="width: 100%;" />
</p>
<ul>
  <li><strong>Execution environment</strong>: The analysis environment. Here, it means that the execution was done in a ‘singularity’ container environment.</li>
  <li><strong>Input filename</strong>: The path of the data used.</li>
  <li><strong>Versions</strong>: The versions of the packages used, such as MRIQC, NiPype, and TemplateFlow.</li>
  <li><strong>MD5sum</strong>: The MD5 checksum for verifying the integrity of the input file.</li>
  <li><strong>Warnings</strong>: ‘large_rot_frame’ indicates whether there were large rotation frames in the image, and ‘small_air_mask’ indicates whether there were small air masks. Both factors can affect the accuracy of image analysis.</li>
</ul>

<h3 id="dataset-information">Dataset Information</h3>

<p>Metadata related to the data used in the analysis is provided.</p>

<p class="b" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/data_info.png?raw=true" alt="data_info" style="width: 100%;" />
</p>
<ul>
  <li><strong>AcquisitionMatrixPE</strong>: The size of the matrix in the encoding direction. In this example, it is 256 x 256.</li>
  <li><strong>AcquisitionTime</strong>: The time the image scan was performed.</li>
  <li><strong>ConversionSoftware</strong>: The software used to convert DICOM to NIfTI. Here, ‘dcm2niix’ was used.</li>
  <li><strong>ConversionSoftwareVersion</strong>: The version of the above conversion software.</li>
  <li><strong>HeudiconvVersion</strong>: The version of Heudiconv used to convert files to BIDS format.</li>
  <li><strong>ImageOrientationPatientDICOM</strong>: Vector information related to the orientation of the patient’s body.</li>
  <li><strong>ImageType</strong>: The type of image, which here means it is a ‘derivative’ image.</li>
  <li><strong>InstitutionName</strong>: The name of the institution where the data originated.</li>
  <li><strong>Modality</strong>: The imaging method. Here, ‘Magnetic Resonance (MR)’ imaging was used.</li>
  <li><strong>ProtocolName</strong>: The name of the protocol used.</li>
  <li><strong>RawImage</strong>: Indicates whether it is a raw image or not.</li>
  <li><strong>ReconMatrixPE</strong>: The size of the reconstructed matrix in the encoding direction. Here, it is 256 x 256.</li>
  <li><strong>ScanningSequence</strong>: The scanning sequence used.</li>
  <li><strong>SeriesNumber</strong>: The series number, used to identify the series to which the dataset belongs.</li>
  <li><strong>SliceThickness</strong>: The thickness of the slices.</li>
  <li><strong>SpacingBetweenSlice</strong>: The spacing between each slice.</li>
</ul>

<h3 id="image-quality-metrics">Image Quality Metrics</h3>

<p>Various Image Quality Metrics (IQMs) scores are reported to quantitatively evaluate the image quality. The metric items vary depending on the image modality.</p>

<ul>
  <li><strong>IQMs for structural images</strong>: Such as T1WI, T2WI, etc.</li>
  <li><strong>IQMs for functional images</strong>: Such as fMRI-related images, etc.</li>
  <li><strong>IQMs for diffusion images</strong>: Such as DWI, etc.</li>
</ul>

<p>IQM score results can also be found in the JSON files generated in the MRIQC output directory for each image.</p>

<p class="b" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/iqm.png?raw=true" alt="iqm" style="width: 100%;" />
</p>
<p><br /></p>

<h1 id="iqms-for-structural-images">IQMs for Structural Images</h1>

<p>In this example, let’s explore IQMs for structural images, considering the use of T1-weighted imaging (T1WI).</p>

<h2 id="measures-based-on-noise-measurements">Measures based on noise measurements</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">cjv</code> <span style="background-color:#FFEFD5">Coefficient of joint variation (CJV)</span>
    <ul>
      <li>A measure of relative variation considering two or more variables simultaneously, indicating how much variation of several variables is compared to their mean.</li>
      <li>It is useful when dealing with datasets that include multiple variables and helps understand overall variability</li>
      <li>It is calculated as the ratio of the standard deviation of multiple variables to their mean:</li>
    </ul>

\[CJV={(Standard \ Deviation \ of \ Combined \ Variables)\over(Mean \ of \ Combined \ Variables)}\times100\%\]

    <ul>
      <li>MRIQC calculates CJV between gray matter (GM) and white matter (WM) of the brain. The CJV of GM and WM serves as the objective function for optimizing the Intensity Non-Uniformity (INU) correction algorithm, as proposed by <a href="https://www.frontiersin.org/articles/10.3389/fninf.2016.00010/full">Ganzetti et al.</a>.
        <ul>
          <li>INU refers to the unevenness in brightness observed across different regions in MRI, often caused by non-uniformity in the magnetic field, especially by variations in radiofrequency (RF) transmission intensity.</li>
          <li>INU can degrade image accuracy, making interpretation difficult, hence it’s advisable to correct INU for improving MRI quality.</li>
        </ul>
      </li>
      <li>A higher CJV implies stronger head motion or larger INU defects, indicating poorer image quality. Therefore, lower CJV values are indicative of better image quality.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">snr</code> <span style="background-color:#FFEFD5">Signal-to-noise ratio (SNR)</span>
    <ul>
      <li>A measure of the relationship between the strength of the measured signal and the level of surrounding noise, indicating the quality and accuracy of the measured signal. Signal represents the signal observed in the tissue of interest, while noise refers to signals arising from patient motion or electronic interference, among others. SNR is used to distinguish between the two.</li>
      <li>A higher SNR indicates that the signal of interest is larger compared to the noise, signifying better data quality.</li>
    </ul>

\[SNR={Signal \ Strength\over Stnadard \ Deviation \ of \ Noise}\]
  </li>
  <li><code class="language-plaintext highlighter-rouge">snrd</code> <span style="background-color:#FFEFD5">Dietrich’s SNR (SNRd)</span>
    <ul>
      <li>Calculates SNR with reference to the surrounding air background in MRI, serving as a vital metric for assessing MRI quality. <a href="https://onlinelibrary.wiley.com/doi/10.1002/jmri.20969">Proposed by Dietrich et al.</a>.</li>
      <li>Since air typically exhibits uniform signal, referencing it allows for a more precise differentiation between signal and noise, thereby enhancing diagnostic accuracy.</li>
    </ul>

\[SNRd={Signal \ Strength\over Stnadard \ Deviation \ of \ Air Background}\]
  </li>
  <li><code class="language-plaintext highlighter-rouge">cnr</code> <span style="background-color:#FFEFD5">Contrast-to-noise ratio (CNR)</span>
    <ul>
      <li>Extends the concept of SNR, representing the relationship between contrast and noise levels in an image. Contrast refers to the brightness difference between structures or objects in an image, while noise refers to irregular or random signals.</li>
      <li>A Higher CNR indicates lower noise when achieving the desired image contrast, signifying a clearer representation of objects or structures with minimal noise.  This facilitates easier interpretation and improves image quality.</li>
      <li>MRIQC employs CNR to evaluate how well GM and WM are delineated and how easily the image can be interpreted.</li>
    </ul>

\[CNR={|\mu_{GM}-\mu_{WM}|\over \sqrt{\sigma^2_{GM}+\sigma^2_{wM}}}\]
  </li>
  <li><code class="language-plaintext highlighter-rouge">qi_2</code> <span style="background-color:#FFEFD5">Mortamet’s Quality index 2 (QI2)</span>
    <ul>
      <li>Evaluates the appropriateness of data distribution within the air mask after the removal of artificial intensities. The suitability of data distribution within the air mask region can affect the reliability of image processing and interpretation.</li>
      <li>Lower values indicate better quality.</li>
    </ul>
  </li>
</ul>

<h2 id="measures-based-on-information-theory">Measures based on information theory</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">efc</code> <span style="background-color:#FFEFD5">Entropy-focus criterion (EFC)</span>
    <ul>
      <li>Uses the Shannon entropy of voxel intensities to measure ghosting and blurring caused by head movements. <a href="https://ieeexplore.ieee.org/document/650886">Proposed by Atkinson et al.</a>.</li>
      <li>As ghosting and blurring increase, voxels lose information, causing the Shannon entropy of the voxels to increase. Thus, EFC has higher values with more ghosting and blurring, meaning that lower values indicate better image quality.</li>
      <li>The formula is normalized by maximum entropy, allowing comparison across images of different dimensions. $p_i$ represents the probability of each voxel, and $N$ represents the number of pixels.</li>
    </ul>

\[EFC={-\sum^N_i=1 p_i\log_2(p_i) \over \log_2(N)}\]
  </li>
  <li><code class="language-plaintext highlighter-rouge">fber</code> <span style="background-color:#FFEFD5">Fraction of brain explained by resting-state data (FBER)</span>
    <ul>
      <li>Compares the mean energy of brain tissue within the image to the mean air value outside the brain, measuring how much brain tissue is included in the image to assess image quality. <a href="">Proposed by Shehzad et al.</a></li>
      <li>It is one of the Quality Assurance Protocol (QAP) metrics.</li>
    </ul>

\[FBER ={Mean \ energy \ of image \ value \ within \ the \ head \over Mean \ energy \ of image \ value \ outside \ the \ head}\]
  </li>
</ul>

<h2 id="measures-targeting-specific-artifacts">Measures targeting specific artifacts</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">inu</code> : <span style="background-color:#FFEFD5">Summary statistics of the INU bias field extracted by N4ITK (max, min, median)</span>
    <ul>
      <li>The <a href="https://ieeexplore.ieee.org/document/5445030">N4ITK</a> algorithm is an advanced technique that improves MRI image quality by correcting RF field inhomogeneity.</li>
      <li>The INU field, or bias field, refers to the field filtered through N4ITK. The quality of an image can be assessed through the statistics of the INU field. Values closer to 0 indicate greater RF field inhomogeneity, while values closer to 1 indicate better correction and higher quality images.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">qi_1</code> <span style="background-color:#FFEFD5">Mortamet’s Quality index 1 (QI1)</span>
    <ul>
      <li>An index used to detect artificial intensities on air masks. It is used to properly analyze air masks by removing artificial intensities.</li>
      <li>It is generally considered an important metric in preprocessing stages of image data, such as MRI, to enhance image quality.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">wm2max</code> <span style="background-color:#FFEFD5">White-matter to maximum intensity ratio</span>
    <ul>
      <li>The ratio of the median intensity within the WM to the 95th percentile of the overall intensity distribution. This measures the proportion of significant intensities within the WM region.</li>
      <li>This ratio can reveal when the tail of the intensity distribution is extended, which often occurs due to the intensities from arterial blood vessels or fatty tissue.</li>
      <li>If the ratio falls outside the range of 0.6 to 0.8, the WM region of the image is considered non-uniform, indicating lower quality.</li>
    </ul>
  </li>
</ul>

<h2 id="other-measures">Other measures</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">fwhm</code> <span style="background-color:#FFEFD5">Full width ad half maximum (FWHM)</span>
    <ul>
      <li>Represents the full width at half maximum of the intensity values’ spatial distribution in an image, used to measure the image’s resolution and sharpness.</li>
      <li>Determined by the full width value at half the maximum point of the spatial distribution.</li>
      <li>Lower FWHM values indicate sharper, higher-resolution images.</li>
      <li>In MRIQC, FWHM is calculated using the Gaussian width estimator filter implemented in AFNI’s 3dWHMx.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">icvs_*</code> <span style="background-color:#FFEFD5">Intracranial volume scaling (ICVS)</span>
    <ul>
      <li>Intracranial volume (ICV) refers to the total volume of fluid within the cranial membrane surrounding the brain and intracranial fluid. ICVS represents the relative proportion of a specific tissue based on ICV in MRI.</li>
      <li>In MRIQC, the <code class="language-plaintext highlighter-rouge">volume_fraction()</code> function is used to calculate the ICVS for cerebrospinal fluid (CSF), GM, and WM</li>
      <li>The state of the brain can be assessed by determining whether each ICVS fluctuates within the normal range and whether they maintain ideal ratios to one another.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">summary_*_*</code>
    <ul>
      <li>MRIQC’s <code class="language-plaintext highlighter-rouge">summary_stats()</code> function provides various statistics related to the pixel distribution in the background, CSF, GM, and WM regions of an MRI. These statistics can be used to evaluate image quality.</li>
      <li>Includes mean, median, median absolute deviation (MAD), standard deviation, kurtosis, 5th percentile, 95th percentile, and number of voxels.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">tpm</code> <span style="background-color:#FFEFD5">Tissue probability map (TPM)</span>
    <ul>
      <li>Refers to the probability distribution of brain tissue types (e.g., GM, WM). In MRIQC, it measures the overlap between the estimated TPM from the image and the map of the ICBM nonlinear-asymmetric 2009c template.</li>
      <li>ICBM nonlinear-asymmetric 2009c template: One of the standard brain maps provided by the International Consortium for Brain Mapping (ICBM).
        <blockquote>
          <p>A number of unbiased non-linear averages of the MNI152 database have been generated that combines the attractions of both high-spatial resolution and signal-to-noise while not being subject to the vagaries of any single brain (Fonov et al., 2011). … We present an unbiased standard magnetic resonance imaging template brain volume for normal population. These volumes were created using data from ICBM project.</p>

          <p>6 different templates are available: …</p>

          <p>ICBM 2009c Nonlinear Asymmetric template – 1×1x1mm template which includes T1w,T2w,PDw modalities, and tissue probabilities maps. Intensity inhomogeneity was performed using N3 version 1.11 Also included brain mask, eye mask and face mask.Sampling is different from 2009a template. … <a href="https://nist.mni.mcgill.ca/icbm-152-nonlinear-atlases-2009/">[Reference]</a></p>
        </blockquote>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="references">References</h1>

<ul>
  <li><a href="https://mriqc.readthedocs.io/en/latest/iqms/t1w.html#ganzetti2016">MRIQC’s Documentation - IQMs for Structural Images</a></li>
</ul>

<p><br /></p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/">
        [MRIQC 3-1] Opening an HTML file using Flask
      </a>
    </h1>
    <!--<span class="post-date">21 May 2024</span>-->
    <p class="post-date">21 May 2024&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="bio">
              <a href="https://alatteaday.github.io/tags/?tag=bio">
                #bio
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="brainImaging">
              <a href="https://alatteaday.github.io/tags/?tag=brainImaging">
                #brainImaging
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <p>MRIQC analyzes and evaluates the quality of MRI images and outputs a report as an HTML file. To view the HTML file, I used Flask. Here is a summary of the method I used.</p>

<h1 id="flask">Flask</h1>

<p>Flask is a micro web framework written in Python. With its lightweight and flexible structure, it helps you quickly develop simple web applications and API servers. Since it includes only basic features, it is highly extensible, allowing you to add various plugins and extension modules as needed. It also has an easy-to-learn and intuitive code structure, making it suitable for beginners. However, because it comes with minimal features, you need to use external libraries to add complex functionalities, and maintaining the project can become challenging as the project size grows.</p>

<h1 id="opening-an-html-file-using-flask">Opening an HTML file using Flask</h1>

<h2 id="installing-flask">Installing Flask</h2>

<p>You can install it via PyPI:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install Flask
</code></pre></div></div>

<h2 id="static-and-templates">‘static/’ and ‘templates/’</h2>

<p>Flask requires two folders, <code class="language-plaintext highlighter-rouge">static/</code> and <code class="language-plaintext highlighter-rouge">templates/</code>. <code class="language-plaintext highlighter-rouge">static/</code> stores static files such as images, CSS, and JavaScript that exist in or are applied to HTML files. <code class="language-plaintext highlighter-rouge">templates/</code> stores the HTML files to be rendered.</p>

<p>Let me explain the process of opening an MRIQC report as an example. After creating these two folders in your project folder, save the static files and the HTML file you want to open in each respective folder:</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/1.png?raw=true" style="zoom: 70%;" />
</p>

<p>If the file path stored in the HTML file under <code class="language-plaintext highlighter-rouge">static/</code> already exists, it will be updated to the new path. When opening the MRIQC report HTML file, you’ll find that image files are specified with relative paths. Since the images have been moved to the <code class="language-plaintext highlighter-rouge">static/</code> directory, the paths will be changed to absolute paths accordingly:</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/2.png?raw=true" style="zoom: 70%;" />
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/3.png?raw=true" style="zoom: 70%;" />
</p>

<h2 id="writing-execution-code">Writing execution code</h2>

<p>And then, write the code to render the HTML file. Here is the code I used in <code class="language-plaintext highlighter-rouge">main.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="o">*</span> 

<span class="n">app</span> <span class="o">=</span> <span class="nc">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>
<span class="nd">@app.route</span><span class="p">(</span><span class="s">"/"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">():</span>
    <span class="k">return</span> <span class="nf">render_template</span><span class="p">(</span><span class="s">"sub-001_ses-001_T1w.html"</span><span class="p">)</span>
<span class="n">app</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="s">"0.0.0.0"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">5001</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">app = Flask(__name__)</code>: Creates an instance of a Flask application. <code class="language-plaintext highlighter-rouge">__name__</code> refers to the name of the current module and is used by Flask to locate resources for the application.</li>
  <li><code class="language-plaintext highlighter-rouge">@app.route("/")</code>: A decorator that instructs Flask to call the test function for the root URL (/).</li>
  <li><code class="language-plaintext highlighter-rouge">test()</code>: The function that will be executed when the root URL is requested</li>
  <li><code class="language-plaintext highlighter-rouge">return render_template("HTML_FILE_NAME.html")</code>: Renders and returns the HTML file located in the <code class="language-plaintext highlighter-rouge">templates/</code> directory.</li>
  <li><code class="language-plaintext highlighter-rouge">app.run("0.0.0.0", port=5001)</code>: Runs the application on address 0.0.0.0 and port 5001.</li>
</ul>

<h2 id="result">Result</h2>

<p>When you visit the specified address, you will see that the HTML file is displayed correctly.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/4.png?raw=true" style="zoom: 70%;" />
</p>

<p><br /></p>

<h1 id="reference">Reference</h1>

<ul>
  <li><a href="https://flask.palletsprojects.com/en/3.0.x/">Flask’s Documentation</a></li>
  <li><a href="https://daeunnniii.tistory.com/103">https://daeunnniii.tistory.com/103</a></li>
</ul>

<p><br /></p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/study/2024/05/20/mriqc_run/">
        [MRIQC 3] Running MRIQC: A Step-by-Step Guide using nii2dcm, Heudiconv, and MRIQC
      </a>
    </h1>
    <!--<span class="post-date">20 May 2024</span>-->
    <p class="post-date">20 May 2024&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="bio">
              <a href="https://alatteaday.github.io/tags/?tag=bio">
                #bio
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="brainImaging">
              <a href="https://alatteaday.github.io/tags/?tag=brainImaging">
                #brainImaging
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <style>
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: -0.5em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
p.b{
   margin-top: 1em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
</style>

<p>MRIQC analyzes and evaluates the quality of the input MRI images and compiles the relevant information into a report. To use MRIQC, you need MRI images stored in the <a href="https://alatteaday.github.io/ko/study/2024/05/20/bids/">BIDS</a> format. In this post, I will detail the process of running MRIQC and obtaining analysis results using DICOM files.</p>

<p><br /></p>

<h1 id="nii2dcm">nii2dcm</h1>

<p>While I used DICOM files here, NIfTI is also a common MRI file format. If you are using NIfTI files, you can use a BIDS converter that supports NIfTI or convert the NIfTI files to DICOM and then use a DICOM-supported BIDS converter. Based on my personal experience, BIDS converters that support NIfTI did not work reliably (though this might have been due to my own mistakes). You can use the <a href="https://github.com/tomaroberts/nii2dcm"><code class="language-plaintext highlighter-rouge">nii2dcm</code> library</a> to convert NIfTI files to DICOM. Refer to the code below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nii2dcm NIFTI_FILE_DIR OUTPUT_DIR -d MR
</code></pre></div></div>
<ul>
  <li><code class="language-plaintext highlighter-rouge">NIFTI_FILE_DIR</code>: Path to the NIfTI file you want to convert</li>
  <li><code class="language-plaintext highlighter-rouge">OUTPUT_DIR</code>: Path where the converted DICOM files will be saved</li>
</ul>

<p><br /></p>

<h1 id="heudiconv">Heudiconv</h1>

<p>I used Heudiconv as the BIDS converter. I summarized the instructions by referring to the tutorial provided on the official page. Here’s how to use it:</p>

<h2 id="installing-heudiconv">Installing Heudiconv</h2>

<p>Install via PyPI:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install heudiconv
</code></pre></div></div>

<h2 id="adjusting-heuristicpy">Adjusting heuristic.py</h2>

<p>Write a code to define the rules for saving each image in the BIDS format. You can refer to or modify the <code class="language-plaintext highlighter-rouge">heuristic.py</code> file from the data repository provided in the tutorial. This file determines the modality of the input image files and creates file paths that conform to the BIDS format for each modality, saving the images accordingly. Modify the judgment criteria and save paths as necessary.</p>

<p>The function to refer to and modify is <code class="language-plaintext highlighter-rouge">infotodict()</code> in <code class="language-plaintext highlighter-rouge">heuristic.py</code>.</p>

<ul>
  <li>Identify the modality of the images to be used: T1WI, T2WI, DWI, etc.</li>
  <li>Delete or comment out the code related to unused modalities.</li>
  <li>Check the path format where the modality images will be saved and modify it if needed.</li>
  <li>Specify and modify the criteria (dimensions, current filename characteristics, etc.) in the conditional statements to distinguish each modality.</li>
</ul>

<p>The modified example code is as follows. For T1WI and DWI, the path where the images will be saved and the conditions to determine the image modality have been set.</p>

<p align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/bids_ex1.png?raw=true" style="zoom: 90%;" />
</p>

<h2 id="running-heudiconv">Running Heudiconv</h2>

<p>After installation, set the parameters and run it as follows. Heudiconv can process multiple sets of subject data, i.e., multiple bundles of DICOM files, at once.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>heudiconv --files DICOM_FILE_DIRS -o OUTPUT_DIR -f HEURISTIC.PY -s SUB_ID -ss SES_ID -c dcm2niix -b minmeta --overwrite 
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">DICOM_FILE_DIRS</code>: Input the DICOM files for multiple subjects in a globbing format (e.g., dataset/sub-001/ses-001/<em>/</em>.dcm)</li>
  <li><code class="language-plaintext highlighter-rouge">OUTPUT_DIR</code>: Path where the converted BIDS format folder will be saved</li>
  <li><code class="language-plaintext highlighter-rouge">HEURISTIC.PY</code>: Path to the <code class="language-plaintext highlighter-rouge">heuristic.py</code> file created above</li>
  <li><code class="language-plaintext highlighter-rouge">SUB_ID</code>: Subject id (e.g. 001)</li>
  <li><code class="language-plaintext highlighter-rouge">SES_ID</code>: Session id (e.g. 001)</li>
</ul>

<p>Here is an example of how to run it. Enter the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>heudiconv --files data/*/*.dcm -o bids/data/ -f heuristic.py -s 0 -ss 0 -c dcm2niix -b minmeta --overwrite 
</code></pre></div></div>

<p>BIDS format folders will be created under <code class="language-plaintext highlighter-rouge">bids/data/</code> as follows:</p>

<p class="b" align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/bids_ex2.png?raw=true" style="zoom: 100%;" />
</p>

<p><br /></p>

<h1 id="mriqc">MRIQC</h1>

<p>Once the MRI images are stored in the BIDS format, they can be input into MRIQC. MRIQC can be used by downloading the package via PyPI or through a Docker container.</p>

<h2 id="with-pypi">With PyPI</h2>

<p>First, install it using the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m pip install -U mriqc
</code></pre></div></div>

<p>After installation, run the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mriqc BIDS_ROOT_DIR OUTPUT_DIR participant --participant-label SUB_ID
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">BIDS_ROOT_DIR</code>: Root path of the BIDS format folder</li>
  <li><code class="language-plaintext highlighter-rouge">OUTPUT_DIR</code>: Path where the MRIQC results will be saved</li>
  <li><code class="language-plaintext highlighter-rouge">participant OR group</code>: If set to <code class="language-plaintext highlighter-rouge">participant</code>, MRIQC analysis results will be obtained per subject; if set to <code class="language-plaintext highlighter-rouge">group</code>, MRIQC will analyze all images under the root path.</li>
  <li><code class="language-plaintext highlighter-rouge">SUB_ID</code>: In <code class="language-plaintext highlighter-rouge">participant</code> mode, specify the subject ID for analysis by entering it in <code class="language-plaintext highlighter-rouge">--participant-label</code>. Multiple IDs can be entered at once (e.g., <code class="language-plaintext highlighter-rouge">--participant-label 001 002 003</code>).</li>
</ul>

<h2 id="with-docker">With Docker</h2>

<p>I used MRIQC through Docker. The advantage of Docker containers is that they include all dependencies needed to run the program, ensuring a consistent environment. Enter the following code to run MRIQC at the <code class="language-plaintext highlighter-rouge">participant</code> level:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run -it --rm -v BIDS_ROOT_DIR:/data:ro -v OUTPUT_DIR:/out nipreps/mriqc:latest /data /out participant --participant_label SUB_ID [--verbose-reports]
</code></pre></div></div>

<p>Even if the <code class="language-plaintext highlighter-rouge">nipreps/mriqc</code> image is not downloaded, it will automatically download when you run the code.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">BIDS_ROOT_DIR</code>: Root path of the BIDS format folder. This is connected to the <code class="language-plaintext highlighter-rouge">/data</code> folder inside the container using the <code class="language-plaintext highlighter-rouge">-v</code> flag. The <code class="language-plaintext highlighter-rouge">ro</code> option stands for ‘read only’, meaning the path can only be read from the local path to the container path.</li>
  <li><code class="language-plaintext highlighter-rouge">OUTPUT_DIR</code>: Path where the MRIQC results will be saved. This is connected to the <code class="language-plaintext highlighter-rouge">/out</code> folder inside the container. If you copy the contents of the <code class="language-plaintext highlighter-rouge">/out</code> folder in the container to your local machine, you will see that the results are saved in the <code class="language-plaintext highlighter-rouge">OUTPUT_DIR</code>.
    <ul>
      <li>To copy the internal container files: When running the above <code class="language-plaintext highlighter-rouge">docker run</code> command, remove the <code class="language-plaintext highlighter-rouge">--rm</code> (remove container after completion) option. After completion, execute <code class="language-plaintext highlighter-rouge">docker cp CONTAINER_NAME:FILE_PATH LOCAL_PATH</code>.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">SUB_ID</code>: Subject ID. Multiple IDs can be entered.  (e.g. <code class="language-plaintext highlighter-rouge">--participant_label 001 002 003</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">--verbose-reports</code> (Optional): If this flag is included, four additional plots will be reported along with the default visual report plot.</li>
</ul>

<p>After running the above code, you can check the list of Docker images and containers to see the MRIQC-related items that have been executed.</p>

<p class="b" align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/docker_ex.png?raw=true" alt="docker_ex" style="zoom: 100%;" />
</p>

<p><br /></p>

<h1 id="mriqc-results">MRIQC Results</h1>

<p class="b" align="center">
   <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/mriqc_ex1_1.png?raw=true" alt="mriqc_ex1_1" style="zoom: 45%;" />
</p>

<p>When the MRIQC analysis is complete, the above-mentioned files will appear under the <code class="language-plaintext highlighter-rouge">OUTPUT_DIR</code>. Among these, the analysis results are contained in the plot image files within the <code class="language-plaintext highlighter-rouge">figures</code> folder, and the JSON and HTML files named after the respective files, such as <code class="language-plaintext highlighter-rouge">sub-0_ses-0_T1w.json</code> and <code class="language-plaintext highlighter-rouge">sub-0_ses-0_T1w.html</code> in this example. The results report is generated as an HTML file based on the plot images and JSON files.</p>

<p class="b" style="width: 100%;" align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex1.png?raw=true" alt="ex1" style="width: 32%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex2.png?raw=true" alt="ex2" style="width: 32%;" />
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex3.png?raw=true" alt="ex3" style="width: 32%;" />
</p>
<p>By <a href="">opening the HTML file</a>, you can view a report like the one above. By <a href="https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/">interpreting the report</a> using the visualized plots and quality metric scores, you can determine the quality of the images.</p>

<p><br /></p>

<h1 id="references">References</h1>

<ul>
  <li><a href="https://github.com/tomaroberts/nii2dcm">nii2dcm Github</a></li>
  <li><a href="https://heudiconv.readthedocs.io/en/latest/">Heudiconv’s Tutorial</a></li>
  <li><a href="https://mriqc.readthedocs.io/en/latest/">MRIQC’s Documentation</a></li>
</ul>

<p><br /></p>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://alatteaday.github.io/study/2024/05/20/bids/">
        [MRIQC 2] Brain Imaging Data Structure (BIDS)
      </a>
    </h1>
    <!--<span class="post-date">20 May 2024</span>-->
    <p class="post-date">20 May 2024&nbsp;&nbsp;&nbsp;&nbsp;
      
        
          
            <span class="tag" data-tag="bio">
              <a href="https://alatteaday.github.io/tags/?tag=bio">
                #bio
              </a>
            </span>
            
        
          
            <span class="tag" data-tag="brainImaging">
              <a href="https://alatteaday.github.io/tags/?tag=brainImaging">
                #brainImaging
              </a>
            </span>
            
        
      
    </p>
    <!--
    
    -->
    <h1 id="brain-imaging-data-structure-bids">Brain Imaging Data Structure (BIDS)</h1>

<p>The Brain Imaging Data Structure (BIDS) was created to streamline the organization and sharing of neuroimaging and behavioral data. The driving force behind BIDS is the need for a standardized format in neuroimaging research to prevent misunderstandings, eliminate the time spent on data reorganization, and improve reproducibility. By offering a straightforward and intuitive structure for data, BIDS aims to promote collaboration, speed up research, and make neuroimaging data more accessible to a diverse range of scientists.</p>

<p>BIDS provides detailed guidelines on how to format and name files, ensuring consistency across studies. It supports various neuroimaging modalities, including MRI, MEG, EEG, and iEEG, and is extensible, allowing for the integration of new data types and metadata. Additionally, BIDS is supported by a growing ecosystem of tools and software that facilitate data validation, analysis, and sharing, further enhancing its utility in the research community.</p>

<h1 id="bids-format">BIDS Format</h1>

<p>BIDS was inspired by the format used by the OpenfMRI repository, which is now known as OpenNeuro. The BIDS format is essentially a method for organizing data and metadata within a hierarchical folder structure. It makes minimal assumptions about the tools needed to interact with the data, allowing for flexibility and broad compatibility. This structure helps standardize data organization, facilitating easier data sharing, analysis, and collaboration within the neuroimaging research community.</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig1.png?raw=true" alt="fig1" style="zoom: 70%;" />
</p>

<h2 id="folders">Folders</h2>

<p>There are four levels of the folder hierarchy, and all sub-folders except for the root folder have a specific structure to their name. The format and the example names can be described as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Project/
└─ Subject (e.g. 'sub-01/')
  └─ Session (e.g. 'ses-01/')
    └─ Datatype (e.g. 'anat')
</code></pre></div></div>

<ul>
  <li>Project: contains all the dataset. can have any name.</li>
  <li>Subject: contains data of one subject. One folder per subject. A subject has a unique label.
    <ul>
      <li>Name format: <code class="language-plaintext highlighter-rouge">sub-PARTICIPANT LABEL</code></li>
    </ul>
  </li>
  <li>
    <p>Session: represents a recording session. Each subject may have multiple sessions if the data is gathered from several occasions.</p>

    <ul>
      <li>If there’s only a single session per subject, this level may be omitted.</li>
      <li>Name format: <code class="language-plaintext highlighter-rouge">ses-SESSION LABEL</code></li>
    </ul>
  </li>
  <li>
    <p>Datatype: represents different types of data.</p>

    <p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig2.png?raw=true" alt="fig2" style="zoom: 70%;" />
</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">anat</code>: anatomical MRI data</li>
      <li><code class="language-plaintext highlighter-rouge">func</code>: functional MRI data</li>
      <li><code class="language-plaintext highlighter-rouge">fmap</code>: fieldmap data</li>
      <li><code class="language-plaintext highlighter-rouge">dwi</code>: diffusion MRI data</li>
      <li><code class="language-plaintext highlighter-rouge">perf</code>: arterial spin labeling data</li>
      <li><code class="language-plaintext highlighter-rouge">eeg</code>: electroencephalography data</li>
      <li><code class="language-plaintext highlighter-rouge">meg</code>: magnetoencephalography data</li>
      <li><code class="language-plaintext highlighter-rouge">ieeg</code>: intracranial EEG data</li>
      <li><code class="language-plaintext highlighter-rouge">beh</code>: behavioral data</li>
      <li><code class="language-plaintext highlighter-rouge">pet</code>: positron emission tomography data</li>
      <li><code class="language-plaintext highlighter-rouge">micr</code>: microscopy data</li>
      <li><code class="language-plaintext highlighter-rouge">nirs</code>: near-infrared spectroscopy data</li>
      <li><code class="language-plaintext highlighter-rouge">motion</code>: motion capture data</li>
    </ul>
  </li>
</ul>

<h2 id="files">Files</h2>

<p>Three main types of files:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">.json</code> file: contains metadata</li>
  <li><code class="language-plaintext highlighter-rouge">.tsv</code> file: contains tables of metadata</li>
  <li>Raw data files: e.g. <code class="language-plaintext highlighter-rouge">.jpg</code>, <code class="language-plaintext highlighter-rouge">.nii.gz</code></li>
</ul>

<p>Standardized ways of naming files:</p>

<ul>
  <li>Do not include white spaces in file names</li>
  <li>Use only letters, numbers, hyphens, and underscores.</li>
  <li>Do not rely on letter case: Some operating systems regard <code class="language-plaintext highlighter-rouge">a</code> as the same as <code class="language-plaintext highlighter-rouge">A</code></li>
  <li>Use separators and case in a systematic and meaningful way.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">CamelCase</code> or <code class="language-plaintext highlighter-rouge">snake_case</code></li>
    </ul>
  </li>
</ul>

<p>Filename template</p>

<p align="center">
  <img src="https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig3.png?raw=true" alt="fig3" style="zoom: 70%;" />
</p>

<p>Example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dataset/
 └─ participants.json
 └─ participants.tsv
 └─ sub-01/
   └─ anat/
     └─ sub-01_t1w.nii.gz
     └─ sub-01_t1w.json
   └─ func/
     └─ sub-01_task-rest_bold.nii.gz
     └─ sub-01_task-rest_bold.json
   └─ dwi/
     └─ sub-01-task-rest_dwi.nii.gz
</code></pre></div></div>

<p><br /></p>

<h1 id="reference">Reference</h1>

<ul>
  <li><a href="https://bids.neuroimaging.io/">BIDS Official Website</a></li>
  <li><a href="https://bids-standard.github.io/bids-starter-kit/index.html">BIDS Starter Kit</a></li>
  <li><a href="https://github.com/bids-standard">BIDS Github</a></li>
</ul>

    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$$', '$$$'], ['\\[', '\\]'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    
  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="https://alatteaday.github.io/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>

        
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>
