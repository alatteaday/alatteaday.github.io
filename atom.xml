<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<!--site.title -->
 <title>Coffee Chat</title>
 <link href="https://alatteaday.github.io/atom.xml" rel="self"/>
 <link href="https://alatteaday.github.io/"/>
 <updated>2024-06-27T02:14:44-05:00</updated>
 <id>https://alatteaday.github.io</id>
 <author>
   <name>Jiyun</name>
   <email>jyuun.k@gmail.com</email>
 </author>

 
 <entry>
   <title>Summaries of papers on MRI Quality Assessment and Control</title>
   <link href="https://alatteaday.github.io/paper/2024/06/14/mriqcsurvey/"/>
   <updated>2024-06-14T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/paper/2024/06/14/mriqcsurvey</id>
   <content type="html">&lt;p&gt;I summarized four papers related to MRI quality assessment and control. Below are the summaries:&lt;/p&gt;

&lt;h1 id=&quot;paper-list&quot;&gt;Paper list&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Liao, Lufan, et al. “Joint image quality assessment and brain extraction of fetal MRI using deep learning.” &lt;em&gt;Medical Image Computing and Computer Assisted Intervention–MICCAI&lt;/em&gt; &lt;em&gt;2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part VI 23&lt;/em&gt;. Springer International Publishing, 2020.&lt;/li&gt;
  &lt;li&gt;Giganti, Francesco, et al. “Prostate Imaging Quality (PI-QUAL): a new quality control scoring system for multiparametric magnetic resonance imaging of the prostate from the PRECISION trial.” European urology oncology 3.5 (2020): 615-619.&lt;/li&gt;
  &lt;li&gt;Esses, Steven J., et al. “Automated image quality evaluation of T2‐weighted liver MRI utilizing deep learning architecture.” &lt;em&gt;Journal&lt;/em&gt; &lt;em&gt;of&lt;/em&gt; &lt;em&gt;Magnetic&lt;/em&gt; &lt;em&gt;Resonance&lt;/em&gt; &lt;em&gt;Imaging&lt;/em&gt; 47.3 (2018): 723-728.&lt;/li&gt;
  &lt;li&gt;Monereo-Sánchez, Jennifer, et al. “Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations-insights from the Maastricht study.” &lt;em&gt;Neuroimage&lt;/em&gt; 237 (2021): 118174.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;joint-image-quality-assessment-and-brain-extraction-of-fetal-mri-using-deep-learning-2020&quot;&gt;Joint Image Quality Assessment and Brain Extraction of Fetal MRI Using Deep Learning (2020)&lt;/h1&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Quality Assessment (QA): Evaluates MRI image quality for analysis suitability.&lt;/li&gt;
  &lt;li&gt;Brain Extraction (BE): Identifies and isolates the brain region from the MRI image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Traditionally handled separately, this paper proposes a joint deep learning model for simultaneous QA and BE, enhancing performance and efficiency, since both tasks focus on the brain region. Besides, dealing with fetal brain images are difficult, in that they can appear in different positions and angles within the MRI scans. And their shapes and appearances change as fetuses grow. To solve this difficulty, the study leverages deformable convolution method.&lt;/p&gt;

&lt;h2 id=&quot;main-contributions&quot;&gt;Main Contributions&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Joint Optimization: Combining QA and BE, allowing the network to learn shared features and reducing the risk of overfitting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-Stage Deep Learning Model:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Brain Detector: Locates the brain region within the MRI scan. This helps in focusing the subsequent analysis on the relevant part of the image.&lt;/li&gt;
      &lt;li&gt;Deformable Convolution: Adapts the receptive field to the varying shapes and sizes of fetal brains. This is crucial because fetal brain shapes change significantly across different gestational ages.&lt;/li&gt;
      &lt;li&gt;Task-Specific Module: Simultaneously performs QA and BE.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-Step Training Strategy: Progressive training enhances model learning.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Datasets: Fetal MRI images, focusing on 2D slice quality.&lt;/li&gt;
  &lt;li&gt;Metrics:
    &lt;ul&gt;
      &lt;li&gt;Dice Similarity Coefficient (DSC): The primary metric for evaluating the accuracy of brain extraction, measuring the overlap between the predicted and true brain regions.&lt;/li&gt;
      &lt;li&gt;Quality Scores: For image quality assessment, the model was trained to classify images into different quality levels.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Results:
    &lt;ul&gt;
      &lt;li&gt;The model achieved a DSC score of 0.89, which is comparable to or better than existing methods, indicating high accuracy in brain extraction.&lt;/li&gt;
      &lt;li&gt;The image quality assessment module successfully classified image slices into quality categories, with 85% accuracy in distinguishing between high and low-quality images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The study introduces a DL model for simultaneous QA and BE in fetal MRI scans, using deformable convolutions to handle variability in brain images. The multi-step training and diverse dataset validation demonstrate its effectiveness, making it a promising tool for fetal MRI analysis.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;prostate-imaging-quality-pi-qual-a-new-quality-control-scoring-system-for-multiparametric-magnetic-resonance-imaging-of-the-prostate-from-the-precision-trial-2020&quot;&gt;Prostate Imaging Quality (PI-QUAL): A New Quality Control Scoring System for Multiparametric Magnetic Resonance Imaging of the Prostate from the PRECISION trial (2020)&lt;/h1&gt;

&lt;h2 id=&quot;background-1&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The PRECISION trial was a multicenter randomized study that demonstrated multiparametric magnetic resonance imaging (mpMRI)-targeted biopsy is superior to standard transrectal ultrasound-guided biopsy for detecting prostate cancer. The success of mpMRI-targeted biopsies relies heavily on the quality of the mpMRI scans, but there was no existing scoring system to evaluate this quality.&lt;/p&gt;

&lt;h2 id=&quot;prostate-imaging-quality-pi-qual&quot;&gt;Prostate Imaging Quality (PI-QUAL)&lt;/h2&gt;

&lt;p&gt;To address this gap, the researchers introduced a novel scoring system called the Prostate Imaging Quality (PI-QUAL) score. PI-QUAL is a Likert scale from 1 to 5:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1: No mpMRI sequences are of diagnostic quality.&lt;/li&gt;
  &lt;li&gt;5: Each sequence is independently of optimal diagnostic quality.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Selection of MRI scans: From the PRECISION trial, 58 out of 252 mpMRI scans (23%) were randomly selected for evaluation. These scans were taken from 22 different centers involved in the trial.&lt;/li&gt;
  &lt;li&gt;Radiologist assessment: Two experienced radiologists from the coordinating trial center independently assessed the selected MRI scans. The radiologists were blinded to the pathology results to ensure unbiased evaluation.&lt;/li&gt;
  &lt;li&gt;Scoring system: The scans were scored using the newly developed PI-QUAL system.&lt;/li&gt;
  &lt;li&gt;Quality Metrics
    &lt;ul&gt;
      &lt;li&gt;Overall quality: The overall diagnostic quality of the scans was evaluated.&lt;/li&gt;
      &lt;li&gt;Specific sequence quality: The quality of individual sequences such as T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and dynamic contrast-enhanced imaging (DCE) was assessed separately.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Statistical Analysis
    &lt;ul&gt;
      &lt;li&gt;The percentage of scans with sufficient diagnostic quality (PI-QUAL score ≥3) was calculated.&lt;/li&gt;
      &lt;li&gt;The percentage of scans with good or optimal diagnostic quality (PI-QUAL score ≥4) was determined.&lt;/li&gt;
      &lt;li&gt;The diagnostic quality of the specific imaging sequences (T2WI, DWI, DCE) was also analyzed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Overall Diagnostic Quality:
    &lt;ul&gt;
      &lt;li&gt;55 out of 58 scans (95%) had sufficient diagnostic quality (PI-QUAL score ≥3).&lt;/li&gt;
      &lt;li&gt;35 out of 58 scans (60%) had good or optimal diagnostic quality (PI-QUAL score ≥4).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sequence-Specific Quality: 95% of T2WI scans, 79% of DWI scans, and 66% of DCE scans were of diagnostic quality.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-1&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The introduction of the PI-QUAL score provides a standardized method to assess the quality of mpMRI scans. Further validation of this scoring system is recommended to ensure its effectiveness in various clinical settings.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;automated-image-quality-evaluation-of-t2-weighted-liver-mri-utilizing-deep-learning-architecture-2018&quot;&gt;Automated image quality evaluation of T2-weighted liver MRI utilizing deep learning architecture (2018)&lt;/h1&gt;

&lt;h2 id=&quot;background-2&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Accurate screening of T2-weighted (T2WI) liver MRI scans is essential for effective diagnosis, but manual evaluation by radiologists is time-consuming and subject to variability. Automated methods, specifically using deep learning (DL) approaches like Convolutional Neural Networks (CNNs), offer a promising solution for consistent and efficient image quality assessment. This study aimed to develop and evaluate a CNN for automated screening to identify non-diagnostic images and compare its performance to radiologists’ evaluations.&lt;/p&gt;

&lt;h2 id=&quot;method-1&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data Collection: The study utilized 522 liver MRI exams performed at 1.5T and 3T between November 2014 and May 2016 for training and validation of the CNN.&lt;/li&gt;
  &lt;li&gt;CNN Architecture: The CNN consisted of several layers, including an input layer, convolutional layer, fully connected layer, and output layer.&lt;/li&gt;
  &lt;li&gt;Training Data: 351 T2WI images were anonymized and labeled as diagnostic or non-diagnostic based on their ability to detect lesions and assess liver morphology. These were used to train CNN.&lt;/li&gt;
  &lt;li&gt;Validation Data: An independent set of 171 T2WI images was used for blind testing. Two radiologists independently evaluated these images, labeling them as diagnostic or non-diagnostic.&lt;/li&gt;
  &lt;li&gt;Comparison: The image quality (IQ) output from the CNN was compared to the evaluations made by the two radiologists.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-1&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The agreement between the CNN and the radiologists was: 79% with Reader 1, and 73% with Reader 2&lt;/li&gt;
  &lt;li&gt;Sensitivity and Specificity of the CNN in identifying non-diagnostic images:
    &lt;ul&gt;
      &lt;li&gt;Sensitivity: 67% with respect to Reader 1 and 47% with respect to Reader 2&lt;/li&gt;
      &lt;li&gt;Specificity: 81% with respect to Reader 1 and 80% with respect to Reader 2&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Negative predictive value: 94% with respect to Reader 1 and 86% with respect to Reader 2&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-2&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This research shows the potential of using deep learning, specifically a CNN, for automated quality evaluation of T2-weighted liver MRI images. The CNN’s performance was compared to radiologists’ assessments, showing a high negative predictive value, which indicates its reliability in identifying diagnostic images. This automated approach could be assist radiologists in clinical settings by quickly and accurately determining the quality of MRI scans.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;quality-control-strategies-for-brain-mri-segmentation-and-parcellation-practical-approaches-and-recommendations---insights-from-the-maastricht-study-2021&quot;&gt;Quality control strategies for brain MRI segmentation and parcellation: Practical approaches and recommendations - insights from the Maastricht study (2021)&lt;/h1&gt;

&lt;h2 id=&quot;background-3&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Quality control (QC) in brain MRI segmentation is crucial for ensuring accurate data. Manual QC, although considered the gold standard, is impractical for large datasets due to its time-consuming nature. Automated methods offer faster and reproducible alternatives but lack a consensus on the best approach. This study aims to highlight the impact of manual edits on brain segmentation accuracy and compare various QC strategies to reduce measurement errors effectively.&lt;/p&gt;

&lt;h2 id=&quot;method-2&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data: Structural brain MRI from 259 participants of The Maastricht Study.&lt;/li&gt;
  &lt;li&gt;Segmentation Tool: FreeSurfer 6.0 was used to automatically extract morphological estimates.&lt;/li&gt;
  &lt;li&gt;Manual Editing: Segmentations with inaccuracies were manually edited, and the differences in morphological estimates before and after editing were compared.&lt;/li&gt;
  &lt;li&gt;Quality Control Strategies:
    &lt;ul&gt;
      &lt;li&gt;Manual Strategies: Visual inspection to exclude or manually edit images.&lt;/li&gt;
      &lt;li&gt;Automated strategies: Exclusion of outliers using MRIQC and Qoala-T, and metrics such as morphological global measures, Euler numbers, and Contrast-to-Noise ratio.&lt;/li&gt;
      &lt;li&gt;Semi-Automated Strategies: Visual inspection and manual editing of outliers detected by tools and metrics without excluding them.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation: Measuring the proportion of unexplained variance relative to the total variance after applying each strategy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-2&quot;&gt;Results&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Manual Editing: Significant changes in subcortical brain volumes and moderate changes in cortical surface area, thickness, and hippocampal volumes.&lt;/li&gt;
  &lt;li&gt;Strategy Performance: Depended on the morphological measure of interest.
    &lt;ul&gt;
      &lt;li&gt;Manual Strategies: Provided the largest reduction in unexplained variance.&lt;/li&gt;
      &lt;li&gt;Automated Alternatives: Based on Euler numbers and MRIQC scores.&lt;/li&gt;
      &lt;li&gt;Global Morphological Measures: Excluding outliers increased unexplained variance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion-3&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The study underscores the importance of QC in brain MRI segmentation, advocating for manual methods as the most reliable, though impractical for large datasets. Automated methods, especially those using Euler numbers and MRIQC, provide effective alternatives. Excluding outliers based on global measures may increase errors, guiding practical QC recommendations for neuroimaging research to ensure data accuracy and reliability.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 4] MRIQC Report and Image Quality Metrics (IQMs)</title>
   <link href="https://alatteaday.github.io/study/2024/05/28/mriqc_report/"/>
   <updated>2024-05-28T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/28/mriqc_report</id>
   <content type="html">&lt;style&gt;
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: -0.5em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
p.b{
   margin-top: 1em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
&lt;/style&gt;

&lt;h1 id=&quot;mriqc-results&quot;&gt;MRIQC Results&lt;/h1&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex1.png?raw=true&quot; alt=&quot;ex1&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex2.png?raw=true&quot; alt=&quot;ex2&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex3.png?raw=true&quot; alt=&quot;ex3&quot; style=&quot;width: 32%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Using MRIQC to analyze magnetic resonance imaging (MRI) images yields a report in HTML format. The report is divided into two main sections:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Basic visual report&lt;/strong&gt;: View of the background of the anatomical image, Zoomed-in mosaic view of the brain&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;About&lt;/strong&gt;: Errors, Reproducibility and provenance information&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;view-of-the-background-of-the-anatomical-image&quot;&gt;View of the background of the anatomical image&lt;/h2&gt;

&lt;p&gt;The extent of artifacts in the background surrounding the brain region on MRI scans is visualized. Here, the background outside the brain is referred to as air. Typically, there is no signal in the air surrounding the head. Any signal detected in this air mask can be considered noise or unusual patterns, known as artifacts, generated during the imaging process. Let’s compare an MRIQC report of a well-acquired &lt;a href=&quot;http://localhost:4000/ko/study/2023/12/26/mri2/&quot;&gt;T1 weighted image (T1WI)&lt;/a&gt; with that of a T1WI with artificially added noise. The noise was introduced using the torchio library to create a &lt;a href=&quot;https://mriquestions.com/ghosting.html&quot;&gt;ghosting effect&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_normal1.png?raw=true&quot; alt=&quot;mosaic_bg_normal1&quot; style=&quot;width: 49%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_normal2.png?raw=true&quot; alt=&quot;mosaic_bg_normal2&quot; style=&quot;width: 49%;&quot; /&gt;
&lt;/p&gt;

&lt;p class=&quot;a&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_abnormal1.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal1&quot; style=&quot;width: 49%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_bg_abnormal2.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal2&quot; style=&quot;width: 49%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;The top result is from the well-acquired image (1), and the bottom is from the noise-added image (2). Signal intensity within the slices is indicated by brightness; the stronger the signal, the darker the color. In the first image, the head mask is generally dark, and the air mask is bright, making a clear distinction. In contrast, the second image shows less difference in brightness between the head and air masks, and some head regions appear weaker than the air. A closer look reveals wave-like patterns, indicating the artificially induced ghosting effect. Through this background artifact check, it is possible to qualitatively assess whether the brain region was well-captured without noise interference, ensuring the background is excluded.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;zoomed-in-mosaic-view-of-brain&quot;&gt;Zoomed-in mosaic view of brain&lt;/h2&gt;

&lt;p&gt;The MRI slices are arranged in order and displayed in a mosaic view. To examine the brain area in detail, the background is mostly excluded, and the images are zoomed in to fit the size of the head mask. Using the mosaic view, we can assess the quality by checking for head motion during the MRI scan, uniformity of image intensity (intensity inhomogeneities), and the presence of global or local noise. Let’s compare the MRIQC report results of the two images used earlier.&lt;/p&gt;

&lt;p style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_normal1.png?raw=true&quot; alt=&quot;mosaic_bg_normal1&quot; style=&quot;width: 48.5%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_normal2.png?raw=true&quot; alt=&quot;mosaic_bg_normal2&quot; style=&quot;width: 50.5%;&quot; /&gt;
&lt;/p&gt;

&lt;p class=&quot;a&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_abnormal1.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal1&quot; style=&quot;width: 48.5%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/mosaic_zo_abnormal2.png?raw=true&quot; alt=&quot;mosaic_bg_abnormal2&quot; style=&quot;width: 50.5%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;The top result is from image 1, and the bottom is from image 2. Overall, image 1 appears sharper based on the image quality and the distinction between different structures. Regarding head motion, neither image shows significant related issues when reviewing all slices in the mosaic view. However, in image 2, the artificially added ghosting noise is observed within the slices. Wave patterns within the head mask degrade the image quality. By directly examining the images through the mosaic view, we can identify and assess such issues.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reproducibility-and-provenance-information&quot;&gt;Reproducibility and provenance information&lt;/h2&gt;

&lt;p&gt;To ensure the reproducibility and transparency of the MRIQC report results, provenance information related to quality checks is provided.&lt;/p&gt;

&lt;h3 id=&quot;provenance-information&quot;&gt;Provenance Information&lt;/h3&gt;

&lt;p&gt;Provenance and reproducibility metadata are provided. This includes information such as the analysis environment (Execution environment), the path of the data used (Input filename), the versions of the packages used (Versions), and the MD5 checksum for file integrity verification (MD5sum).&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/prov_info.png?raw=true&quot; alt=&quot;prov_info&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Execution environment&lt;/strong&gt;: The analysis environment. Here, it means that the execution was done in a ‘singularity’ container environment.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Input filename&lt;/strong&gt;: The path of the data used.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Versions&lt;/strong&gt;: The versions of the packages used, such as MRIQC, NiPype, and TemplateFlow.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MD5sum&lt;/strong&gt;: The MD5 checksum for verifying the integrity of the input file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Warnings&lt;/strong&gt;: ‘large_rot_frame’ indicates whether there were large rotation frames in the image, and ‘small_air_mask’ indicates whether there were small air masks. Both factors can affect the accuracy of image analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dataset-information&quot;&gt;Dataset Information&lt;/h3&gt;

&lt;p&gt;Metadata related to the data used in the analysis is provided.&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/data_info.png?raw=true&quot; alt=&quot;data_info&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AcquisitionMatrixPE&lt;/strong&gt;: The size of the matrix in the encoding direction. In this example, it is 256 x 256.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AcquisitionTime&lt;/strong&gt;: The time the image scan was performed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ConversionSoftware&lt;/strong&gt;: The software used to convert DICOM to NIfTI. Here, ‘dcm2niix’ was used.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ConversionSoftwareVersion&lt;/strong&gt;: The version of the above conversion software.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;HeudiconvVersion&lt;/strong&gt;: The version of Heudiconv used to convert files to BIDS format.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ImageOrientationPatientDICOM&lt;/strong&gt;: Vector information related to the orientation of the patient’s body.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ImageType&lt;/strong&gt;: The type of image, which here means it is a ‘derivative’ image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;InstitutionName&lt;/strong&gt;: The name of the institution where the data originated.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Modality&lt;/strong&gt;: The imaging method. Here, ‘Magnetic Resonance (MR)’ imaging was used.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ProtocolName&lt;/strong&gt;: The name of the protocol used.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RawImage&lt;/strong&gt;: Indicates whether it is a raw image or not.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ReconMatrixPE&lt;/strong&gt;: The size of the reconstructed matrix in the encoding direction. Here, it is 256 x 256.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ScanningSequence&lt;/strong&gt;: The scanning sequence used.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SeriesNumber&lt;/strong&gt;: The series number, used to identify the series to which the dataset belongs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SliceThickness&lt;/strong&gt;: The thickness of the slices.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SpacingBetweenSlice&lt;/strong&gt;: The spacing between each slice.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;image-quality-metrics&quot;&gt;Image Quality Metrics&lt;/h3&gt;

&lt;p&gt;Various Image Quality Metrics (IQMs) scores are reported to quantitatively evaluate the image quality. The metric items vary depending on the image modality.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;IQMs for structural images&lt;/strong&gt;: Such as T1WI, T2WI, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IQMs for functional images&lt;/strong&gt;: Such as fMRI-related images, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;IQMs for diffusion images&lt;/strong&gt;: Such as DWI, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IQM score results can also be found in the JSON files generated in the MRIQC output directory for each image.&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/iqm.png?raw=true&quot; alt=&quot;iqm&quot; style=&quot;width: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;iqms-for-structural-images&quot;&gt;IQMs for Structural Images&lt;/h1&gt;

&lt;p&gt;In this example, let’s explore IQMs for structural images, considering the use of T1-weighted imaging (T1WI).&lt;/p&gt;

&lt;h2 id=&quot;measures-based-on-noise-measurements&quot;&gt;Measures based on noise measurements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cjv&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Coefficient of joint variation (CJV)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;A measure of relative variation considering two or more variables simultaneously, indicating how much variation of several variables is compared to their mean.&lt;/li&gt;
      &lt;li&gt;It is useful when dealing with datasets that include multiple variables and helps understand overall variability&lt;/li&gt;
      &lt;li&gt;It is calculated as the ratio of the standard deviation of multiple variables to their mean:&lt;/li&gt;
    &lt;/ul&gt;

\[CJV={(Standard \ Deviation \ of \ Combined \ Variables)\over(Mean \ of \ Combined \ Variables)}\times100\%\]

    &lt;ul&gt;
      &lt;li&gt;MRIQC calculates CJV between gray matter (GM) and white matter (WM) of the brain. The CJV of GM and WM serves as the objective function for optimizing the Intensity Non-Uniformity (INU) correction algorithm, as proposed by &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/fninf.2016.00010/full&quot;&gt;Ganzetti et al.&lt;/a&gt;.
        &lt;ul&gt;
          &lt;li&gt;INU refers to the unevenness in brightness observed across different regions in MRI, often caused by non-uniformity in the magnetic field, especially by variations in radiofrequency (RF) transmission intensity.&lt;/li&gt;
          &lt;li&gt;INU can degrade image accuracy, making interpretation difficult, hence it’s advisable to correct INU for improving MRI quality.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;A higher CJV implies stronger head motion or larger INU defects, indicating poorer image quality. Therefore, lower CJV values are indicative of better image quality.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snr&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Signal-to-noise ratio (SNR)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;A measure of the relationship between the strength of the measured signal and the level of surrounding noise, indicating the quality and accuracy of the measured signal. Signal represents the signal observed in the tissue of interest, while noise refers to signals arising from patient motion or electronic interference, among others. SNR is used to distinguish between the two.&lt;/li&gt;
      &lt;li&gt;A higher SNR indicates that the signal of interest is larger compared to the noise, signifying better data quality.&lt;/li&gt;
    &lt;/ul&gt;

\[SNR={Signal \ Strength\over Stnadard \ Deviation \ of \ Noise}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snrd&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Dietrich’s SNR (SNRd)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Calculates SNR with reference to the surrounding air background in MRI, serving as a vital metric for assessing MRI quality. &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/jmri.20969&quot;&gt;Proposed by Dietrich et al.&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;Since air typically exhibits uniform signal, referencing it allows for a more precise differentiation between signal and noise, thereby enhancing diagnostic accuracy.&lt;/li&gt;
    &lt;/ul&gt;

\[SNRd={Signal \ Strength\over Stnadard \ Deviation \ of \ Air Background}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cnr&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Contrast-to-noise ratio (CNR)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Extends the concept of SNR, representing the relationship between contrast and noise levels in an image. Contrast refers to the brightness difference between structures or objects in an image, while noise refers to irregular or random signals.&lt;/li&gt;
      &lt;li&gt;A Higher CNR indicates lower noise when achieving the desired image contrast, signifying a clearer representation of objects or structures with minimal noise.  This facilitates easier interpretation and improves image quality.&lt;/li&gt;
      &lt;li&gt;MRIQC employs CNR to evaluate how well GM and WM are delineated and how easily the image can be interpreted.&lt;/li&gt;
    &lt;/ul&gt;

\[CNR={|\mu_{GM}-\mu_{WM}|\over \sqrt{\sigma^2_{GM}+\sigma^2_{wM}}}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qi_2&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Mortamet’s Quality index 2 (QI2)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Evaluates the appropriateness of data distribution within the air mask after the removal of artificial intensities. The suitability of data distribution within the air mask region can affect the reliability of image processing and interpretation.&lt;/li&gt;
      &lt;li&gt;Lower values indicate better quality.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;measures-based-on-information-theory&quot;&gt;Measures based on information theory&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;efc&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Entropy-focus criterion (EFC)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Uses the Shannon entropy of voxel intensities to measure ghosting and blurring caused by head movements. &lt;a href=&quot;https://ieeexplore.ieee.org/document/650886&quot;&gt;Proposed by Atkinson et al.&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;As ghosting and blurring increase, voxels lose information, causing the Shannon entropy of the voxels to increase. Thus, EFC has higher values with more ghosting and blurring, meaning that lower values indicate better image quality.&lt;/li&gt;
      &lt;li&gt;The formula is normalized by maximum entropy, allowing comparison across images of different dimensions. $p_i$ represents the probability of each voxel, and $N$ represents the number of pixels.&lt;/li&gt;
    &lt;/ul&gt;

\[EFC={-\sum^N_i=1 p_i\log_2(p_i) \over \log_2(N)}\]
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fber&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Fraction of brain explained by resting-state data (FBER)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Compares the mean energy of brain tissue within the image to the mean air value outside the brain, measuring how much brain tissue is included in the image to assess image quality. &lt;a href=&quot;&quot;&gt;Proposed by Shehzad et al.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;It is one of the Quality Assurance Protocol (QAP) metrics.&lt;/li&gt;
    &lt;/ul&gt;

\[FBER ={Mean \ energy \ of image \ value \ within \ the \ head \over Mean \ energy \ of image \ value \ outside \ the \ head}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;measures-targeting-specific-artifacts&quot;&gt;Measures targeting specific artifacts&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inu&lt;/code&gt; : &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Summary statistics of the INU bias field extracted by N4ITK (max, min, median)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;a href=&quot;https://ieeexplore.ieee.org/document/5445030&quot;&gt;N4ITK&lt;/a&gt; algorithm is an advanced technique that improves MRI image quality by correcting RF field inhomogeneity.&lt;/li&gt;
      &lt;li&gt;The INU field, or bias field, refers to the field filtered through N4ITK. The quality of an image can be assessed through the statistics of the INU field. Values closer to 0 indicate greater RF field inhomogeneity, while values closer to 1 indicate better correction and higher quality images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qi_1&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Mortamet’s Quality index 1 (QI1)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;An index used to detect artificial intensities on air masks. It is used to properly analyze air masks by removing artificial intensities.&lt;/li&gt;
      &lt;li&gt;It is generally considered an important metric in preprocessing stages of image data, such as MRI, to enhance image quality.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wm2max&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;White-matter to maximum intensity ratio&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;The ratio of the median intensity within the WM to the 95th percentile of the overall intensity distribution. This measures the proportion of significant intensities within the WM region.&lt;/li&gt;
      &lt;li&gt;This ratio can reveal when the tail of the intensity distribution is extended, which often occurs due to the intensities from arterial blood vessels or fatty tissue.&lt;/li&gt;
      &lt;li&gt;If the ratio falls outside the range of 0.6 to 0.8, the WM region of the image is considered non-uniform, indicating lower quality.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-measures&quot;&gt;Other measures&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fwhm&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Full width ad half maximum (FWHM)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Represents the full width at half maximum of the intensity values’ spatial distribution in an image, used to measure the image’s resolution and sharpness.&lt;/li&gt;
      &lt;li&gt;Determined by the full width value at half the maximum point of the spatial distribution.&lt;/li&gt;
      &lt;li&gt;Lower FWHM values indicate sharper, higher-resolution images.&lt;/li&gt;
      &lt;li&gt;In MRIQC, FWHM is calculated using the Gaussian width estimator filter implemented in AFNI’s 3dWHMx.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icvs_*&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Intracranial volume scaling (ICVS)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Intracranial volume (ICV) refers to the total volume of fluid within the cranial membrane surrounding the brain and intracranial fluid. ICVS represents the relative proportion of a specific tissue based on ICV in MRI.&lt;/li&gt;
      &lt;li&gt;In MRIQC, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volume_fraction()&lt;/code&gt; function is used to calculate the ICVS for cerebrospinal fluid (CSF), GM, and WM&lt;/li&gt;
      &lt;li&gt;The state of the brain can be assessed by determining whether each ICVS fluctuates within the normal range and whether they maintain ideal ratios to one another.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summary_*_*&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;MRIQC’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;summary_stats()&lt;/code&gt; function provides various statistics related to the pixel distribution in the background, CSF, GM, and WM regions of an MRI. These statistics can be used to evaluate image quality.&lt;/li&gt;
      &lt;li&gt;Includes mean, median, median absolute deviation (MAD), standard deviation, kurtosis, 5th percentile, 95th percentile, and number of voxels.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tpm&lt;/code&gt; &lt;span style=&quot;background-color:#FFEFD5&quot;&gt;Tissue probability map (TPM)&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Refers to the probability distribution of brain tissue types (e.g., GM, WM). In MRIQC, it measures the overlap between the estimated TPM from the image and the map of the ICBM nonlinear-asymmetric 2009c template.&lt;/li&gt;
      &lt;li&gt;ICBM nonlinear-asymmetric 2009c template: One of the standard brain maps provided by the International Consortium for Brain Mapping (ICBM).
        &lt;blockquote&gt;
          &lt;p&gt;A number of unbiased non-linear averages of the MNI152 database have been generated that combines the attractions of both high-spatial resolution and signal-to-noise while not being subject to the vagaries of any single brain (Fonov et al., 2011). … We present an unbiased standard magnetic resonance imaging template brain volume for normal population. These volumes were created using data from ICBM project.&lt;/p&gt;

          &lt;p&gt;6 different templates are available: …&lt;/p&gt;

          &lt;p&gt;ICBM 2009c Nonlinear Asymmetric template – 1×1x1mm template which includes T1w,T2w,PDw modalities, and tissue probabilities maps. Intensity inhomogeneity was performed using N3 version 1.11 Also included brain mask, eye mask and face mask.Sampling is different from 2009a template. … &lt;a href=&quot;https://nist.mni.mcgill.ca/icbm-152-nonlinear-atlases-2009/&quot;&gt;[Reference]&lt;/a&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mriqc.readthedocs.io/en/latest/iqms/t1w.html#ganzetti2016&quot;&gt;MRIQC’s Documentation - IQMs for Structural Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 3-1] Opening an HTML file using Flask</title>
   <link href="https://alatteaday.github.io/study/dev%20tips%20&%20fixes/2024/05/21/html_flask/"/>
   <updated>2024-05-21T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/dev%20tips%20&%20fixes/2024/05/21/html_flask</id>
   <content type="html">&lt;p&gt;MRIQC analyzes and evaluates the quality of MRI images and outputs a report as an HTML file. To view the HTML file, I used Flask. Here is a summary of the method I used.&lt;/p&gt;

&lt;h1 id=&quot;flask&quot;&gt;Flask&lt;/h1&gt;

&lt;p&gt;Flask is a micro web framework written in Python. With its lightweight and flexible structure, it helps you quickly develop simple web applications and API servers. Since it includes only basic features, it is highly extensible, allowing you to add various plugins and extension modules as needed. It also has an easy-to-learn and intuitive code structure, making it suitable for beginners. However, because it comes with minimal features, you need to use external libraries to add complex functionalities, and maintaining the project can become challenging as the project size grows.&lt;/p&gt;

&lt;h1 id=&quot;opening-an-html-file-using-flask&quot;&gt;Opening an HTML file using Flask&lt;/h1&gt;

&lt;h2 id=&quot;installing-flask&quot;&gt;Installing Flask&lt;/h2&gt;

&lt;p&gt;You can install it via PyPI:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install Flask
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;static-and-templates&quot;&gt;‘static/’ and ‘templates/’&lt;/h2&gt;

&lt;p&gt;Flask requires two folders, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates/&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt; stores static files such as images, CSS, and JavaScript that exist in or are applied to HTML files. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates/&lt;/code&gt; stores the HTML files to be rendered.&lt;/p&gt;

&lt;p&gt;Let me explain the process of opening an MRIQC report as an example. After creating these two folders in your project folder, save the static files and the HTML file you want to open in each respective folder:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/1.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;If the file path stored in the HTML file under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt; already exists, it will be updated to the new path. When opening the MRIQC report HTML file, you’ll find that image files are specified with relative paths. Since the images have been moved to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static/&lt;/code&gt; directory, the paths will be changed to absolute paths accordingly:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/2.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/3.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;writing-execution-code&quot;&gt;Writing execution code&lt;/h2&gt;

&lt;p&gt;And then, write the code to render the HTML file. Here is the code I used in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.py&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flask&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@app.route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;render_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sub-001_ses-001_T1w.html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;0.0.0.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app = Flask(__name__)&lt;/code&gt;: Creates an instance of a Flask application. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__name__&lt;/code&gt; refers to the name of the current module and is used by Flask to locate resources for the application.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@app.route(&quot;/&quot;)&lt;/code&gt;: A decorator that instructs Flask to call the test function for the root URL (/).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test()&lt;/code&gt;: The function that will be executed when the root URL is requested&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;return render_template(&quot;HTML_FILE_NAME.html&quot;)&lt;/code&gt;: Renders and returns the HTML file located in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templates/&lt;/code&gt; directory.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.run(&quot;0.0.0.0&quot;, port=5001)&lt;/code&gt;: Runs the application on address 0.0.0.0 and port 5001.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;When you visit the specified address, you will see that the HTML file is displayed correctly.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-21-html_flask/4.png?raw=true&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://flask.palletsprojects.com/en/3.0.x/&quot;&gt;Flask’s Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://daeunnniii.tistory.com/103&quot;&gt;https://daeunnniii.tistory.com/103&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 3] Running MRIQC: A Step-by-Step Guide using nii2dcm, Heudiconv, and MRIQC</title>
   <link href="https://alatteaday.github.io/study/2024/05/20/mriqc_run/"/>
   <updated>2024-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/20/mriqc_run</id>
   <content type="html">&lt;style&gt;
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: -0.5em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
p.b{
   margin-top: 1em;
   margin-bottom: -1em;
   margin-left: 0em;
   margin-right: 0em;
}
&lt;/style&gt;

&lt;p&gt;MRIQC analyzes and evaluates the quality of the input MRI images and compiles the relevant information into a report. To use MRIQC, you need MRI images stored in the &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/20/bids/&quot;&gt;BIDS&lt;/a&gt; format. In this post, I will detail the process of running MRIQC and obtaining analysis results using DICOM files.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;nii2dcm&quot;&gt;nii2dcm&lt;/h1&gt;

&lt;p&gt;While I used DICOM files here, NIfTI is also a common MRI file format. If you are using NIfTI files, you can use a BIDS converter that supports NIfTI or convert the NIfTI files to DICOM and then use a DICOM-supported BIDS converter. Based on my personal experience, BIDS converters that support NIfTI did not work reliably (though this might have been due to my own mistakes). You can use the &lt;a href=&quot;https://github.com/tomaroberts/nii2dcm&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nii2dcm&lt;/code&gt; library&lt;/a&gt; to convert NIfTI files to DICOM. Refer to the code below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nii2dcm NIFTI_FILE_DIR OUTPUT_DIR -d MR
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NIFTI_FILE_DIR&lt;/code&gt;: Path to the NIfTI file you want to convert&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: Path where the converted DICOM files will be saved&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;heudiconv&quot;&gt;Heudiconv&lt;/h1&gt;

&lt;p&gt;I used Heudiconv as the BIDS converter. I summarized the instructions by referring to the tutorial provided on the official page. Here’s how to use it:&lt;/p&gt;

&lt;h2 id=&quot;installing-heudiconv&quot;&gt;Installing Heudiconv&lt;/h2&gt;

&lt;p&gt;Install via PyPI:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install heudiconv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;adjusting-heuristicpy&quot;&gt;Adjusting heuristic.py&lt;/h2&gt;

&lt;p&gt;Write a code to define the rules for saving each image in the BIDS format. You can refer to or modify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;heuristic.py&lt;/code&gt; file from the data repository provided in the tutorial. This file determines the modality of the input image files and creates file paths that conform to the BIDS format for each modality, saving the images accordingly. Modify the judgment criteria and save paths as necessary.&lt;/p&gt;

&lt;p&gt;The function to refer to and modify is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;infotodict()&lt;/code&gt; in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;heuristic.py&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Identify the modality of the images to be used: T1WI, T2WI, DWI, etc.&lt;/li&gt;
  &lt;li&gt;Delete or comment out the code related to unused modalities.&lt;/li&gt;
  &lt;li&gt;Check the path format where the modality images will be saved and modify it if needed.&lt;/li&gt;
  &lt;li&gt;Specify and modify the criteria (dimensions, current filename characteristics, etc.) in the conditional statements to distinguish each modality.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The modified example code is as follows. For T1WI and DWI, the path where the images will be saved and the conditions to determine the image modality have been set.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/bids_ex1.png?raw=true&quot; style=&quot;zoom: 90%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;running-heudiconv&quot;&gt;Running Heudiconv&lt;/h2&gt;

&lt;p&gt;After installation, set the parameters and run it as follows. Heudiconv can process multiple sets of subject data, i.e., multiple bundles of DICOM files, at once.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;heudiconv --files DICOM_FILE_DIRS -o OUTPUT_DIR -f HEURISTIC.PY -s SUB_ID -ss SES_ID -c dcm2niix -b minmeta --overwrite 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DICOM_FILE_DIRS&lt;/code&gt;: Input the DICOM files for multiple subjects in a globbing format (e.g., dataset/sub-001/ses-001/&lt;em&gt;/&lt;/em&gt;.dcm)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: Path where the converted BIDS format folder will be saved&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HEURISTIC.PY&lt;/code&gt;: Path to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;heuristic.py&lt;/code&gt; file created above&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUB_ID&lt;/code&gt;: Subject id (e.g. 001)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SES_ID&lt;/code&gt;: Session id (e.g. 001)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an example of how to run it. Enter the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;heudiconv --files data/*/*.dcm -o bids/data/ -f heuristic.py -s 0 -ss 0 -c dcm2niix -b minmeta --overwrite 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;BIDS format folders will be created under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bids/data/&lt;/code&gt; as follows:&lt;/p&gt;

&lt;p class=&quot;b&quot; align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/bids_ex2.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mriqc&quot;&gt;MRIQC&lt;/h1&gt;

&lt;p&gt;Once the MRI images are stored in the BIDS format, they can be input into MRIQC. MRIQC can be used by downloading the package via PyPI or through a Docker container.&lt;/p&gt;

&lt;h2 id=&quot;with-pypi&quot;&gt;With PyPI&lt;/h2&gt;

&lt;p&gt;First, install it using the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python -m pip install -U mriqc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After installation, run the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mriqc BIDS_ROOT_DIR OUTPUT_DIR participant --participant-label SUB_ID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BIDS_ROOT_DIR&lt;/code&gt;: Root path of the BIDS format folder&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: Path where the MRIQC results will be saved&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant OR group&lt;/code&gt;: If set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant&lt;/code&gt;, MRIQC analysis results will be obtained per subject; if set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;group&lt;/code&gt;, MRIQC will analyze all images under the root path.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUB_ID&lt;/code&gt;: In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant&lt;/code&gt; mode, specify the subject ID for analysis by entering it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--participant-label&lt;/code&gt;. Multiple IDs can be entered at once (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--participant-label 001 002 003&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;with-docker&quot;&gt;With Docker&lt;/h2&gt;

&lt;p&gt;I used MRIQC through Docker. The advantage of Docker containers is that they include all dependencies needed to run the program, ensuring a consistent environment. Enter the following code to run MRIQC at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;participant&lt;/code&gt; level:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -it --rm -v BIDS_ROOT_DIR:/data:ro -v OUTPUT_DIR:/out nipreps/mriqc:latest /data /out participant --participant_label SUB_ID [--verbose-reports]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nipreps/mriqc&lt;/code&gt; image is not downloaded, it will automatically download when you run the code.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BIDS_ROOT_DIR&lt;/code&gt;: Root path of the BIDS format folder. This is connected to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/data&lt;/code&gt; folder inside the container using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-v&lt;/code&gt; flag. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ro&lt;/code&gt; option stands for ‘read only’, meaning the path can only be read from the local path to the container path.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;: Path where the MRIQC results will be saved. This is connected to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/out&lt;/code&gt; folder inside the container. If you copy the contents of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/out&lt;/code&gt; folder in the container to your local machine, you will see that the results are saved in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;To copy the internal container files: When running the above &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run&lt;/code&gt; command, remove the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--rm&lt;/code&gt; (remove container after completion) option. After completion, execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker cp CONTAINER_NAME:FILE_PATH LOCAL_PATH&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SUB_ID&lt;/code&gt;: Subject ID. Multiple IDs can be entered.  (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--participant_label 001 002 003&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--verbose-reports&lt;/code&gt; (Optional): If this flag is included, four additional plots will be reported along with the default visual report plot.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After running the above code, you can check the list of Docker images and containers to see the MRIQC-related items that have been executed.&lt;/p&gt;

&lt;p class=&quot;b&quot; align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/docker_ex.png?raw=true&quot; alt=&quot;docker_ex&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mriqc-results&quot;&gt;MRIQC Results&lt;/h1&gt;

&lt;p class=&quot;b&quot; align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-mriqc_run/mriqc_ex1_1.png?raw=true&quot; alt=&quot;mriqc_ex1_1&quot; style=&quot;zoom: 45%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;When the MRIQC analysis is complete, the above-mentioned files will appear under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT_DIR&lt;/code&gt;. Among these, the analysis results are contained in the plot image files within the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;figures&lt;/code&gt; folder, and the JSON and HTML files named after the respective files, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-0_ses-0_T1w.json&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-0_ses-0_T1w.html&lt;/code&gt; in this example. The results report is generated as an HTML file based on the plot images and JSON files.&lt;/p&gt;

&lt;p class=&quot;b&quot; style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex1.png?raw=true&quot; alt=&quot;ex1&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex2.png?raw=true&quot; alt=&quot;ex2&quot; style=&quot;width: 32%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-28-mriqc_report/ex3.png?raw=true&quot; alt=&quot;ex3&quot; style=&quot;width: 32%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;By &lt;a href=&quot;&quot;&gt;opening the HTML file&lt;/a&gt;, you can view a report like the one above. By &lt;a href=&quot;https://alatteaday.github.io/ko/study/2024/05/28/mriqc_report/&quot;&gt;interpreting the report&lt;/a&gt; using the visualized plots and quality metric scores, you can determine the quality of the images.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tomaroberts/nii2dcm&quot;&gt;nii2dcm Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://heudiconv.readthedocs.io/en/latest/&quot;&gt;Heudiconv’s Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mriqc.readthedocs.io/en/latest/&quot;&gt;MRIQC’s Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 2] Brain Imaging Data Structure (BIDS)</title>
   <link href="https://alatteaday.github.io/study/2024/05/20/bids/"/>
   <updated>2024-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/20/bids</id>
   <content type="html">&lt;h1 id=&quot;brain-imaging-data-structure-bids&quot;&gt;Brain Imaging Data Structure (BIDS)&lt;/h1&gt;

&lt;p&gt;The Brain Imaging Data Structure (BIDS) was created to streamline the organization and sharing of neuroimaging and behavioral data. The driving force behind BIDS is the need for a standardized format in neuroimaging research to prevent misunderstandings, eliminate the time spent on data reorganization, and improve reproducibility. By offering a straightforward and intuitive structure for data, BIDS aims to promote collaboration, speed up research, and make neuroimaging data more accessible to a diverse range of scientists.&lt;/p&gt;

&lt;p&gt;BIDS provides detailed guidelines on how to format and name files, ensuring consistency across studies. It supports various neuroimaging modalities, including MRI, MEG, EEG, and iEEG, and is extensible, allowing for the integration of new data types and metadata. Additionally, BIDS is supported by a growing ecosystem of tools and software that facilitate data validation, analysis, and sharing, further enhancing its utility in the research community.&lt;/p&gt;

&lt;h1 id=&quot;bids-format&quot;&gt;BIDS Format&lt;/h1&gt;

&lt;p&gt;BIDS was inspired by the format used by the OpenfMRI repository, which is now known as OpenNeuro. The BIDS format is essentially a method for organizing data and metadata within a hierarchical folder structure. It makes minimal assumptions about the tools needed to interact with the data, allowing for flexibility and broad compatibility. This structure helps standardize data organization, facilitating easier data sharing, analysis, and collaboration within the neuroimaging research community.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;folders&quot;&gt;Folders&lt;/h2&gt;

&lt;p&gt;There are four levels of the folder hierarchy, and all sub-folders except for the root folder have a specific structure to their name. The format and the example names can be described as:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Project/
└─ Subject (e.g. &apos;sub-01/&apos;)
  └─ Session (e.g. &apos;ses-01/&apos;)
    └─ Datatype (e.g. &apos;anat&apos;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Project: contains all the dataset. can have any name.&lt;/li&gt;
  &lt;li&gt;Subject: contains data of one subject. One folder per subject. A subject has a unique label.
    &lt;ul&gt;
      &lt;li&gt;Name format: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub-PARTICIPANT LABEL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Session: represents a recording session. Each subject may have multiple sessions if the data is gathered from several occasions.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;If there’s only a single session per subject, this level may be omitted.&lt;/li&gt;
      &lt;li&gt;Name format: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ses-SESSION LABEL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Datatype: represents different types of data.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anat&lt;/code&gt;: anatomical MRI data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;func&lt;/code&gt;: functional MRI data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fmap&lt;/code&gt;: fieldmap data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dwi&lt;/code&gt;: diffusion MRI data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt;: arterial spin labeling data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eeg&lt;/code&gt;: electroencephalography data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;meg&lt;/code&gt;: magnetoencephalography data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ieeg&lt;/code&gt;: intracranial EEG data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;beh&lt;/code&gt;: behavioral data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pet&lt;/code&gt;: positron emission tomography data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;micr&lt;/code&gt;: microscopy data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nirs&lt;/code&gt;: near-infrared spectroscopy data&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;motion&lt;/code&gt;: motion capture data&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;files&quot;&gt;Files&lt;/h2&gt;

&lt;p&gt;Three main types of files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.json&lt;/code&gt; file: contains metadata&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tsv&lt;/code&gt; file: contains tables of metadata&lt;/li&gt;
  &lt;li&gt;Raw data files: e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.jpg&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.nii.gz&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Standardized ways of naming files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do not include white spaces in file names&lt;/li&gt;
  &lt;li&gt;Use only letters, numbers, hyphens, and underscores.&lt;/li&gt;
  &lt;li&gt;Do not rely on letter case: Some operating systems regard &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt; as the same as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;A&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Use separators and case in a systematic and meaningful way.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CamelCase&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;snake_case&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Filename template&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-05-20-bids/fig3.png?raw=true&quot; alt=&quot;fig3&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Dataset/
 └─ participants.json
 └─ participants.tsv
 └─ sub-01/
   └─ anat/
     └─ sub-01_t1w.nii.gz
     └─ sub-01_t1w.json
   └─ func/
     └─ sub-01_task-rest_bold.nii.gz
     └─ sub-01_task-rest_bold.json
   └─ dwi/
     └─ sub-01-task-rest_dwi.nii.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://bids.neuroimaging.io/&quot;&gt;BIDS Official Website&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bids-standard.github.io/bids-starter-kit/index.html&quot;&gt;BIDS Starter Kit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bids-standard&quot;&gt;BIDS Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>[MRIQC 1] MRIQC: Magnetic Resonance Imaging Quality Control</title>
   <link href="https://alatteaday.github.io/study/2024/05/19/mriqc/"/>
   <updated>2024-05-19T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2024/05/19/mriqc</id>
   <content type="html">&lt;p&gt;To advance research on MRI images and enhance quality, it is essential to check the condition of the image data and secure high-quality data. However, assessing MRI quality is challenging due to several factors. There are many types of artifacts that can occur during MRI scans, people evaluate image quality differently, and some artifacts are difficult for humans to detect. In this context, an objective MRI quality control (QC) system can be helpful in the early stages of MRI quality assessment. Additionally, the recent trend of acquiring very large image data samples from multiple scanning sites increases the need for fully automated and minimally biased QC protocols.&lt;/p&gt;

&lt;h1 id=&quot;magnetic-resonance-imaging-quality-control-mriqc&quot;&gt;Magnetic Resonance Imaging Quality Control (MRIQC)&lt;/h1&gt;

&lt;p&gt;MRIQC (Magnetic Resonance Imaging Quality Control) can be used as an automated tool for assessing MRI quality. MRIQC is an open-source tool designed to evaluate the quality of structural(anatomical) and functional MRI images. MRIQC extracts &lt;a href=&quot;https://alatteaday.github.io/study/2024/05/28/mriqc_report/&quot;&gt;image quality metrics (IQMs)&lt;/a&gt; solely from the input images themselves, without referencing any target images. Additionally, it provides a standardized method for evaluating and comparing MRI scans from various sources or sessions.&lt;/p&gt;

&lt;h1 id=&quot;priciples&quot;&gt;Priciples&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Modular and Integrable&lt;/strong&gt;: MRIQC uses a modular workflow built on the Nipype framework, integrating various third-party software toolboxes such as ANTs and AFNI​.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimal Preprocessing&lt;/strong&gt;: It focuses on minimal preprocessing to estimate IQMs from the original or minimally processed data, ensuring that the quality metrics reflect the raw image data as closely as possible​.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability and Standards&lt;/strong&gt;: MRIQC adheres to the &lt;a href=&quot;https://alatteaday.github.io/study/2024/05/20/bids/&quot;&gt;Brain Imaging Data Structure (BIDS)&lt;/a&gt; standard, promoting interoperability and facilitating integration into various neuroimaging workflows​​.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reliability and Robustness&lt;/strong&gt;: The tool is rigorously tested for robustness against data variability, ensuring consistent performance across different datasets and acquisition parameters​.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual Reports&lt;/strong&gt;: MRIQC generates detailed &lt;a href=&quot;https://alatteaday.github.io/study/2024/05/28/mriqc_report/&quot;&gt;visual reports&lt;/a&gt; for both individual images and group analyses. These reports include mosaic views and segmentation contours for individual images, and scatter plots for group analyses to identify outliers​.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;image-quality-metrics-iqms&quot;&gt;Image Quality Metrics (IQMs)&lt;/h1&gt;

&lt;p&gt;MRIQC computes a range of &lt;a href=&quot;https://alatteaday.github.io/study/2024/05/28/mriqc_report/&quot;&gt;IQMs&lt;/a&gt; categorized into four main groups:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Noise-related metrics&lt;/strong&gt;: Evaluate the impact and characteristics of noise within the images.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Information theory-based metrics&lt;/strong&gt;: Assess the spatial distribution of information using prescribed masks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Artifact detection metrics&lt;/strong&gt;: Identify and measure the impact of specific artifacts, such as inhomogeneity and motion-related signal leakage.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical and morphological metrics&lt;/strong&gt;: Characterize the statistical properties of tissue distributions and the sharpness/blurriness of images​.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;paper&quot;&gt;Paper&lt;/h1&gt;

&lt;p&gt;Esteban O, Birman D, Schaer M, Koyejo OO, Poldrack RA, Gorgolewski KJ (2017) MRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites. PLoS ONE 12(9): e0184661. https://doi.org/10.1371/journal.pone.0184661&lt;/p&gt;

&lt;h1 id=&quot;how-to-run-mriqc&quot;&gt;How to run MRIQC&lt;/h1&gt;

&lt;p&gt;Interested in running MRIQC? Check out &lt;a href=&quot;https://alatteaday.github.io/study/2024/05/20/mriqc_run/&quot;&gt;this post&lt;/a&gt; for detailed instructions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mriqc.readthedocs.io/en/latest/&quot;&gt;MRIQC’s Official Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184661&quot;&gt;Paper Link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] Amyloid-β prediction machine learning model using source-based morphometry across neurocognitive disorders (2024)</title>
   <link href="https://alatteaday.github.io/papers/2024/04/18/keioAbMRI/"/>
   <updated>2024-04-18T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2024/04/18/keioAbMRI</id>
   <content type="html">&lt;p&gt;Momota, Yuki, et al. “Amyloid-β prediction machine learning model using source-based morphometry across neurocognitive disorders.” &lt;em&gt;Scientific Reports&lt;/em&gt; 14.1 (2024): 7633.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41598-024-58223-3&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Investigated MRI-based machine learning models to predict Alzheimer’s disease (AD), with a focus on diverse patient populations.&lt;/li&gt;
  &lt;li&gt;Utilized source-based morphometry (SBM) to assess Amyloid-beta deposition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Preprocessed 3D T1 weighted images into voxel-based gray matter (GM) images, then subjected them to SBM.&lt;/li&gt;
  &lt;li&gt;Implemented a support vector machine (SVM) as a classifier.&lt;/li&gt;
  &lt;li&gt;Employed SHapley ADditive exPlanations (SHAP) for model interpretability and accountability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Achieved a final model accuracy of 89.8% when incorporating MR images, cognitive test results, and apolipoprotein E status.&lt;/li&gt;
  &lt;li&gt;Attained an 84.7% accuracy with the model based solely on MR images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;AD is a neurodegenerative disorder characterized by the presentce of A$\beta$ plaques, neurofibrillary tangles, and brain atrophy.&lt;/li&gt;
  &lt;li&gt;A$\beta$ is a defining characteristics of AD, but detecting it is not covenient in routine clinical practice.
    &lt;ul&gt;
      &lt;li&gt;Methods such as position emission tomography (PET), cerebrospinal fluid (CSF) testing, and Blood biomarkers are used for A$\beta$ detection but are not yet applicable in routine clinical practice.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MRI-based A$\beta$ prediction may serve as a useful screening tool before definitive diagnosis through the aforementioned methods.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/subfig1.png?raw=true&quot; alt=&quot;supfig1&quot; style=&quot;zoom: 90%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;Features&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Participants and clinical measurements&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Recruited in Jury 2018 ~ August 2021 from the memory clinic at Keio University Hospital.&lt;/li&gt;
  &lt;li&gt;AD / MCI / HC&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cognitive assessment&lt;/strong&gt; (9 measures)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Global cognitive function: Mimi-mental state examination (MMSE), Clinical dementia rating (CDR), Functional activity questionnaire (FAQ)&lt;/li&gt;
  &lt;li&gt;Memory: Wechsler Memory Scale-Revised (WMS-R) Lgical Memeory immediate recall (LM I) and delayed recall (LM II)&lt;/li&gt;
  &lt;li&gt;Executive function and attention: Word Fluency, Trail Making Test (TMT)&lt;/li&gt;
  &lt;li&gt;Specific cognitive abilities: Japanese version of Alzheimer’s Disease Assessment Scale-Cognitive subscale (ADAS-cog-J), Japanese Adult Reading Test (JART)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Apolipoprotein E (APOE) genotyping&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Magnetic nanoparticle DNA extraction kit (EX1 DNA Blodd 200 $\mu$L Kit)&lt;/li&gt;
  &lt;li&gt;real-time polymerase chain reaction (PCR)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;[&lt;sup&gt;18&lt;/sup&gt;F] Florbetaben (FBB) amyloid-PET imaging&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;[&lt;sup&gt;18&lt;/sup&gt;F] Florbetaben (FBB)&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;Florbetaben, a fluorine-18 (18F)-labeled stilbene derivative (formerly known as BAY-949172), trade name NeuraCeq, is a diagnostic radiotracer developed for routine clinical application to visualize β-amyloid plaques in the brain.  [&lt;a href=&quot;https://en.wikipedia.org/wiki/Florbetaben_(18F)&quot;&gt;reference&lt;/a&gt;]&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mri&quot;&gt;MRI&lt;/h2&gt;

&lt;h3 id=&quot;acquisition---3d-t1-weighted-mr-images-t1-wi&quot;&gt;Acquisition - 3D T1 weighted MR images (T1 WI)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;MRI scanner: Discovery MR750 3.0 T scanner (GE Healthcare)&lt;/li&gt;
  &lt;li&gt;Coil: 32-channel head coil&lt;/li&gt;
  &lt;li&gt;Imaging parameters: field of view (FOV) 230mm, matrix size 256$\times$256, slice thickness 1.0mm, voxel size 0.9$\times$0.9$\times$1.0mm&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pre-processing&quot;&gt;Pre-processing&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Segmentation&lt;/strong&gt;: Segmented the MR images into different tissue types: gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) using Statistical Parametric Mapping toolbox CAT12.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Normalization&lt;/strong&gt;: The Segmented GM images are then normalized to the Montreal Neurological Institute (MNI) template, which is a standard anatomical template commonly used in neuroimaging research.&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;Standard anatomical templates are widely used in human neuroimaging processing pipelines to facilitate group level analyses and comparisons across different subjects and populations. The MNI-ICBM152 template is the most commonly used standard template, representing an average of 152 healthy young adult brains.  [&lt;a href=&quot;https://nist.mni.mcgill.ca/mni-ftd-templates/&quot;&gt;reference&lt;/a&gt;]&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Resampling and Smoothing&lt;/strong&gt;: Resampled the images to an isotropic voxel size of 2$\times$2$\times$2mm&lt;sup&gt;3&lt;/sup&gt; and smoothed using a 5mm full-width-at-half-maximum Gaussian kernel.
    &lt;ul&gt;
      &lt;li&gt;This step helps to standardize the voxel size an reduce noise in the images.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Source-based morphometry (SBM)&lt;/strong&gt;: Incorporates independent component analysis (ICA) to automatically decompose the anatomical brain images into independent spatial maps characterizing different modes of anatomical variability accorss all individuals.&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is Gaussian and that the subcomponents are statistically independent from each other.  [&lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_component_analysis&quot;&gt;reference&lt;/a&gt;]&lt;/p&gt;

      &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/ica.png?raw=true&quot; alt=&quot;ica&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ICA processing&lt;/strong&gt;: The 3D GM images (91$\times$109$\times$91 voxels) are loaded and converted into a 1D array format (1$\times$902,629) for processing.
    &lt;ul&gt;
      &lt;li&gt;A brain mask is created to select relevant (208,082) voxels for ICA using FastICA.&lt;/li&gt;
      &lt;li&gt;The number of extracted independent components (ICs) is a hyperparameter that is tuned for subsequent model building.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Spatial Regression&lt;/strong&gt;: The extracted ICs are used as spatial regressors for each participant’s GM images, with weighting coefficients ($\beta$) determining the effect of each IC on the GM image.&lt;/p&gt;

\[I_{GM}=\beta_1 IC_1 + \beta_2 IC_2 + ... + \beta_K IC_K\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;machine-learning&quot;&gt;Machine learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Input features: ICA’s $\beta$-values, demographic characteristics (age and sex), cognitive assessments, APOE genotype&lt;/li&gt;
  &lt;li&gt;Input conduction: The model is trained and tested using various combinations of input features.
    &lt;ol&gt;
      &lt;li&gt;All input features together&lt;/li&gt;
      &lt;li&gt;Each combination of features: brain images alone, brain images + cognitive assessments, etc.&lt;/li&gt;
      &lt;li&gt;Different combination of diagnoses: AD+HC, AD+MCI+HC&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Model: Gaussian kernel support vector machine (SVM)
    &lt;ul&gt;
      &lt;li&gt;Training involves classification using 5-vold cross-validation.&lt;/li&gt;
      &lt;li&gt;Testing is performed over all splits (5 times), ensuring robust evaluation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Interpretability: SHaply Additive exPlanations (SHAP)
    &lt;ul&gt;
      &lt;li&gt;SHAP values, based on game theory, indicate the influence of features on predictions.&lt;/li&gt;
      &lt;li&gt;Features with large absolute SHAP values have a strong influence on predictions.&lt;/li&gt;
      &lt;li&gt;Clinical features with positive and negative SHAP values were associated with A$\beta$+ and  A$\beta$- conditions, respectively&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;statistical-analysis&quot;&gt;Statistical analysis&lt;/h2&gt;

&lt;p&gt;Explores relationships between variables, identifying associations with diagnoses, and testing hypotheses in the context of Alzheimer’s disease research.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Two-tailed t-test / Chi-square test
    &lt;ul&gt;
      &lt;li&gt;Two-tailed t-test: used to compare the means of two groups to determine if there is a significant difference between them.&lt;/li&gt;
      &lt;li&gt;Chi-square test: used to test the independence between categorical variables.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Relationships among features: Pearson’s correlation analysis for continous variables
    &lt;ul&gt;
      &lt;li&gt;Measures the strength and direction of linear relationships between pairs of continuous variables.&lt;/li&gt;
      &lt;li&gt;Provides insights into how variables are related to each other.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Associations with diagnoses: Analysis of variance (ANOVA)
    &lt;ul&gt;
      &lt;li&gt;Used to anaylze the difference among group menas in a sample.&lt;/li&gt;
      &lt;li&gt;Useful when there’re more than two groups being compared, as it determines whether there are statistically significant differences among the group means.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;118 cases used for the final model building&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table1.png?raw=true&quot; alt=&quot;table1&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-performance&quot;&gt;Model performance&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table2.png?raw=true&quot; alt=&quot;table2&quot; style=&quot;zoom: 80%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 80%;&quot; /&gt; 
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table3.png?raw=true&quot; alt=&quot;table3&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A$\beta$ positivity prediction&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The final model: the model trained with brain images + cognition + APOE as input&lt;/li&gt;
  &lt;li&gt;The highest accuracy (89.8%) and AUC (0.888) with brain images + cognition + APOE&lt;/li&gt;
  &lt;li&gt;The lowest accuracy (84.7%) and AUC (0.830) with brain images alone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The final model’s performance for predicting A$\beta$ positivity in each diagnosis&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The highest accuracy (89.8%) when including all the paticipants&lt;/li&gt;
  &lt;li&gt;The lowest accuarcy (75.9%) based solely on MCI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sbm&quot;&gt;SBM&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/table4.png?raw=true&quot; alt=&quot;table4&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/addfig2.png?raw=true&quot; alt=&quot;addfig2&quot; style=&quot;zoom: 100%;&quot; /&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;7 independent components (ICs) were derived from the final SBM model&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each component showed spatially maximally independent GM volum patters.&lt;/li&gt;
  &lt;li&gt;IC 1 showed a significant correlation with cognitive measures an A$\beta$ positivity.&lt;/li&gt;
  &lt;li&gt;Only AD and IC 1 showed a significant association.&lt;/li&gt;
  &lt;li&gt;Other diagnoses were not associated with any ICs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;The proposed model predicted A$\beta$ positivity successfully. (accuracy 89.8%, AUC 0.888)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;With 118 participants’ data consisting of the features: brain MRI, cognitive info., genetic info.&lt;/li&gt;
  &lt;li&gt;Predicted correctly in non-AD subjects, such as those with FTLD syndrome and psychiatric disorders.&lt;/li&gt;
  &lt;li&gt;Among covariants in the final model, IC 1 had the strongest impact realted to A$\beta$ positivity prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Informative heterogeneity of features among non-AD participants
    &lt;ul&gt;
      &lt;li&gt;The performance of the model based only on AD continuum achieved slightly lower (88.4%) than on all cases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Advantages of SBM
    &lt;ul&gt;
      &lt;li&gt;The model based on diverse clinical populations may be better suited for application in clinical settings.
        &lt;ul&gt;
          &lt;li&gt;Patients visiting physicians’ would have various neurocongitive disorders beyond the AD continuum.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The proposed model based only on brain images (accuracy 84.7%) may assist for screening of potential candidates for AD-related clinical trials.&lt;/li&gt;
      &lt;li&gt;SBM detects subtle morphological changes and unknown patterns in brain structures associated with ND diseases without relying on existing atlases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Comparable prediction performance in MCI patiences
    &lt;ul&gt;
      &lt;li&gt;Surpassed the accuracy of the physician’s clinical diagnosis of AD (75.9%  &amp;gt; 70%)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;feature-importance-of-the-model---shap&quot;&gt;Feature Importance of the model - SHAP&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-18-keioab/fig3.png?raw=true&quot; alt=&quot;fig3&quot; style=&quot;zoom: 80%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;All ICs demonstrated greater importance compared to demoghrapic and cognitive features such as MMSE. The three most influential features in the model were identified as follows: IC 1, logical memory (LM) I, and LM II.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;IC 1 exhibited a significantly correlation with A$\beta$ positivity and cognivite measures.
    &lt;ul&gt;
      &lt;li&gt;Its spacial pattern of the loading coefficients closely resembled the cortical pattern observed in neurodegeneration (ND) in AD, particularly in the parietal lobe.&lt;/li&gt;
      &lt;li&gt;No Medial temporal lobe (MTL) atrophy was observed in any IC, which is the typical AD pattern.
        &lt;ul&gt;
          &lt;li&gt;This discrepance suggests a potential indication of tau pathodology rather than A$\beta$ pathology.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LM scores reflected memory impairments, a cardinal symptom of AD.&lt;/li&gt;
  &lt;li&gt;The presence of APOE -$\epsilon#4 also emerged as a significant factor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Furthermore, the model revealed distinct associations between IC 1 and A$\beta$ positivity, as well as IC 4 and age.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;This indicates the model’s ability to discriminate between AD-related ND from normal aging in brain imaging, suggesting that the pathological process of AD is not strictly age-dependent. 
→ Brain atrophy patterns in normal aging processes can be distinguished from those in neurodegeneartive disease.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitation&quot;&gt;Limitation&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;A$\beta$ positivity was determined only by amyloid-PET scan: CSF A$\beta$ would be a more sensitive marker in the pre-clinical status.&lt;/li&gt;
  &lt;li&gt;a limited number of samples: could be affect accuracy of a machine learning model.&lt;/li&gt;
  &lt;li&gt;Longitudinal follow-up data might improve model performance, rather than a cross-sectional approach.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Tabtransformer: Tabular data modeling using contextual embeddings (2020)</title>
   <link href="https://alatteaday.github.io/papers/2024/04/11/tabtf/"/>
   <updated>2024-04-11T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2024/04/11/tabtf</id>
   <content type="html">&lt;p&gt;Huang, Xin, et al. “Tabtransformer: Tabular data modeling using contextual embeddings.” &lt;em&gt;arXiv preprint arXiv:2012.06678&lt;/em&gt; (2020).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.06678&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;TabTransformer&lt;/strong&gt;: A cutting-edge tabular data model leveraging contextual embeddings.&lt;/li&gt;
  &lt;li&gt;Pre-trained by innovative two-phase approach for robust feature representation.&lt;/li&gt;
  &lt;li&gt;Showed SOTA performance in both supervised and semi-supervised learning.&lt;/li&gt;
  &lt;li&gt;Handles missing and noisy data robustly, ensuring reliable performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;The current state-of-the-art (SOTA) mdoels for tabular data primarily consist of tree-based ensemble methods, notably gradient boosted decision trees (GBDT). However, these models exhibit several limitations in comparison to deep learning models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Not suitable for continual learning from streaming data.&lt;/li&gt;
  &lt;li&gt;Ineffective for end-to-end learning of multi-modality of tabular data, such as incorporating image or text features.&lt;/li&gt;
  &lt;li&gt;Not suitable for semi-supervised learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the other had, while multi-layer perceptrons (MLPs) offer the potential for end-to-end learning of image or text encoders, they are constrained by several drawbacks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lack of interpretability.&lt;/li&gt;
  &lt;li&gt;Vulnerability to missing and noisy data.&lt;/li&gt;
  &lt;li&gt;Limited performance in semi-supervised learning scenarios.&lt;/li&gt;
  &lt;li&gt;Inability to match the performance of tree-based models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/archi.png?raw=true&quot; alt=&quot;archi&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Transformer layers receive only categorical inputs $x_{cat}$.&lt;/li&gt;
  &lt;li&gt;Continuous inputs $x_{cont}$ are concatenated with the outputs of the Transformer modules of the categorical inputs.&lt;/li&gt;
  &lt;li&gt;During the pre-training phase, the Transformer layers undergo training on two different tasks using unlabeled data
    &lt;ul&gt;
      &lt;li&gt;Only the categorical inputs are utilized for pre-training, with the exclusion of the continuous inputs.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code1.png?raw=true&quot; alt=&quot;code1&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The pre-trained model is fine-tuned alongsidethe MLP head, utilizing labeled data to predict a target $y$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continuous values are incorporated during the fine-tuning phase by concatenating them with the categorical values.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code2.png?raw=true&quot; alt=&quot;code2&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each instance $x\equiv \lbrace x_{cat}, x_{cont}\rbrace$ is paired with its corresponding label $y$: $(x, y)$.&lt;/li&gt;
  &lt;li&gt;$x_{cat} \equiv \lbrace x_1, x_2, …, x_m\rbrace$ represents categorical features, with each $x_i$ being a categorical feature $i \in {1, …, m}$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$x_{cat}$ undergoes transformation into column embedding $E_\phi$:&lt;/p&gt;

\[E_\phi(x_{cat}) \equiv \lbrace e_{\phi_1}(x_1), ..., e_{\phi_m}(x_m) \rbrace, \ e_{\phi_i}(x_i) \in \mathbb{R}^d\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The embeddings are fed into the multiple Transformer layers $f_\theta$, producing contextual embeddings:&lt;/p&gt;

\[\{h_1, ..., h_m\}=f_\theta(E_\phi(x_{cat})), \ h\in \mathbb{R}^d\]
  &lt;/li&gt;
  &lt;li&gt;Contextual embeddings of $x_{cat}$ are concatenated with the $x_{cont} \in \mathbb{R}^c $ to form a vector of dimension $(d\times m+c)$.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The vector is passed through an MLP layer $g_\psi$ and a cross-entropy loss $H$ is computed between the predicted output and the target $y$:&lt;/p&gt;

\[L(x, y) \equiv H(g_\psi(f_\theta(E_\phi(x_{cat})), x_{cont}), y)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Column Embedding&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/colemb.png?raw=true&quot; alt=&quot;colemb&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each categorical feature $x_i$ has its own embedding lookup table $e_{\phi_i}(.)$.&lt;/li&gt;
  &lt;li&gt;For the $i$th feature with $d_i$ classes, the embedding table $e_{\phi_i}(.)$ contains $(d_1+1)$ embeddings. The additional $d_1+1$th embedding is reserved for representing the missing(masked) values.&lt;/li&gt;
  &lt;li&gt;Each embedding $e_{\phi_i}(j)$ is represented as $[c_{\phi_i}, w_{\phi_{ij}}]$, where:
    &lt;ul&gt;
      &lt;li&gt;$c_{\phi_i}$ helps distinguish the classes in column $i$ from those in the other columns.&lt;/li&gt;
      &lt;li&gt;$w_{\phi_{ij}}$ distinguishes the class of the feature $j$ within the $i$th column from the other classes within the same column.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;*The dimension $d$ likely is set to be the same as the hidden dimension $h$ according to the codes.&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/code3.png?raw=true&quot; alt=&quot;code3&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h2&gt;

&lt;p&gt;The Transformer layers are trained using inputs consisting of categorical values $x_{cat}=\lbrace x_1, x_2, …, x_m\rbrace$ on two pre-training tasks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Masked language modeling (MLM)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Randomly masks $k\%$ features of the input, where $k$ is set to 30 in experiments.&lt;/li&gt;
      &lt;li&gt;Minimizes the cross-entropy loss of a multi-class classifier $g_\psi$, which predicts the original features of the masked features.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Replaced token detection (RTD)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Replaces the original feature by a random value of that feature.&lt;/li&gt;
      &lt;li&gt;Minimizes the loss of a binary classifier predicting whether the feature has been replaced.&lt;/li&gt;
      &lt;li&gt;Each column has its own embedding lookup table, necessitating the definition of a separate binary classifier for each column.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;settings&quot;&gt;Settings&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Models were evaluated on 15 publicly available binary classification datasets sourced from UCI repository, AutoML Challenge, and Kaggle.&lt;/li&gt;
  &lt;li&gt;Each dataset was divided into 5 cross-validation splits.&lt;/li&gt;
  &lt;li&gt;Training:Validation:Testing proportion was set to 65:15:20 (%).&lt;/li&gt;
  &lt;li&gt;The number of categorical features ranged from 2 to136.&lt;/li&gt;
  &lt;li&gt;Semi-supervised and supervised experiments
    &lt;ul&gt;
      &lt;li&gt;Semi-supervised: Training data consisted of $p$ labeled data points + the remaining unlabeled data, with $p\in (50, 200, 500)$ for 3 different scenarios.&lt;/li&gt;
      &lt;li&gt;Supervised: Fully labeled training data was used.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hidden dimension: 32&lt;/li&gt;
  &lt;li&gt;The num of layers: 6&lt;/li&gt;
  &lt;li&gt;The num of attention heads: 8&lt;/li&gt;
  &lt;li&gt;MLP layer architecture: $\lbrace 4\times l, \ 2\times l \rbrace$ (where $l$ represents the size of its input).&lt;/li&gt;
  &lt;li&gt;Hyperparamter optimization (HPO) conducted with 20 rounds for each cross-validation split.&lt;/li&gt;
  &lt;li&gt;Metrics: Area under the curve (AUC).&lt;/li&gt;
  &lt;li&gt;Pre-training was exclusively applied in the semi-supervised scenario.
    &lt;ul&gt;
      &lt;li&gt;It was not found to be significantly beneficial when the entire dataset was labeled.&lt;/li&gt;
      &lt;li&gt;Its benefits were more apparent when there is a large number of unlabeled examples and a few labeled examples, as pre-training provided representations of the data that could not be learned solely from the labeled examples.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Baseline model&lt;/strong&gt;: An MLP model without Transformers was employed to evaluate the effectiveness of Transformers in comparison.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-effectiveness-of-the-transformer-layers&quot;&gt;The effectiveness of the Transformer Layers&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Performance comparison&lt;/strong&gt;&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table1.png?raw=true&quot; alt=&quot;table1&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Conducted in a supervised learning scenario, comparing TabTransformer to MLP.&lt;/li&gt;
      &lt;li&gt;TabTransformer outperforms the baseline MLP on 14 datasets, achieving an average 1.0% gain in AUC.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;t-SNE visualization of contextual embeddings&lt;/strong&gt;&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Each marker in the plot represents an average of 2D points over the test data points for a certain class.&lt;/li&gt;
      &lt;li&gt;In the t-SNE plot of the last layer of TabTransformer (Left), semantically similar classes are closely grouped, forming clusters in the embedding space.&lt;/li&gt;
      &lt;li&gt;Before passing into the Transformer (Center), the embeddings start to distinguish features with different characteristics.&lt;/li&gt;
      &lt;li&gt;The embeddings of MLP (Right) do not reveal any discernible pattern.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Prediction performance of linear models using the embeddings from different Transformer layers&lt;/strong&gt;&lt;/p&gt;

    &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig3.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 60%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Logistic regression models are employed to evaluate the quality of learned embeddings.&lt;/li&gt;
      &lt;li&gt;Each model predicts $y$ using embedding features along with continuous values.&lt;/li&gt;
      &lt;li&gt;Metrics: Cross-validation score in AUC on the test data.&lt;/li&gt;
      &lt;li&gt;Normalization: Each prediction score is normalized by the best score from an end-to-end trained TabTransformer for the corresponding dataset.&lt;/li&gt;
      &lt;li&gt;Features: Embeddings are averaged and processed using maximum pooling instead of concatenation.&lt;/li&gt;
      &lt;li&gt;The effectiveness of the embeddings improves as the Transformer layers progress.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-robustness-of-tabtransformer&quot;&gt;The robustness of TabTransformer&lt;/h2&gt;

&lt;p&gt;The robustness of TabTransformer was evaluated by assessing its performance on datasets containing noisy data and data with missing values.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/fig4_5.png?raw=true&quot; alt=&quot;fig4_5&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Noisy data&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Method: Values were replaced with randomly generated ones from corresponding columns, introducing noise into datasets.&lt;/li&gt;
      &lt;li&gt;Findings: As the noise increases, TabTransformer demonstrated significantly significantly superior compared to the MLP (see fig. 4).&lt;/li&gt;
      &lt;li&gt;The contextual property of embeddings likely contributes to TabTransformer’s robustness in noisy environments.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data with missing values&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Method: Some values artificially made missing, and models were evaluated on these modified datasets.
        &lt;ul&gt;
          &lt;li&gt;The average learned embeddings over all classes in the corresponding columns were used to handle the embeddings of missing values.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Findings: TabTransformer exhibited better stability than MLP in handling missing values (see fig. 5).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;supervised-learning&quot;&gt;Supervised learning&lt;/h2&gt;

&lt;p&gt;TabTransformer’s performance was compared against four categories of methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Logistic Regression and GBDT&lt;/li&gt;
  &lt;li&gt;MLP and sparse MLP&lt;/li&gt;
  &lt;li&gt;TabNet model&lt;/li&gt;
  &lt;li&gt;Variational Information Bottleneck (VIB) model&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table2.png?raw=true&quot; alt=&quot;table2&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Findings:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TabTransformer demonstrated comparable performance with GBDT.&lt;/li&gt;
  &lt;li&gt;It significantly outperformed than recent deep learning models designed for tabular data, including TabNet and VIB.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-supervised learning&lt;/h2&gt;

&lt;p&gt;TabTransformer was evaluated under the semi-supervised learning scenario and compared against other semi-supervised models, including baseline models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Entropy Regularization (ER)&lt;/li&gt;
  &lt;li&gt;Pseudo Labeling (PL) combined with MLP, TabTransformer, and GBDT&lt;/li&gt;
  &lt;li&gt;MLP (DAE): An unsupervised pre-training method designed for deep models on tabular data, specifically the swap noise Denoising AutoEncoder&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-04-11-tabtf/table3_4.png?raw=true&quot; alt=&quot;table3_4&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Method:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pre-trained models (TabTransformer-RTD/MLM and MLP): pre-trained on the unlabeled data and then fine-tuned on labeled data.&lt;/li&gt;
  &lt;li&gt;Semi-supervised learning methods (ER and PL): trained on the mix of labeled and unlabeled training data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Findings:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;TabTransformer-RTD/MLM are outperformed all the other models.&lt;/li&gt;
  &lt;li&gt;TabTransformer (ER), TabTransformer (PL) and GBDT (PL) performed worse than the average of all the models.&lt;/li&gt;
  &lt;li&gt;TabTransformer-RTD consistently showed better results when the number of unlabeled data decreased, surpassing TabTransformer-MLM.
    &lt;ul&gt;
      &lt;li&gt;This could be attributed to the easier pre-training task of a binary classification compared to the multi-class classification of MLM.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;With only 50 data points, MLM (ER) and MLM (PL) outperformed TabTransformer models.
    &lt;ul&gt;
      &lt;li&gt;The suggests that the proposed approach allows for informative embeddings but does not enable the weights of the classifier itself to be trained with unlabeled data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Overall, TabTransformer models are promise in extracting useful information from unlabeled data to aid supervised training, and are particularly useful when the size of unlabeled data is large.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Github.io에서 markdown 수식 문법 적용이 안될 때</title>
   <link href="https://alatteaday.github.io/error%20resolution/2024/03/15/GitioMathError/"/>
   <updated>2024-03-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/error%20resolution/2024/03/15/GitioMathError</id>
   <content type="html">&lt;p&gt;Github blog 포스트에 수식을 작성했는데, markdown 수식 문법 적용이 되지 않는 문제가 있었습니다. 해결 방법을 기록해두고자 포스팅합니다.&lt;/p&gt;

&lt;h2 id=&quot;1-_configyml-파일-수정&quot;&gt;1. _config.yml 파일 수정&lt;/h2&gt;

&lt;p&gt;markdown process 관련 설정을 확인하여 수정, 없으면 추가합니다. markdown engine을 kramdown으로 설정해야 한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml1.png?raw=true&quot; style=&quot;zoom:52%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-_includes-폴더-내-수식-문법-관련-html-파일-작성&quot;&gt;2. _includes 폴더 내 수식 문법 관련 HTML 파일 작성&lt;/h2&gt;

&lt;p&gt;일반적으로 github blog 내에는 _include 폴더가 존재합니다. 폴더 내에 수식 문법이 포스트에 적용될 수 있게끔 하기 위한 스크립트를 작성합니다. 아래 내용이 HTML 파일에 작성되면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html0.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inlineMath&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt; 항목에서 각각의 수식 문법 기호를 설정할 수 있습니다. 위 예시의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt; 와 같이 리스트 내에 여러 기호를 설정할 수 있습니다. 위 예시에 따르면 수식을  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt; 로 감싸거나, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt; 사이에 입력하면 display style로 작성할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;*&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt; 말고  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\]&lt;/code&gt; 로 문법을 설정하여 포스트에 적용하면, [ ] 괄호를 사용한 일반 텍스트까지 수식으로 처리되는 경우가 있었습니다.&lt;/p&gt;

&lt;h3 id=&quot;inline과-display-style&quot;&gt;Inline과 Display style&lt;/h3&gt;

&lt;p&gt;수식 입력 방식에는 inline style과 display style이 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inline style: 줄 바꿈 없이, 문장 내에서 수식을 표기하는 방법&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Display style: 수식을 블록으로 생성해 표기하는 방법&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$2$ plus $3$ is $5$: $$2+3=5$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;$2$ plus $3$ is $5$: \[2+3=5\]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-2에서-작성한-html-스크립트를-포스트에-적용&quot;&gt;3. 2에서 작성한 HTML 스크립트를 포스트에 적용&lt;/h2&gt;

&lt;p&gt;위에서 작성한 스크립트를 실제 포스팅 시 적용하기 위해 layout에 관련한 HTML 파일을 수정합니다. _layout 폴더에 있는 HTML 파일 중 적합한 파일을 찾아 포스트의 내용 부분에 새로 작성한 HTML 파일의 내용을 가져와 적용합니다. 저는 ‘default.html’ 파일 중 content가 입력되는 부분을 찾아 수정했습니다. 아래 예시와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html1.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;content&quot;&lt;/code&gt; 블록 내 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ content }&lt;/code&gt; 의 위치에 작성한 포스트의 본문이 보여집니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;include file.html&lt;/code&gt; 은 ‘file.html’의 내용을 가져온다는 뜻입니다. 따라서 해당 블록 내에 ‘math.html’에서 작성한 수식 문법 사항을 적용하겠다는 의미의 코드가 됩니다.&lt;/p&gt;

&lt;p&gt;위 코드를 아래와 같이 수정하면 수식 문법 적용 여부를 포스팅 시 설정해 줄 수 있는데요,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt; 가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt; 이면 ‘math.html’ 내용을 적용한다는 의미의 코드입니다. 여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page&lt;/code&gt; 는 각 포스트를 의미합니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt; 을 설정하기 위해서는 매 포스트 작성 시 Front Matter에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math: true&lt;/code&gt; 를 추가해주면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;수식이 필요 없거나, 수식을 적용하기 싫은 포스트에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math&lt;/code&gt; 를 추가하지 않거나 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt; 로 설정하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://junia3.github.io/blog/markdown&quot;&gt;https://junia3.github.io/blog/markdown&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://an-seunghwan.github.io/github.io/mathjax-error/&quot;&gt;https://an-seunghwan.github.io/github.io/mathjax-error/&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>When mathematical expression syntax isn't applying on GitHub Pages</title>
   <link href="https://alatteaday.github.io/dev%20tips%20&%20fixes/2024/03/15/GitioMathError/"/>
   <updated>2024-03-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/dev%20tips%20&%20fixes/2024/03/15/GitioMathError</id>
   <content type="html">&lt;p&gt;I wrote a math expression in a GitHub blog post, but there was an issue with applying markdown syntax. I’m posting this to document the solution that I applied.&lt;/p&gt;

&lt;h2 id=&quot;1-modify-the-_configyml-file&quot;&gt;1. Modify the _config.yml file&lt;/h2&gt;

&lt;p&gt;Check and modify the markdown-related settings in the _config.yml file like below. If they don’t exist, add them like below. It’s recommended to set the markdown engine to kramdown.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml1.png?raw=true&quot; style=&quot;zoom:52%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-write-a-html-file-of-math-expression-syntax-within-the-_includes-folder&quot;&gt;2. Write a HTML file of math expression syntax within the _includes folder&lt;/h2&gt;

&lt;p&gt;Generally, GitHub blogs contain an _include folder. Write a script within this folder to enable math expression syntax to be applied to posts. Let’s assume creating a html file named ‘math’&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html0.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can set each math syntax mark for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inlineMath&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt;. Similar to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displayMath&lt;/code&gt; item in the above code, you can specifiy multiple marks in the list. Following the example, if you wrap the formula in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt;, the math style will be displayed as the display style.&lt;/p&gt;

&lt;p&gt;*When setting the syntax as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\[&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\]&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\[&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\\]&lt;/code&gt;, there might be instances where ordinary text enclosed within square brackets is also treated as part of the math expression.&lt;/p&gt;

&lt;h3 id=&quot;inline-and-display-style&quot;&gt;Inline and Display style&lt;/h3&gt;

&lt;p&gt;The inline style and the display style are two styles of math expression.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inline style: Representing math expression within a sentence without line breaks&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Display style: Generating math expression as blocks for representation&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$2$ plus $3$ is $5$: $$2+3=5$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;$2$ plus $3$ is $5$: \[2+3=5\]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-apply-the-html-script-created-in-2-to-the-post&quot;&gt;3. Apply the HTML script created in 2. to the post&lt;/h2&gt;

&lt;p&gt;To apply the script created above to an actual post, you’ll need to modify the HTML file related to the layout. Find an appropriate file in the _layout folder and incorporate the content of the html file into the section where the post’s content is inserted. For example, I found and modified the ‘default.html’ file like the example below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html1.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{ content }&lt;/code&gt; displalys the main body of the post. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;include file.html&lt;/code&gt; means it includes the content of ‘file.html’. Therefore, within this block, it signifies applying the math syntax written in ‘math.html’&lt;/p&gt;

&lt;p&gt;You can modify the code and adjust if applying the math syntax or not,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/html2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The code &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt; being &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt; indicates that the content of ‘math.html’ will be applied. Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page&lt;/code&gt; refers to the each page. To set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;page.use_math&lt;/code&gt;, simply add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math: true&lt;/code&gt; to the Front Matter of each post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-03-15-GitioMathError/yaml2.png?raw=true&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For posts where math expressions are not needed or you prefer not to apply them, simply omit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;use_math&lt;/code&gt; tag or set it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://junia3.github.io/blog/markdown&quot;&gt;https://junia3.github.io/blog/markdown&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://an-seunghwan.github.io/github.io/mathjax-error/&quot;&gt;https://an-seunghwan.github.io/github.io/mathjax-error/&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Docker CMDs</title>
   <link href="https://alatteaday.github.io/dev%20tips%20&%20fixes/2024/02/20/dockercmd/"/>
   <updated>2024-02-20T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/dev%20tips%20&%20fixes/2024/02/20/dockercmd</id>
   <content type="html">&lt;h1 id=&quot;image&quot;&gt;Image&lt;/h1&gt;

&lt;h2 id=&quot;search-for-an-image&quot;&gt;Search for an Image&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker search [OPTIONS] IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--automated=false&lt;/code&gt; Show only automated builds&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--no-trunc=false&lt;/code&gt; Show all results&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-s=n&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--stars=n&lt;/code&gt; Show only images with at least n stars&lt;/li&gt;
  &lt;li&gt;e.g.,
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker search --stars=100 mysql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;download-an-image&quot;&gt;Download an Image&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull [OPTIONS] IMAGE_NAME[:TAG_NAME]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; Download all versions of the image&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TAG_NAME&lt;/code&gt; Specify the version to download, if not specified, the latest version is downloaded&lt;/li&gt;
  &lt;li&gt;e.g.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull ubuntu:22.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;list-images&quot;&gt;List Images&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker images [OPTIONS] [REPOSITORY]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--all&lt;/code&gt; Show all images&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--digests&lt;/code&gt; Show digests&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-q&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--quiet&lt;/code&gt; Show only image IDs&lt;/li&gt;
  &lt;li&gt;*To list containers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker ps&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inspect-image-details&quot;&gt;Inspect Image Details&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker image inspect IMAGE_ID
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;You can enter part of the ID instead of the full ID.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;delete-an-image&quot;&gt;Delete an Image&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rmi [OPTION] IMAGE_NAME:TAG_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-f&lt;/code&gt; Force delete the image of a running container, but in practice, it only untagged and does not actually delete the image or container.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Delete multiple images at once&lt;/em&gt;:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rmi IMAGE_NAME_1 IMAGE_NAME_2 ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Stop all running containers of a specific image and then delete the image&lt;/em&gt;:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rm -f $(docker ps -a --filter ancestor=IMAGE_NAME)
docker rmi IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;save-and-load-images&quot;&gt;Save and Load Images&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker save -o DIRECTORY IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-o&lt;/code&gt; (output) Specify the directory to save the image&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker load -i DIRECTORY
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-i&lt;/code&gt; Specify the directory of the image to load (input), the image in that directory is loaded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tag-an-image&quot;&gt;Tag an Image&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker tag IMAGE_NAME:TAG NEW_NAME:NEW_TAG
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Allows you to reference an existing image with a new name and tag.&lt;/li&gt;
  &lt;li&gt;e.g.,
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker tag ubuntu:22.04 abcd:0.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;container&quot;&gt;Container&lt;/h1&gt;

&lt;h2 id=&quot;list-containers&quot;&gt;List Containers&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker ps [OPTION]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Lists running containers.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; Show all containers, including those that have stopped.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;inspect-container-details&quot;&gt;Inspect Container Details&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker inspect CONTAINER_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;run-a-container-from-an-image&quot;&gt;Run a Container from an Image&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run [OPTIONS] IMAGE_NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--name CONTAINER_NAME&lt;/code&gt; Set the container name&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--rm&lt;/code&gt; Delete the container after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; command is executed. One-time use of the container&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-it&lt;/code&gt; Keep passing terminal input to the container
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-i&lt;/code&gt; Keep stdin open even if not attached&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-t&lt;/code&gt; Allocate a pseudo-TTY, use TTY mode to write commands to the shell&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-d&lt;/code&gt; Run the container in the background. When this option is specified, the container ID is output.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-e&lt;/code&gt; Add environment variables. Use as many as you need to add.
    &lt;ul&gt;
      &lt;li&gt;e.g.,
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -e APP_ENV=production APP2_ENV=dev ubuntu:22.04 env
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p HOST_PORT:CONTAINER_PORT&lt;/code&gt; Bind a specific port of the container connected to the host to the port of the host. Usually used to expose the web server port to the outside.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-w DIR&lt;/code&gt; Change the working directory&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-v HOST_DIR:CONTAINER_DIR&lt;/code&gt; Mount a specific directory of the host to the container
    &lt;ul&gt;
      &lt;li&gt;e.g.,
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -v volume:/data ubuntu:22.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Mount the current working directory to the container&lt;/em&gt;
        &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run -v `pwd`:/opt ubuntu:22.04
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-u USER_ID&lt;/code&gt; Access the container with a specific user ID. The account must be added when building the image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;execute-commands-in-a-running-container&quot;&gt;Execute Commands in a Running Container&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker exec CONTAINER_ID or NAME CMD
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-it&lt;/code&gt; Run the shell in the container environment,&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;difference-between-run-and-exec&quot;&gt;Difference between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exec&lt;/code&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt;: Run a container from an image&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;exec&lt;/code&gt;: Run commands in an already running container&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stop-a-container&quot;&gt;Stop a Container&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker stop CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Stop a running container (Graceful shutdown)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker kill CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Force stop a running container&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;restart-a-container&quot;&gt;Restart a Container&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker start CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Restart a stopped container&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker restart CONTAINER_ID/NAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Stop and then restart the container&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records (2023)</title>
   <link href="https://alatteaday.github.io/papers/2024/02/15/transformehr/"/>
   <updated>2024-02-15T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2024/02/15/transformehr</id>
   <content type="html">&lt;p&gt;Yang, Zhichao, et al. “TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records.” &lt;em&gt;Nature Communications&lt;/em&gt; 14.1 (2023): 7857.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41467-023-43715-z&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;New pre-training objective: predicting all diseases or outcomes of a future visit&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Helps the model uncover the complex interrelations among different diseases and outcomes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;TransformEHR&lt;/strong&gt;, the generative encoder-decoder framework to predict patients’ ICD codes using their longitudinal EHRs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Validation of the generalizability using both internal and external datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Demonstrated a strong transfer learning capability of the model&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Could be great with limited data and computing resources&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Longitudinal electronic health records (EHRs) have been successfully used to predict clinical diseases or outcomes (congestive heart failure, sepsis mortality, mechanical ventilation, septic shock, diabetes, PTSD, etc.)&lt;/li&gt;
  &lt;li&gt;With the availability of large cohorts and computational resources, deep learning (DL) based models outperform traditional machine learning (ML) models (Med-BERT, BEHRT, BRLTM, etc.)&lt;/li&gt;
  &lt;li&gt;The existing pre-training tasks were limited in predicting a fraction of ICD codes within each visit :arrow_right: A novel pre-training strategy, which predicts the complete set of diseases and outcomes within a visit, might improve clinical predictive modeling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;*VHA: Veterans Health Administration, the largest integrated healthcare system in the US, providing care at 1,321 healthcare facilities&lt;/p&gt;

&lt;p&gt;Pre-training data: around 6M patients who received care from more than 1,200 facilities of the US VHA&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/table1.png?raw=true&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two common and uncommon disease/outcome agnostic prediction (DOAP) datasets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;ICD-10CM codes with more than a 2% prevalence ratio for common dataset&lt;/li&gt;
      &lt;li&gt;Those with a 0.04%-0.05% prevalence ratio for uncommon dataset&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Non-VHA dataset: from MIMIC-IV dataset (29,482)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Only selected objects with ICD-10CM records to match the cohorts from VHA&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;longitudinal-ehrs&quot;&gt;Longitudinal EHRs&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/fig1.png?raw=true&quot; alt=&quot;image-20240306100731535&quot; style=&quot;zoom:67%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Include demographic information (gender, age, race, and marital status) and ICD-10CM codes as predictors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Group ICD codes at the visit level&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Order the codes by priority, where the primary diagnosis is typically given the highest priority&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Form multiple visits as a time-stamped input of a sequence by date of visit&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;file:///Users/jiyun/Documents/Gitlog/alatteaday.github.io/post_images/2024-02-15-transformehr/KakaoTalk_Image_2024-03-13-09-14-52_007.png&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/fig2.png?raw=true&quot; alt=&quot;image-20240306103928325&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/code_embeds1.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-level embeddings: visit embeddings + time embeddings + code embeddings&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Time embeddings: embed days difference as relative time information by getting the difference between a certain visit and the last visit in the EHR
    &lt;ul&gt;
      &lt;li&gt;Includes the date of each visit to integrate temporal information, not only sequential order&lt;/li&gt;
      &lt;li&gt;Date is important as the importance of predictor in a visit can vary over time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;Encoder-decoder transformer-based architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/fig3.png?raw=true&quot; alt=&quot;image-20240306114845710&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encoder: performs cross-attention, unlikely BERT, over representations and assigns an attention weight for each representation
    &lt;ul&gt;
      &lt;li&gt;Cross-attention is implemented by masking the complete set of ICD codes of a future visit as shown in Fig. 2b&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Decoder: generates ICD codes of the masked future visit with the weighted representations from the encoder
    &lt;ul&gt;
      &lt;li&gt;Generates the codes following the order of code priority within a visit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;Metrics: PPV (precision), AUROC, AUPRC&lt;/p&gt;

&lt;p&gt;Baseline models: logistic regression, LSTM, BERT without pre-training, BERT with pre-training   &lt;span style=&quot;color:silver;&quot;&gt; # what’s the objective when pre-training BERT? MLM or the objective proposed in this paper? &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-02-15-transformehr/table2_3_4.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h2&gt;

&lt;p&gt;Task: Disease or outcome agnostic prediction (DOAP); Predicting the ICD codes of a patient’s future visit based on longitudinal information up to the current visit&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ablation study&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Visit masking vs. code (part of visit) masking for an encoder-decoder model
    &lt;ul&gt;
      &lt;li&gt;Visit masking performed better; pre-training of all diseases outperform traditional pre-training objective (2.52-2.96% in AUROC)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encoder-decoder vs. encoder-only (BERT) on DOAP
    &lt;ul&gt;
      &lt;li&gt;​Encoder-decoder outperformed; 0.74-1.16% in AUROC   &lt;span style=&quot;color:silver;&quot;&gt;# the possibility if the parameter size affected this result? &lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Time embeddings O vs. X
    &lt;ul&gt;
      &lt;li&gt;The model with the time embeddings outperformed moderately; 0.43 in AUROC&lt;/li&gt;
      &lt;li&gt;Days difference is more effective than specific date as the embeddings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning&quot;&gt;Fine-tuning&lt;/h2&gt;

&lt;p&gt;Tasks: the pancreatic cancer onset prediction (Table 3) and intentional self-harm prediction in patients with PTSD (Table 4)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TransformEHR outperforms on the both tasks&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;AUPRC was consistent when using different set of demographics&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Results with all visits was better than with recent few(five) visits&lt;/li&gt;
  &lt;li&gt;In generalizability evaluation,
    &lt;ul&gt;
      &lt;li&gt;When testing with internal dataset which is included data from VHA facilities not used for pre-training, there’s no statistical difference in AUPRC on the intentional self-harm prediction task among PTSD&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>[Study] Relations between white matter hyperintensity (WMH) feature and amyloid-beta (β-amyloid) and tau burden</title>
   <link href="https://alatteaday.github.io/study/2024/01/31/WMHandAbTau/"/>
   <updated>2024-01-31T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2024/01/31/WMHandAbTau</id>
   <content type="html">&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://newsimg-hams.hankookilbo.com/2023/03/25/0187e938-b461-4abc-9fa1-2279ecbc1f03.jpg&quot; alt=&quot;백질 변성이 오래되고 진행한 73세 환자의 뇌 MRI 사진. 뇌 중심부에 하얀색으로 넓게 퍼져 있음. 부천성모병원 제공&quot; style=&quot;zoom:30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;WMH: A form of cerebrovascular degeneration in which the white matter, the medulla, which acts as a pathway between the cortex and gray matter, is dilated and damaged. Usually seen with MR FLAIR imaging&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Alban, Sierra L., et al. “The association between white matter hyperintensities and amyloid and tau deposition.” &lt;em&gt;NeuroImage: Clinical&lt;/em&gt; 38 (2023): 103383.  &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2213158223000724&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Graff-Radford, Jonathan, et al. “White matter hyperintensities: relationship to amyloid and tau burden.” &lt;em&gt;Brain&lt;/em&gt; 142.8 (2019): 2483-2491.  &lt;a href=&quot;https://academic.oup.com/brain/article/142/8/2483/5519094?login=false&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hedden, Trey, et al. “Cognitive profile of amyloid burden and white matter hyperintensities in cognitively normal older adults.” &lt;em&gt;Journal of Neuroscience&lt;/em&gt; 32.46 (2012): 16233-16242.  &lt;a href=&quot;https://www.jneurosci.org/content/32/46/16233.short&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;keys&quot;&gt;Keys&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The relationship between WMH and amyloid beta accumulation remains unclear: From whether they are related to each other to how they are related if they are related.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A growing number of studies are attempting to elucidate the relationship between WMH and Ab.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is no relationship between WMH and tau accumulation.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;The association between white matter hyperintensities and amyloid and tau deposition (2023)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;… Finally, the regions where β-amyloid and WMH count were most positively associated were &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the middle temporal region in the right hemisphere&lt;/span&gt; (r = 0.18, p = 0.002) and &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the fusiform region in the left hemisphere&lt;/span&gt; (r = 0.017, p = 0.005). β-amyloid and WMH have a clear association, though the mechanism facilitating this association is still not fully understood. The associations found between β-amyloid and WMH burden emphasize the relationship between β-amyloid and vascular lesion formation while factors like CVRFs, age, and sex affect AD development through various mechanisms&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The subset of ADNI-3 participants who had all the T1-weighted, 3D FLAIR, Amyloid, and Tau PET modalities available&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Snippets&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The percentage of white matter volume occupied by WMH was significantly and positively correlated with β-amyloid PET SUVR (Fig. 1; r = 0.28, p &amp;lt; 0.001). We observed &lt;span style=&quot;background-color:#fff5b1&quot;&gt;WMH volume to significantly predict global amyloid accumulation when controlling for age, sex, years of education, and scanner manufacturer&lt;/span&gt; (F(1, 309) = 13.9, p = 0.0002).&lt;/li&gt;
  &lt;li&gt;The correlational analyses were repeated after the log transformation of WMH volume and outcomes remained the same, resulting in &lt;span style=&quot;background-color:#fff5b1&quot;&gt;a significant positive correlation between WMH volume and β-amyloid&lt;/span&gt; (r = 0.24, p = 4.9e-5), and &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;a nonsignificant positive correlation between WMH volume and meta-temporal tau&lt;/span&gt; (r = 0.09, p = 0.12).&lt;/li&gt;
  &lt;li&gt;The inclusion of MOCA, MMSE, and Global CDR, as covariates, did not change the significant relationship between WMH volume and β-amyloid. We observed &lt;span style=&quot;background-color:#fff5b1&quot;&gt;a significant effect of hippocampal volume fraction on WMH volume&lt;/span&gt; (F(1, 580) = 16.9, p = 4.5e-5) and β-amyloid (F(1, 309) = 32.5, p = 2.8e-8).&lt;/li&gt;
  &lt;li&gt;WMH volume percent of participants with either amyloid (A+) or tau (T+) pathology was higher than controls (A-/T-) (Fig. 2). We observed &lt;span style=&quot;background-color:#fff5b1&quot;&gt;a significantly higher WMH percent in AD pathology participants (A+/T+) compared to controls (A-/T-)&lt;/span&gt; (p = 0.007, Cohen’s d = 0.4, t = -2.5). … &lt;span style=&quot;background-color:#fff5b1&quot;&gt;No significant association was found in the A-/T- group&lt;/span&gt; (r = 0.06, p = 0.45). A significant positive correlation was observed between β-amyloid SUVR and WMH count in the A+/T+ group only.&lt;/li&gt;
  &lt;li&gt;WMH count was used as another method of measuring WMH burden. … Both statistical tests on &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the WMH volume and the WMH count showed similar results, confirming that WMH count is an accurate measure to use alongside WMH volume.&lt;/span&gt; Correlations of WMH count with β-amyloid within A/T pathological groups also paralleled the WMH volume analysis result.&lt;/li&gt;
  &lt;li&gt;Our regional analysis showed that β-amyloid and WMH accumulation in &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the precentral, cuneus, fusiform, isthmus cingulate, lateral occipital, lingual, superior parietal, and supramarginal regions were most significantly associated across all pathological groups when averaged across hemispheres.&lt;/span&gt; Variations in the locations of increased WMHs are indicative of AD and its phase of progression, some of these regions being more implicated in cognitive decline than others.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We observed &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;neither a correlation nor an association between WMH and Tau uptake in the entire cohort&lt;/span&gt; (Fig. 4: correlation p = 0.25; association p = 0.4, controlling for age, sex, years of education, and scanner manufacturer).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, &lt;span style=&quot;background-color:#E6E6FF&quot;&gt;WMH volume was only predicted by CN and MCI diagnoses, not AD.&lt;/span&gt; The relationship between WMH volume often predicts AD in the preclinical stages, likely accounting for the relationship we observed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#E6F2FF&quot;&gt;The inclusion of cognitive scores MMSE, MOCA, and Global CDR had no effect on the associations found between β-amyloid and WMH volume&lt;/span&gt;, therefore not significantly impacting this relationship. In the literature, &lt;span style=&quot;background-color:#E6F2FF&quot;&gt;worse performance on these cognitive tests has been associated with increased WMH volume&lt;/span&gt; (Wang et al., 2020).&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;   
  &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Gray727_cingulate_gyrus.png/250px-Gray727_cingulate_gyrus.png&quot; alt=&quot;img&quot; width=&quot;30%&quot; align=&quot;center&quot; /&gt;
  &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Gray726_temporal_pole.png/800px-Gray726_temporal_pole.png?20100306033920&quot; alt=&quot;File:Gray726 temporal pole.png&quot; width=&quot;30%&quot; style=&quot;zoom: 33%;&quot; align=&quot;center&quot; /&gt;
  &lt;figcaption align=&quot;center&quot; style=&quot;color: grey;&quot;&gt; above: the isthmus cingulate; bottom: the temporal pole &lt;/figcaption&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The relationship between higher MMSE scores and increased WMH volume showed significance with multiple comparison corrections while controlling for age and sex in the isthmus cingulate region&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For Global CDR scores, this spatial distribution was significant in the isthmus cingulate, temporal pole, and pars triangularis&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lastly, the MOCA scores showed significant positive spatial relationships in the isthmus cingulate, linguistic, and temporal pole regions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We also did not observe any significant effect of APOE-ε4 presence on the established relationship between β-amyloid and WMH volume or WMH count. … Although, we did have a small sample size of individuals with the homozygous APOE-ε4 genotype.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;White matter hyperintensities: relationship to amyloid and tau burden (2019)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;White matter hyperintense volumes in the detected topographic pattern correlated strongly with lobar cerebral microbleeds&lt;/span&gt; (P &amp;lt; 0.001, age and sex-adjusted Cohen’s d = 0.703). In contrast, &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;there were no white matter hyperintense regions significantly associated with increased tau burden&lt;/span&gt; using voxel-based analysis or region-specific analysis, among non-demented elderly, amyloid load correlated with a topographic pattern of white matter hyperintensities. Further, &lt;span style=&quot;background-color:#fff5b1&quot;&gt;the amyloid-associated, white matter hyperintense regions strongly correlated with lobar cerebral microbleeds suggesting that cerebral amyloid angiopathy contributes to the relationship between amyloid and white matter hyperintensities.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://lh7-us.googleusercontent.com/8lRtc_4VpROGK82z1LNG6_csgW2j70aFK-1z6EtUHavsTxGrkjAOzyxf_l_6ZNFMVsEWDE2WBBkDnYTC8eSIGRPwtYTdmvVaMSYl-XidCzDWZu1aouniUOTX3hUtN76xS_ESzJ5gW6Hg0eZ9Lv0jo-U&quot; alt=&quot;img&quot; style=&quot;zoom: 60%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Participants, aged 50 to 89, were enrolled in the Mayo Clinic Study of Aging (MCSA), a population-based study of Olmsted County, Minnesota residents.&lt;/li&gt;
  &lt;li&gt;434 non-demented participants with FLAIR-MRI, tau-PET (AV-1451), and Pittsburgh compound B (PiB)-PET (amyloid) scans to assess the relationship between FLAIR WMH and Alzheimer’s disease pathologies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Snippets&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In the study of non-demented individuals, we found that amyloid burden measured by PET was associated with a topographic pattern of WMH. &lt;span style=&quot;background-color:#fff5b1&quot;&gt;These amyloid-related WMH regions were associated with lobar CMBs suggesting that regional changes correlate with CAA.&lt;/span&gt; We found &lt;span style=&quot;background-color:#FFE6E6&quot;&gt;no evidence to support an association between tau burden and WMH burden.&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We did not detect an association between tau burden and WMH in either the voxel-level analyses or region-level analyses.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Cognitive Profile of Amyloid Burden and White Matter Hyperintensities in Cognitively Normal Older Adults (2012)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Amyloid burden and WMH were not correlated with one another.&lt;/span&gt; &lt;span style=&quot;background-color:#E6F2FF&quot;&gt;Age was associated with lower performance in all cognitive domains&lt;/span&gt;, while &lt;span style=&quot;background-color:#E6F2FF&quot;&gt;higher estimated verbal intelligence was associated with higher performance in all domains.&lt;/span&gt; Hypothesis-driven tests revealed that &lt;span style=&quot;background-color:#fff5b1&quot;&gt;amyloid burden and WMH had distinct cognitive profiles&lt;/span&gt;, with &lt;span style=&quot;background-color:#fff5b1&quot;&gt;amyloid burden having a specific influence on episodic memory&lt;/span&gt; and &lt;span style=&quot;background-color:#fff5b1&quot;&gt;WMH primarily associated with executive function but having broad (but lesser) effects on the other domains.&lt;/span&gt; These findings suggest that even before clinical impairment, amyloid burden, and WMH likely represent neuropathological cascades with distinct etiologies and dissociable influences on cognition.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;168 (95 female) cognitively normal, community-dwelling older adults (aged 65–86, M=73.24, SD=5.80).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Participants in the Harvard Aging Brain Study, an ongoing longitudinal study currently in the baseline assessment phase&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Because of the staged nature of the visits (all baseline visits must be completed within 6 months), positron emission tomography (PET) amyloid imaging and magnetic resonance imaging (MRI) estimates of WMH were currently available for 109 of the older adults.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] BEHRT: Transformer for Electronic Health Records (2020)</title>
   <link href="https://alatteaday.github.io/papers/2024/01/22/BEHRT/"/>
   <updated>2024-01-22T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2024/01/22/BEHRT</id>
   <content type="html">&lt;p&gt;Li, Yikuan, et al. “BEHRT: transformer for electronic health records.” &lt;em&gt;Scientific reports&lt;/em&gt; 10.1 (2020): 7155.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nature.com/articles/s41598-020-62922-y&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;BEHRT&lt;/strong&gt; (BERT for EHR): BERT-based architecture&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;In EHR, certain diseases can be reversed, or the time interval between two diagnoses can be shorter or longer than recorded.&lt;/p&gt;

        &lt;p&gt;→ Bidirectional contextual awareness of the model’s representation is a big advantage with EHR data.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transfer Learning: pre-training on predicting of masked disease words, such as Masked Language Modeling (MLM), and then fine-tuning on three disease prediction tasks&lt;/li&gt;
  &lt;li&gt;Disease embeddings: show the relations between the various diseases&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In traditional research on EHR data, individuals are represented by models as features. Experts had to define the appropriate features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Studies applying Deep Learning (DL) to EHR started to show that DL models can outperform the traditional Machine Learning (ML) methods&lt;/li&gt;
  &lt;li&gt;With DL architectures for sequence data, such as Recurrent Neural Networks (RNNs), the application of DL models for EHR was improved in terms of capturing the long-term dependencies among events.&lt;/li&gt;
  &lt;li&gt;Similarities between sequences in EHR and natural language lead to the successful transferability of techniques&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;Clinical Practice Research Datalink (CPRD)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;one of the largest linked primary care EHR systems&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Contains longitudinal data from a network of 674 general practitioner practices in the UK&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/1_fig1.png?raw=true&quot; alt=&quot;스크린샷 2024-02-29 오후 8.29.23&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;from ​8 million patients (eligible for linkage to HES, meet CPRD’s quality standards) to 1.6 million patients  (having at least 5 visits in the EHR)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Only the data from GP practices considered in this study, which consented to record linkage with HES&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;input-features&quot;&gt;Input Features&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/2_fig2.png?raw=true&quot; alt=&quot;스크린샷 2024-02-29 오후 8.30.30&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Only consider the diagnoses and ages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The patient $p$’s EHR:
\[
V_p={v^1_p},{v^2_p},{v^3_p}, …,{v^n_p}
\]&lt;/p&gt;

&lt;p&gt;The patient $p$’s EHR of the $j$th visit: $v^j_p$&lt;/p&gt;

&lt;p&gt;$v^j_p$ is a list consisting of single or multiple $m$ diagnoses: $v^j_p={d_1, …d_m}$&lt;/p&gt;

&lt;p&gt;Input sequence:
\[
I_p={CLS, v^1_p, SEP, v^2_p, SEP,…,v^{n_p}_p, SEP}
\]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$CLS$ token: the start of a medical history&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$SEP$ token: between visits&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/3_fig3.png?raw=true&quot; alt=&quot;스크린샷 2024-02-29 오후 8.31.09&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A combination of 4 embeddings: disease + position + age + visit segment&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Disease embeddings: past diseases can improve the accuracy of the prediction for future diagnoses&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Positional encodings: determine the relative position in the EHR sequence&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;p&gt;Pre-training: prediction of masked disease words (MLM)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;86.5% of the disease words were unchanged, 12% of them replaced with the mask token, and the remaining 1.5% words replaced with randomly-chosen disease words&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fine-tuning: 3 different disease prediction tasks&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Prediction of diseases in the next visit (T1)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Randomly choose an index $j (3&amp;lt;j&amp;lt;n_p)$ for each patient&lt;/li&gt;
      &lt;li&gt;Form input as $x_p={v^1_p, …, v^j_p}$&lt;/li&gt;
      &lt;li&gt;Output $y_p=w_{j+1}$, $w_{j+1}$ is a multi-one-hot vector, indexed for disease that exist in $v^{j+1}_p$&lt;/li&gt;
    &lt;/ol&gt;

    &lt;ul&gt;
      &lt;li&gt;Each patient contributes only an input-output pair to the training and evaluation process.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prediction of diseases in the next 6 months (T2) &amp;amp; in the next 12 months (T3)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Not include patients that don’t have 6 or 12 months worth of EHR in the analysis&lt;/li&gt;
      &lt;li&gt;choose $j$ randomly from $(3, n*)$, where $n*$ is the highest index after 6 or 12 months&lt;/li&gt;
      &lt;li&gt;output $y_p=w_{6m}$ and $y_p=w_{12m}$&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The number of patients for each task: 699K, 391K, 342K&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;h2 id=&quot;disease-embeddings&quot;&gt;Disease Embeddings&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/4_fig4.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 9.39.18&quot; /&gt;&lt;/p&gt;

&lt;p&gt;T-SNE Results&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The natural stratification of gender-specific diseases&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The natural clusters are formed that in most cases consist fo disease of the same chapter&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;10 closest diseases by cosine similarity of their embeddings are founded as similar as those provided by a clinical researcher (0.757 overlab) - Supplementary table S3&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;attention-and-interpretability&quot;&gt;Attention and Interpretability&lt;/h2&gt;

&lt;p&gt;Found the relationships among events which goes beyond temporal/sequence adjacency&lt;/p&gt;

&lt;p&gt;Analysed the attention-based patterns by Vig&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Strong connections between rheumatoid arthritis and enthesopathies and synovial disorders → Attention can go beyond recent events and find long-range dependencies among diseases&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;disease-prediction&quot;&gt;Disease Prediction&lt;/h2&gt;

&lt;p&gt;Metrics: average precision score(APS), AUROC&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;APS: a weighted mean of precision and recall achieved at different thresholds&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Performance scores&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/5_table1.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 9.49.39&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;→ Outperformed the best model by more than around 8% in predicting for a range of more than 300 diseases&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comparing performance for each disease&lt;/p&gt;

    &lt;p&gt;APS and AUROC scores with the all \(y_p\) and \(y*_p\) vectors of a disease&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/6_fig6.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 10.08.39&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2024-01-22-behrt/7_table2.png?raw=true&quot; alt=&quot;스크린샷 2024-03-05 오전 10.08.53&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Flexibility of BEHRT from employed 4 key concepts from EHR: disease, age, segment, position
    &lt;ul&gt;
      &lt;li&gt;gains insights about underlying generating process of EHR&lt;/li&gt;
      &lt;li&gt;Distributed/complex representations that are capable of capturing concepts of disease&lt;/li&gt;
      &lt;li&gt;Future work: adding new concepts to the embeddings&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Disease embeddings provide great insights into how various diseases are related to each other: co-occurrence and closeness of diseases
    &lt;ul&gt;
      &lt;li&gt;Could be used for future research as reliable disease vectors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Important features of EHR for prediction&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Robust, gender-specific predictions without inclusion of gender&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Position was important&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Age embeddings might be vital in diagnosing age-related diseases&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>[LearnMRI] Vascular changes related to Alzheimer's dementia observed through MRI</title>
   <link href="https://alatteaday.github.io/study/2023/12/26/mri3/"/>
   <updated>2023-12-26T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2023/12/26/mri3</id>
   <content type="html">&lt;p&gt;When amyloid beta (A$\beta$) deposits, the likelihood of Alzheimer’s dementia increases. Prior to brain damage caused by A$\beta$ deposition, vascular changes are observed, which can be improved through lifestyle changes or medication. Various types of vascular changes that can be observed through &lt;a href=&quot;https://alatteaday.github.io/study/2023/12/26/mri2/&quot;&gt;various forms of MRI&lt;/a&gt; exist.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;choroid-plexus-chp&quot;&gt;Choroid plexus (ChP)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/chp.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MRI type: T1-weighted image (T1WI)&lt;/li&gt;
  &lt;li&gt;ChP: A network of vessels and cells found in the brain’s ventricles.
    &lt;ul&gt;
      &lt;li&gt;Acts as a gateway for immune cells between the blood and the brian.&lt;/li&gt;
      &lt;li&gt;Produces cerebrospinal fluid (CSF) aiding in clearing waste and toxins from brain cells.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The volume of ChP correlates with the severity of cognitive impairment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;perivascular-space-pvs&quot;&gt;Perivascular space (PVS)&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/pvs1.png?raw=true&quot; style=&quot;zoom: 35%;&quot; /&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/pvs2.png?raw=true&quot; style=&quot;zoom: 25%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MRI type: T2-weighted image (T2WI)&lt;/li&gt;
  &lt;li&gt;PVS: Space surrounding arteries penetrating the brain
    &lt;ul&gt;
      &lt;li&gt;Concept encompassing fluids and tissues within and around vessel walls.&lt;/li&gt;
      &lt;li&gt;A component of the blood-brain barrier (BBB).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fluid dynamic
    &lt;ul&gt;
      &lt;li&gt;Acts as a network between cerebrospinal fluid (CSF) and interstitial fluid (ISF), cleansing byproducts of the brain.&lt;/li&gt;
      &lt;li&gt;Known to play a role in the brain’s lymphatic system → glymphatic system.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Enlarged perivascular spaces (EPVS) are associated with degenerative brain diseases and various brain disorders.&lt;/li&gt;
  &lt;li&gt;The volume of PVS is proportional to the likelihood of amyloid beta (A$\beta$) positivity.
    &lt;ul&gt;
      &lt;li&gt;Pronounced tendency in the temporal lobe: A$\beta$ positive patients often have abundant temporal PVS.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;white-matter-hyperintensity-wmh&quot;&gt;White matter hyperintensity (WMH)&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/wmh.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MRI type: FLAIR&lt;/li&gt;
  &lt;li&gt;WMH: An excessive concentration of white matter
    &lt;ul&gt;
      &lt;li&gt;Primarily caused by aging, manifested by loss of blood flow and cellular damage.&lt;/li&gt;
      &lt;li&gt;Subcortical hyperintensity: WMH near the basal ganglia.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;While direct association with Alzheimer’s dementia hasn’t been confirmed, observations suggest a proportional relationship between A$\beta$ positivity and WMH volume in patients.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[LearnMRI] The Types of MRI Modalities and Observable Brain Patterns</title>
   <link href="https://alatteaday.github.io/study/2023/12/26/mri2/"/>
   <updated>2023-12-26T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2023/12/26/mri2</id>
   <content type="html">&lt;p&gt;According to the &lt;a href=&quot;https://alatteaday.github.io/study/2023/12/26/mri/&quot;&gt;Spin echo&lt;/a&gt; technique, T1-weighted images (T1WI) and T2-weighted images (T2WI) can be obtained. By manipulating these images, various MR modality images can be created. Since the signal intensity of lesion tissue varies in each image, the types of lesions emphasized are different.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/t1_t2_flair.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;t1-weighted-image-t1wi&quot;&gt;T1-weighted Image (T1WI)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Spin echo: Both the repetition time (TR) and the echo time (TE) are set short.
    &lt;ul&gt;
      &lt;li&gt;When TR is shortened, the recovery time (T1 realxation time) of $Mz$ varies depending on the tissue, emphasizing the difference. Some tissues have fully recovered, while others have not fully recovered by the time of the second pulse, leading to varying influences from the second pulse. This difference is reflected in the image.&lt;/li&gt;
      &lt;li&gt;TE should be set short to minimize its influence on the T2 relaxation time values.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Signal intensity
    &lt;ul&gt;
      &lt;li&gt;Signal intensity is higher than T2 → anatomical structures are more clearly distinguished.&lt;/li&gt;
      &lt;li&gt;Subcutaneous fat and blood appear hyperintense, which means brighter, while muscles appear intermediate, and water appears hypointense, which means darker.&lt;/li&gt;
      &lt;li&gt;Marrow, being rich in fat, appears hyperintense, while cortex, having less water, appears hypointense.&lt;/li&gt;
      &lt;li&gt;Lesions: Lipoma, acute hemorrhage, lesions containing high protein content (e.g., mucocele)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Observation: Cortical morphology (anatomical detail), vascular changes, blood-brain barrier integrity&lt;/li&gt;
  &lt;li&gt;Feature: Cortical thickness, &lt;a href=&quot;https://alatteaday.github.io/study/2023/12/26/mri3/&quot;&gt;choroid plexus (ChP)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;t2-weighted-image-t2wi&quot;&gt;T2-weighted image (T2WI)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Spin echo: Both TR and TE are set long.
    &lt;ul&gt;
      &lt;li&gt;Lengthening TR minimizes its impact on T1 relaxation time.&lt;/li&gt;
      &lt;li&gt;Longer TE emphasizes the contrast in the extent of $Mxy$ decrease, resulting in different tissue representations in the image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Signal intensity:
    &lt;ul&gt;
      &lt;li&gt;Water appears hyperintense, aiding in the detection of pathological tissues with higher water content, such as lesions.&lt;/li&gt;
      &lt;li&gt;Most lesions appear as low signal intensity(hypointense) on T1 and high signal intensity(hyperintense) on T2.&lt;/li&gt;
      &lt;li&gt;The brightness of water in T2 images varies, with cysts appearing brightest, followed by edema, and then normal tissue.&lt;/li&gt;
      &lt;li&gt;Muscles, fat, and blood appear hypointense.&lt;/li&gt;
      &lt;li&gt;Cerebrospinal fluid (CSF) also appears hyperintense, making it challenging to distinguish lesions, such as Perivascular space (PVS).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Observation: Lesions, hypointense lesions (such as acute hematomas, fungal balls, etc.), arteries (veins show varying signal intensities due to differing blood flow rates)&lt;/li&gt;
  &lt;li&gt;Feature: &lt;a href=&quot;https://alatteaday.github.io/study/2023/12/26/mri3/&quot;&gt;Perivascular space (PVS)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;flair-fluid-attenuation-inversion-recovery&quot;&gt;FLAIR (Fluid Attenuation Inversion Recovery)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CSF is rendered black in T2 images.&lt;/li&gt;
  &lt;li&gt;Non-free-flowing water appears hyperintense, while fat appears hypointense.&lt;/li&gt;
  &lt;li&gt;Observation: Lesions around the ventricles, edema (which appears bright due to stanant fluid), grey-white matter differentiation.&lt;/li&gt;
  &lt;li&gt;Feature: Lesions, &lt;a href=&quot;https://alatteaday.github.io/study/2023/12/26/mri3/&quot;&gt;white matter hyperintensity (WMH)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gre-gradient-echo-t2&quot;&gt;GRE (Gradient Echo; T2*)&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/gre.png?raw=true&quot; style=&quot;zoom: 25%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Paramagnetic substances such as blood, calcium, and metal appear hyperintense, allowing for the observation of iron deposition.&lt;/li&gt;
  &lt;li&gt;Observation: Excellent for detecting microbleeds in early and late brain hemorrhages, as well as diffuse axonal injury.
    &lt;ul&gt;
      &lt;li&gt;*Diffuse axonal injury: One of the components of brain trauma, characterized by axonal damage leadidng to a coma state after trauma.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Feature: bleeding&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>[LearnMRI] Principles and Characteristics of MRI imaging</title>
   <link href="https://alatteaday.github.io/study/2023/12/26/mri/"/>
   <updated>2023-12-26T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/study/2023/12/26/mri</id>
   <content type="html">&lt;p&gt;I’d like to write posts summarizing various aspects of MRI and its modalities, as well as the Alzheimer’s disease-related features observable through MRI as I studied.&lt;/p&gt;

&lt;h2 id=&quot;magnetic-resonance-imaging-mri&quot;&gt;Magnetic Resonance Imaging (MRI)&lt;/h2&gt;

&lt;p&gt;In a device composed of magnets, high-frequency waves are directed at the human body, resonating hydrogen atomic nuclei in the body’s tissues, and converting the differences in signals emanating from each tissue into digital information, resulting in images.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;principles-of-mri-imaging&quot;&gt;Principles of MRI Imaging&lt;/h2&gt;

&lt;p&gt;Human tissues contain a significant amount of water. Hydrogen nuclei within water molecules possess magnetic properties. By emitting high-frequency waves, these hydrogen nuclei can be resonated. When a radiofrequency (RF) pulse is emitted and then turned off (RF pulse), the atomic nuclei absorb and subsequently release the high-frequency signal. Analyzing the differences in the signals returning to the MRI device and maximizing them, a two-dimensional image is formed, which is the essence of MRI.&lt;/p&gt;

&lt;p&gt;The magnitude and waveform of the emitted signal vary depending on factors such as the concentration of water molecules, blood flow, and the binding state with surrounding chemical structures. Consequently, the relaxation times, T1 and T2, differ based on the composition of tissues and blood. Since the composition varies with different diseases, the signals obtained also differ accordingly. By capturing these signal variations, various types of MRI images can be obtained, including &lt;a href=&quot;https://alatteaday.github.io/study/2023/12/26/mri2/&quot;&gt;T1-weighted images (T1WI), T2-weighted images (T2WI), FLAIR, and others&lt;/a&gt;.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-12-25-mri/mr_axis.png?raw=true&quot; style=&quot;zoom: 40%;&quot; align=&quot;center&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;The T1 and T2 relaxation times are measured based on different criteria after applying a 90-degree RF pulse to the protons. When the magnetization of the protons is flipped from the longitudinal axis ($Mz$) to the transverse axis, an $Mxy$ vector is formed. The T1 and T2 relaxation times are measured from the moment when the $Mz$ vector reaches 0% and the $Mxy$ vector reaches 100%.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;T1 relaxation time: The time it takes for $Mz$ to recover up to 63%.
    &lt;ul&gt;
      &lt;li&gt;Recovery is faster in fat, brain tissue, and cerebrospinal fluid (CSF) in that order (shorter T1 relaxation time).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;T2 relaxation time: The time it takes for $Mxy$ to decay down to 37%, relatively unaffected by magnetic field strength.
    &lt;ul&gt;
      &lt;li&gt;Signal decay is faster in fat, brain tissue, and CSF.&lt;/li&gt;
      &lt;li&gt;Tissues with shorter T1 relaxation times also exhibit a rapid decline in the T2 curve.
Water and fat have opposite signal intensities in T1 and T2 (opposite signal intensity).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spin echo is a technique for acquiring images by manipulating the repetition time (TR) and echo time (TE) while applying RF pulses of 90 and 180 degrees. TR is the time from one 90-degree pulse to the next, while TE is the time until the signal is obtained after the 90-degree pulse. By repeating the pulse during image acquisition, various images can be obtained by adjusting TR and TE.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pros-and-cons-of-mri&quot;&gt;Pros and Cons of MRI&lt;/h2&gt;

&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Better contrast of soft tissues compared to CT.&lt;/li&gt;
  &lt;li&gt;Ability to observe anatomical, physiological, and functional information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ferromagnetic artifacts: Even small amounts of ferromagnetic materials in the body can disrupt the homogeneity of the magnetic field, causing distortion in the images.&lt;/li&gt;
  &lt;li&gt;Presence of dental fillings or other inserted materials can reduce image quality.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Contraindications&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MRI should not be used for patients with implants or other materials inside the body that may be affected by the magnetic field.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (NIPS 2020)</title>
   <link href="https://alatteaday.github.io/papers/2023/11/05/rag/"/>
   <updated>2023-11-05T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/11/05/rag</id>
   <content type="html">&lt;p&gt;Lewis, Patrick, et al. “Retrieval-augmented generation for knowledge-intensive nlp tasks.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 33 (2020): 9459-9474.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Retrieval Augmented Generation (RAG)&lt;/strong&gt; model combines a retriever and a generator for enhanced knowledge-intense tasks.&lt;/li&gt;
  &lt;li&gt;RAG Variants: RAG-Sequence uses a single document for output; RAG-Token integrates multiple documents per token.&lt;/li&gt;
  &lt;li&gt;RAG models outperform baselines in open-domain QA, abstractive QA, Jeopardy question generation, and fact verification.&lt;/li&gt;
  &lt;li&gt;RAG models demonstrate practical benefits with easy updates to the non-parametric memory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Large pre-trained Language models (LLMs) store factual knowledge in their parameters, functioning as implicit knowledge base.&lt;/li&gt;
  &lt;li&gt;LLMs, however, have limitations: they cannot expand their memory, provide insight into their predictions, and may produce ‘hallucinations’.&lt;/li&gt;
  &lt;li&gt;Recently, hybrid models, such as REALM and ORQA, address these issues by using a differentiable retriever to revised and expanded knowledge, showing promising results, primarily in open-domain question answering (QA).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig1.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Retrieval-augmented generation (RAG) fine-tunes pre-trained generation models with a non-parametric memory for a general-purpose tasks.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Parametric memory: a pre-trained seq2seq transformer&lt;/li&gt;
  &lt;li&gt;Non-parametric memory: a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.&lt;/li&gt;
  &lt;li&gt;Dense passage retriever (DPR): retrieves latent documents conditioned on the input.&lt;/li&gt;
  &lt;li&gt;BART: the generator conditions on the latent documents together with the input to generate the output. Other seq2seq models like T5 can also be used and fine-tuned with the retriever.&lt;/li&gt;
  &lt;li&gt;Latent documents: marginalized using a top-K approximation, either on a per-output basis or a per-token basis.
    &lt;ul&gt;
      &lt;li&gt;RAG-Sequence Model: assumes the same document is responsible for all tokens.&lt;/li&gt;
      &lt;li&gt;RAG-Token Model: considers different documents for different tokens.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;RAG models use the input sequence $x$ to retrieve text documents $z$ and use them as additional context when generating the target sequence $y$. RAG has two components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Retriever $p_\eta(z\mid x)$: returns distributions over text passages given a query $x$ with parameters $\eta$.
    &lt;ul&gt;
      &lt;li&gt;Truncated as top-K assumtion.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generator $p_\theta(y_i\mid x,z,y_{1:i-1})$: generates a current token based on the previous $i-1$ tokens $y_{1:i-1}$, the input $x$, and a retrieved passage $z$ with parameters $\theta$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The retriever and the generator are trained end-to-end, treating the retrieved document as a latent variable. To marginalize over the latent documents, two methods are proposed, RAG-Sequence and RAG-Token.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;rag-sequence-and-rag-token&quot;&gt;RAG-Sequence and RAG-Token&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;RAG-Sequence Model&lt;/strong&gt; uses the same retrieved document to generate the complete sequence.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The retrieved document is a single latent variable to get the seq2seq probability $p(y\mid x)$ via a top-K approximation.&lt;/li&gt;
  &lt;li&gt;The top-K documents are retrieved using the retriever, and generator produces the output sequence probability for each document.&lt;/li&gt;
&lt;/ul&gt;

\[p_{RAG-Sequence}(y\mid x) \approx \sum_{z\in top-k(p(\cdot|x))}{p_\eta(z|x)p_\theta(y_i|x,z)} \\ = \sum_{z\in top-k(p(\cdot|x))}{p_\eta(z|x)}\prod_i^N p_\theta(y_i|x,z,y_{1:i-1})\]

&lt;ul&gt;
  &lt;li&gt;Use cases: Better suited for tasks where the context of entire documents is crucial, like summarization tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RAG-Token Model&lt;/strong&gt; uses different latent documents for each target token.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The generator chooses content from several documents for the answer.&lt;/li&gt;
  &lt;li&gt;The top-K documents are retrieved using the retriever, and the generator produces a distribution for the next output token for each document before marginalizing.&lt;/li&gt;
&lt;/ul&gt;

\[p_{RAG-Token}(y|x)\approx \prod_i^N \sum_{z\in top-k(p(\cdot\mid x))}p_\eta(z\mid x)p_\theta(y_i\mid x,z_i,y_{1:i-1})\]

&lt;ul&gt;
  &lt;li&gt;Use cases: More suitable for tasks that benefit from integrating detailed information from multiple sources, like open-domain QA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;retriever-and-generator&quot;&gt;Retriever and Generator&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Retriever&lt;/strong&gt; $p_\mu(z\mid x)$ is based on DPR, which follows a bi-encoder architecture:&lt;/p&gt;

\[p_\mu(z|x)\propto \exp(\bf d \rm (z)^\top \bf q \rm (x)) \\
\bf d \rm (z)=\rm BERT_d(z), \ \bf q \rm (x)=\rm BERT_q(x)\]

&lt;ul&gt;
  &lt;li&gt;$\bf d \rm (z)$: a dense representation of a document produced by a document encoder based on $\rm BERT_{BASE}$.&lt;/li&gt;
  &lt;li&gt;$\bf q \rm (x)$: a query representation produced by a query encoder based on $\rm BERT_{BASE}$.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Maximum inner product search (MIPS)&lt;/span&gt;: caculates top-k $p_\eta(\cdot\mid x)$ approximately in sub-linear time.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Non-parametric memory&lt;/span&gt;: the index of the document. The retriever is trained to retrieve documents containing answers to TriviaQA questions and Natural Questions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Generator&lt;/strong&gt; $p_\theta(y_i\mid x,z,y_{1:i-1})$ can be any encoder-decoder model, based on BART in the paper.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\rm BART_{large}$ is used: a pre-trained seq2seq transformer with 400M parameters, pre-trained using a denoising objective with various noising functions.&lt;/li&gt;
  &lt;li&gt;The input $x$ and the retrieved document $z$ are concatenated and then inputted into $\rm BART$ model to generate the output.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Parametric memory&lt;/span&gt;: $\rm BART$ generator parameters $\theta$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;p&gt;The retriever and generator are trained jointly without direct supervision on which document should be retrieved.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Objective: Minimize the negative marginal log-likelihood of each target with a corpus of input/output pairs $(x_j, y_j)$, $\sum_j-\log(p(y_j\mid x_j))$.
    &lt;ul&gt;
      &lt;li&gt;Adam optimizer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fine-tuning only the query encoder $\rm BERT_q$ and the generator $\rm BART$ during training.
    &lt;ul&gt;
      &lt;li&gt;Updating the document encoder $\rm BERT_d$ is costly and ineffective
        &lt;ul&gt;
          &lt;li&gt;Requires periodic updating of the document index (as REALM).&lt;/li&gt;
          &lt;li&gt;Not necessary for strong performance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;decoding&quot;&gt;Decoding&lt;/h2&gt;

&lt;p&gt;For testing, RAG-Sequence and RAG-Token require different methos to approximate $\arg \max_y{p(y\mid x)}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RAG-Sequence model&lt;/strong&gt; utilizes beam search for each document $z$. It can’t be solved with a single beam search, as the likelihood $p(y\mid x)$ does not break into a conventional per-token likelihood.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Each hypothesis of $z$ is scored by $p_\theta(y_i\mid x,z,y_{1:i-1})$.&lt;/li&gt;
  &lt;li&gt;Some hypothesis $y$ included in the set of hypothesis $Y$ may not have appeared in the beams of all documents.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Thorough Decoding&lt;/span&gt;: To estimate the probability of $y$, (1) Run an additional forward pass for each $z$ where $y$ doesn’t appear in the beam, (2) multiply the generator probability with $p_\eta(z\mid x)$, and (3) sum the probabilities across beams.&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff5b1&quot;&gt;Fast Decoding&lt;/span&gt;: For efficient decoding, Approximate $p_\theta(y\mid x,z_i) \approx 0$ where $y$ wasn’t generated during beam search from $x, z_i$, avoiding additional forward passes once the candidate set $Y$ is generated.&lt;/li&gt;
  &lt;li&gt;For longer output sequences, $\left\vert Y \right\vert$ can be large with many forward passes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RAG-Token model&lt;/strong&gt; is a basic autoregressive seq2seq generator with transition probability:&lt;/p&gt;

\[p&apos;_\theta(y_i\mid x,y_{1,i-1})=\sum_{z\in top-k(p(\cdot \mid x))}p_\eta(z_i \mid x)p_\theta(y_i\mid x,z_i,y_{1:i-1})\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;The experiments were conducted on several datasets to evaluate the model’s performance in knowledge-intensive NLP tasks.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Wikipedia December 2018 dump was used as the non-parametric knowledge source.&lt;/li&gt;
  &lt;li&gt;Wikipedia articles were split into 100-word chunks, totaling 21M documents.&lt;/li&gt;
  &lt;li&gt;An embedding for each document was calculated by the document encoder $\rm BERT_d$, and a single MIPS index was built with Hierarchical Navigable Small World approximation for fast retrieval.&lt;/li&gt;
  &lt;li&gt;When retrieving the top $k$ documents for each query, $k\in {5,10}$ was considered for training, and set using dev data for test time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Open-domain Question Answering (QA)&lt;/strong&gt;: an important real-world application and common testbed for knowledge-intensive tasks.
    &lt;ul&gt;
      &lt;li&gt;Text pairs $(x,y)$ are matched as questions and answers.&lt;/li&gt;
      &lt;li&gt;RAG is trained to minimize the negative log-likelihood of answers.&lt;/li&gt;
      &lt;li&gt;Close-book QA is also a compared task: generating answers without retrieving but purely with parametric knowledge.&lt;/li&gt;
      &lt;li&gt;Datasets: Natural Questions, TriviaQA, WebQuestions, CuratedTREC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Abstractrive Question Answering&lt;/strong&gt;: tests natural language generation (NLG) ability with free-form and abstractive cases.
    &lt;ul&gt;
      &lt;li&gt;Use MSMARCO NLG Task v2.1: only the question and answers, not existing gold passages in the dataset, treated as an open-domain abstractive QA task.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Jeopardy Question Generation&lt;/strong&gt;: evaluates the generation ability in a non-QA setting.
    &lt;ul&gt;
      &lt;li&gt;Jeopardy: guessing an entity from a fact about that entity.
        &lt;ul&gt;
          &lt;li&gt;e.g., “In 1986 Mexico scored as the first contry to host this international sport competition twice.” where the answer is “The World Cup”.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Jeopardy questions are precise and factual, making it a challenging, knowledge-intensive task to generate them conditioned on the anser entities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fact Verification&lt;/strong&gt; (FEVER): a retrieval problem coupled with an challenging entailment reasoning task.
    &lt;ul&gt;
      &lt;li&gt;Requires classifying whether a text is supported or refuted by Wikipedia or whether there’s not enough information to decide.&lt;/li&gt;
      &lt;li&gt;Provides an appropriate testved for exploring a model’s ability to handle classification rather than generation.&lt;/li&gt;
      &lt;li&gt;Two varients: the 3-way classification (supports/refutes/not enough) and the 2-way (support/refutes).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The results demonstrated that both RAG-Sequence and RAG-Token models outperformed baseline models across various datasets and tasks.&lt;/p&gt;

&lt;h2 id=&quot;open-domain-qa&quot;&gt;Open-Domain QA&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table1_2.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RAG models significantly outperformed the baselines, showing higher EM and F1 scores.&lt;/li&gt;
  &lt;li&gt;The RAG-Token model, in particular, performed well due to its ability to integrate detailed information from multiple documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstractive-question-answering&quot;&gt;Abstractive Question Answering&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table3.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RAG models achieved SOTA performance, even though many questions are unanswerable without the gold passages.&lt;/li&gt;
  &lt;li&gt;RAG models hallucinated less and generated more factually correct and diverse text compared to BART (Table 3).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;jeopardy-question-generation&quot;&gt;Jeopardy Question Generation&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table4_5.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/fig2.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both of RAG models outperformed BART on Q-BLEU-1 (Table 2).&lt;/li&gt;
  &lt;li&gt;Human evaluators indicate that RAG-generated content was more factual in 42.7% of cases, demostrating the effectiveness of RAG over the SOTA generation model (Table 4).&lt;/li&gt;
  &lt;li&gt;RAG-Token model performed better than RAG-Sequence, combining content from several documents effectively (Fig 2).&lt;/li&gt;
  &lt;li&gt;The generator’s the parametric knowledge sufficed to complete the generation after initially referencing the document (Fig 2).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fact-verification&quot;&gt;Fact Verification&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;For 3-way classification, RAG achieved scores within 4.3% of SOTA models trained with intermediate retrieval supervision for a specific domain.&lt;/li&gt;
  &lt;li&gt;For 2-way classification, RAG achieved performance within 2.7% of the base model, SotA, which were trained to classify true of false given the gold evidences.&lt;/li&gt;
  &lt;li&gt;The documents retrieved by RAG are overlapped significantly with FEVER’s gold evidence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;additional-results&quot;&gt;Additional Results&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generation Diversity&lt;/strong&gt;: When investigating generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models, RAG models generated more diverse outputs compared to BART. RAG-Sequence produced slightly more diverse outputs than RAG-Token (Table 5).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Retrieval Ablations&lt;/strong&gt;: Freezing the retriever during training resulted in lower performance compared to the original RAG models. Replacing the retriever with a BM25 system showed that learned retrieval improved performance for all task (table 6).
    &lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table6.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Index hot-swapping&lt;/strong&gt;: Demonstrated the advantage of non-parametric memory by using an index from Wikipedia dump from December 2016. RAG models still answered 70% of questions correctly, showing that knowledge can be updated simply by replacing the non-parametric memory.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Effect of Retrieving more documents&lt;/strong&gt;: Adjusting the number of retrieved documents at test time showed improved performance up to a certain point, demonstrating the benefits of retrieveing more relevant documents (fig 3).
    &lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-11-05-rag/table6.png?raw=true&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] Self-Supervised Transformer for Sparse and Irregularly Sampled Multivariate Clinical Time-Series (ACM 2022)</title>
   <link href="https://alatteaday.github.io/papers/2023/09/15/STraTS/"/>
   <updated>2023-09-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/09/15/STraTS</id>
   <content type="html">&lt;p&gt;Tipirneni, Sindhu, and Chandan K. Reddy. “Self-supervised transformer for sparse and irregularly sampled multivariate clinical time-series.” &lt;em&gt;ACM Transactions on Knowledge Discovery from Data (TKDD)&lt;/em&gt; 16.6 (2022): 1-17.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/full/10.1145/3516367&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;points&quot;&gt;Points&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Self-supervised Transformer for Time-Series (STraTS) model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Using observation triplets as time-series components: avoids the problems faced by aggregation and imputation methods for sparse and sporadic multivariate time-series&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Continuous Value Embedding: encodes continuous time and variable values without the need for discretization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Transformer-based model: learns contextual triplet embeddings&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Time series forecasting as a proxy task: leverages unlabeled data to learn better generalized representations&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;Problems&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multivariate time-series data are frequently observed in critical care settings and are typically characterized by sparsity (missing information) and irregular time intervals.&lt;/li&gt;
  &lt;li&gt;Existing approaches, such as aggregation or imputation of values, suppress the fine-grained information and add undesirable noise/overhead into the model.&lt;/li&gt;
  &lt;li&gt;The problem of limited availability of labeled data is easily observed in healthcare applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The clinical domain portrays a unique set of challenges:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Missingness and Sparsity: Not all the variables are observed for every patient. Also, the time-series matrices are very sparse.&lt;/li&gt;
  &lt;li&gt;Irregular time intervals and Sporadicity: Not all clinical variables are measured at regular time intervals. The measurements may occur sporadically in time depending.&lt;/li&gt;
  &lt;li&gt;Limited labeled data: expensive and even more limited for specific tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Existing methods&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Aggregation: could suppress important fine-grained information&lt;/li&gt;
  &lt;li&gt;Imputation/Interpolation: not reasonable as not considering the domain knowledge about each variable&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Self-supervised Transformer for Time-Series (STraTS)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/1_fig3.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Triplet Emgeddings&lt;/strong&gt; = Feature embedding + Value embedding + Time embedding
\(T=\{(t_i, j_i, u_i)\}^n_{i=1}\\
e_i=e_i^f+e_i^v+e_i^t\)
&lt;strong&gt;Continuous Value Embedding (CVE)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For continuous values of feature values and times&lt;/p&gt;

&lt;p&gt;A one-to-many Feed-Forward Network
\(FFN(x) = U tanh(Wx+b)\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature embeddings $e_i^f$: obtained from a simpole lookup table&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Value embeddings $e_i^v$ and Time embeddings$e_i^t$: through CVE&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Demographics Embedding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;the prediction models performed better when demographics were processed separately.
\(e^𝑑 = 𝑡𝑎𝑛ℎ(W^𝑑_2𝑡𝑎𝑛ℎ(W^𝑑_1d + b^𝑑_1) + b^𝑑_2) ∈ R^d\)
where the hidden layer has a dimension of 2d&lt;/p&gt;

&lt;h3 id=&quot;self-supervision&quot;&gt;Self-Supervision&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/2_fig2.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pre-training Tasks: Both masking and forecasting as pretext tasks for providing self-supervision&lt;/p&gt;

&lt;p&gt;The forecasitng improved the results on target tasks&lt;/p&gt;

&lt;p&gt;The loss is:
\(L_{ss}=\frac{1}{|N&apos;|}\sum_{k=1}^{N&apos;}\sum_{j=1}^{|F|}m_j^k\Big(\tilde{z}_j^k-z_j^k\Big)^2\)&lt;/p&gt;

&lt;h3 id=&quot;interpretability&quot;&gt;Interpretability&lt;/h3&gt;

&lt;p&gt;I-STraTS: an interpretable version of STraTS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The output can be expressed using a linear combination of components that are derived from individual features&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Differences with STraTS&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Combine the initial triplet embeddings in Fusion Self-attention module&lt;/li&gt;
  &lt;li&gt;Directly use the raw demographics vector as the demographics embedding&lt;/li&gt;
&lt;/ul&gt;

\[\tilde{y}=sigmoid\Big(\sum_{j=1}^{D}{\bold{w}_0[j]d[j]+\sum_{i=1}^{n}\sum_{j=1}^{d}\alpha_i\bold{w}_o[j+D]\bold{e}_i[j]+b_o}\Big)\]

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;Target Task: Prediction of in-hospital mortality&lt;/p&gt;

&lt;p&gt;Datasets: 2 EHR datasets; MIMIC-III and PhysioNet Challenge 2012&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MIMIC-III: 46,000 patients&lt;/li&gt;
  &lt;li&gt;PhysioNet-2012: 11,988 patients&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Baselines: Gated Recurrent Unit (GRU), Temporal Convolutional Network (TCN), Simply Attend and DIagnose (SaND), GRU with trainable Decays (GRU-D), Interpolation-prediction Network (InterpNet), Set Functions for Time Series (SeFT)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Used 2 dense layers for demographics encoding&lt;/li&gt;
  &lt;li&gt;Concatenated it to the time-series representation before the last dense layer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Metrics&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ROC-AUC: Area under ROC curve&lt;/li&gt;
  &lt;li&gt;PR-AUC: Area under precision-recall curve&lt;/li&gt;
  &lt;li&gt;min(Re, Pr): the max of ‘min of recall and precision’ across all thresholds&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prediction-performance&quot;&gt;Prediction Performance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/3_table4.png?raw=true&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trained each model using 10 different random samplings of 50% labeled data from the train and validation sets&lt;/li&gt;
  &lt;li&gt;STraTS uses the entire labeled data and additional unlabeled data if avaliable&lt;/li&gt;
  &lt;li&gt;STraTS achieves the best performance&lt;/li&gt;
  &lt;li&gt;GRU showed better performance than interpolation-based models (GRU-D, InterpNet) on the MIMIC-III dataset, which was not expected&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Generalizability test of models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Lower propotions of labeled data can be observed in real-world when there are several right-censord samples.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/4_fig5.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;STraTS has an advantage compared to others in scarce labeled data settings, which can be attributed to self-supervision&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h3&gt;

&lt;p&gt;Compared STraTS and I-STraTS with and without self-supervision: ‘ss+’ and ‘ss-‘ indicate each case&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/5_table5.png?raw=true&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I-STraTS showed slightly worse performance as constrained its representations&lt;/li&gt;
  &lt;li&gt;Adding self-supervision improves performance of both models&lt;/li&gt;
  &lt;li&gt;I-STraTS(ss+) outperforms STraTS(ss-): self-supervision can compensate the performance which could get lower by introducing interpretability&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;interpretability-1&quot;&gt;Interpretability&lt;/h3&gt;

&lt;p&gt;How I-STraTS explains its predictions&lt;/p&gt;

&lt;p&gt;A case study: a 85 yrs old female patient from MIMIC-III&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;expired on the 6th day after ICU admission&lt;/li&gt;
  &lt;li&gt;had 380 measurements corresponding to 58 time-series variables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model predicts the probability of her in-hospital mortality as 0.94 using only the data collected the first day&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-09-15-strats/6_table6.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Average contribution score: the average score along with the range, for multiple observations, or value, for only one observation&lt;/li&gt;
  &lt;li&gt;The top 5 variables are the most important factors in predicting she ‘s at high risk of mortality that the model observed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;amp;rarr Can be helpful to identify high-risk patients and also understand the contributing factors and make better diagnoses, especially at the early stages of treatment&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Study] 알츠하이머 치매의 ATN 바이오마커 간 관계 정리</title>
   <link href="https://alatteaday.github.io/study/2023/08/29/demensiapapers/"/>
   <updated>2023-08-29T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2023/08/29/demensiapapers</id>
   <content type="html">&lt;h2 id=&quot;ai-전공자의-알츠하이머-치매-관련-brain-imaging-논문-스터디&quot;&gt;AI 전공자의 알츠하이머 치매 관련 Brain Imaging 논문 스터디&lt;/h2&gt;

&lt;p&gt;Amyloid Beta(A), Tau(T), Neurodegeneration(N)과 관련된 Alzheimer’s Disease(AD) 기전에 대해 이해하기 위하여 다음의 논문들을 읽고 정리한 내용입니다. 기반 지식이 없어 시각 자료와 사전을 찾아가며 읽었습니다. pdf는 찾아본 이미지와 필기한 내용이 담긴 논문 파일입니다.&lt;/p&gt;

&lt;p&gt;Ittner, Lars M., and Jürgen Götz. “Amyloid-β and tau—a toxic pas de deux in Alzheimer’s disease.” &lt;em&gt;Nature Reviews Neuroscience&lt;/em&gt; 12.2 (2011): 67-72. &lt;a href=&quot;https://www.nature.com/articles/nrn2967&quot;&gt;link&lt;/a&gt; &lt;a href=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-08-29-demensiapapers/nrn2967.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Vogel, Jacob W., et al. “Four distinct trajectories of tau deposition identified in Alzheimer’s disease.” &lt;em&gt;Nature medicine&lt;/em&gt; 27.5 (2021): 871-881. &lt;a href=&quot;https://www.nature.com/articles/s41591-021-01309-6&quot;&gt;link&lt;/a&gt; &lt;a href=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-08-29-demensiapapers/s41591-021-01309-6.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Lee, Wha Jin, et al. “Regional Aβ-tau interactions promote onset and acceleration of Alzheimer’s disease tau spreading.” &lt;em&gt;Neuron&lt;/em&gt;110.12 (2022): 1932-1943. &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/35443153/&quot;&gt;link&lt;/a&gt; &lt;a href=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-08-29-demensiapapers/regional_onset.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Amyloid Beta(A)는 뉴런에 의해 생성되는 Amyloid Precursor Protein(APP)이 프로테아제에 의해 4부분으로 나눠질 때 생기는 펩타이드 중 하나로,&lt;/p&gt;

&lt;p&gt;뉴런 근처에 존재하여 기능 장애를 야기하는 것으로 알려졌다. A의 침착은 Alzheimer’s Disease(AD) 발병 10-20년 전부터 이뤄진다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A는 dimers, oligomers, fibrils 등에 이어 plaque를 형성한다. A가 어느 형태에서 toxicity를 갖기 시작하는지는 확실하지 않다. 항 아밀로이드 치매 치료제는 이 plaque의 감소와 증식 및 생성 방지를 목적으로 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A의 toxicity는 postsynaptic compartment, 즉 dendrite(somatodendritic region)를 주 대상으로 하여 작용하고, 특정 수용체의 속성에 따라 세포막을 통해 간접적으로 뉴런에 영향을 끼칠 수 있다. 대표적인 특정 수용체로 NMDAR이 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tau(T)는 신경 세포에서 microtubule과 결합하는 단백질로, 주로 axon에 존재하여 microtubule의 안정화 및 axonal transition을 조절하는 역할을 한다.&lt;/p&gt;

&lt;p&gt;정상 상태의 뉴런의 dendrite에도 소량 존재한다.&lt;/p&gt;

&lt;p&gt;T는 A에 의해 과인산화되고(hyperphosphorylated Tau), 과인산화된 T는 Neurofibrillary Tangle(NFT)를 형성한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;T의 과인산화는 microtubule 형성을 방해하여 뉴런의 기능을 방해한다.&lt;/li&gt;
  &lt;li&gt;NFT는 Somatodendritic region에서 많이 관찰된다. T의 level이 높아지면 T가 dendrite에서 많이 관찰된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dendrite에서 T는 그곳에 위치한 여러 단백질과 상호작용하여 결과적으로 뉴런이 A의 toxicity에 약해지게 만든다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;T가 인산화되면 Tyrosine protein kinasen FYN과 강하게 작용한다. 과인산화된 T가 dendrite에서 증가함에 따라 FYN도 Soma에서 증가한다.&lt;/li&gt;
  &lt;li&gt;FYN은 NMDAR을 인산화한다. 인산화된 NMDAR은 Postsynaptic Density Protein 95(PDS95)와 상호작용한다.&lt;/li&gt;
  &lt;li&gt;이것의 결과로 NMDAR의 excitotoxicity가 나타난다(흥분독성상태). 수용체의 excitotoxicity로 A의 toxicity에 뉴런이 민감해지게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결과적으로 A와 T는 뉴런을 약화시키는 데에 있어 서로 시너지를 갖는다. A는 T의 과인산화를 촉진하고, 과인산화된 T는 뉴런이 A의 toxicity에 약해지게 만든다.&lt;/p&gt;

&lt;p&gt;이 시스템에서 A와 T는 세포의 다른 부분(각각 Complex I, Complex IV)에 악영향을 끼쳐 미토콘드리아 호흡을 방해하고, 결국 Neurodegeneration(N)을 야기한다.&lt;/p&gt;

&lt;p&gt;따라서 A의 침착과 T의 전파는 AD의 중요한 요인이다.&lt;/p&gt;

&lt;p&gt;T의 전파 양상은 Braak Staging System으로 체계화된 바 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transentorhinal cortex → medial and basal temporal lobe → neocortical associative regions → unimodal sensory and motor cortex&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그런데 이 system에 부합하지 않는 전파 양상 또한 관찰되었다. T의 전파 양상을 병의 진행과 뇌 영역의 시공간적 기준으로 분류하여 4가지 subtype으로 정의할 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;S1 limbic (Braak system), S2 MTL, S3 posterior, S4 Lateral Temporal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;침착된 A는 T의 전파에 영향을 준다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A는 heteromodal association cortex에 침착되고, T의 전파는 entorhinal cortex(EC)에서 시작되어 점차 뇌 전반으로 퍼진다. ( ← Braak system; S1 type ? )&lt;/li&gt;
  &lt;li&gt;Remote Interation: A와 T가 같은 영역에 있지 않은 상태에서, 먼저 A가 연결된 뉴런을 통해서 EC 영역에 있는 T에 영향을 준다. A의 영향으로 T는 점차 주위 영역으로 확산된다.&lt;/li&gt;
  &lt;li&gt;Local Interaction: T가 A와 직접적으로 접촉되어 있는 뉴런에 전파되어 만남으로서 T의 전파가 가속화된다(acceleration). 해당 뇌 영역은 Internal Temporal Gyrus(ITG)이다 (propagation hub).&lt;/li&gt;
  &lt;li&gt;T 전파의 acceleration이 진행되면 뇌 전반에서 A와 T의 상호작용이 일어나게 되어 N과 AD의 악화를 막기 어렵다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A와 T의 PET 데이터와 MRI 데이터를 병의 진행에 따라 살펴보면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A의 침착되는 정도는 뇌 전반에 걸쳐 점점 심해질 것이고&lt;/li&gt;
  &lt;li&gt;T는 뇌의 특정 부분에서 시작하여 점차 확산되는 양상으로 관찰되고&lt;/li&gt;
  &lt;li&gt;T의 슈퍼 전파가 관찰된 이후 MRI 상 전반적인 뇌 위축(N)의 정도가 심하게 나타날 것이다.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Recurrent Neural Networks (RNNs)</title>
   <link href="https://alatteaday.github.io/study/2023/05/20/rnn/"/>
   <updated>2023-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2023/05/20/rnn</id>
   <content type="html">&lt;p&gt;Sequence data를 분석하기 위한 딥러닝 모델 구조로, Rumelhart et al., 1986에 근간을 둔다. Deep neural networks (DNNs)와는 달리 hidden state node 간 연결을 통해 이전 시점의 정보를 현재 시점에서 사용할 수 있게 디자인되었다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn.jpg?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;현재 시점 node인 $s_t$에 전 시점의 $s_{t-1}$ node에서 정보가 들어온다. 이 정보를 현재 시점의 입력인 $x_t$와 함께 받아 다음 node $s_{t+1}$로 전송될 값을 계산한다. 이 작업을 회귀적으로(recurrently) 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;weight-sharing&quot;&gt;Weight sharing&lt;/h2&gt;

&lt;p&gt;Weight $U$, $W$, $V$는 모든 시점에서 동일하다. 이것으로&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;학습에 필요한 weight 수를 줄일 수 있다.&lt;/li&gt;
  &lt;li&gt;데이터의 sequence 길이에 유연하다: 하나의 모델을 다른 길이의 sequence에 적용할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;다른 길이의 sequence에 같은 weight값을 계속 사용함으로써 next token generation이 가능하다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;rnn-계산&quot;&gt;RNN 계산&lt;/h2&gt;

&lt;p&gt;위 그림을 보면 hidden state $s_t$와 output $o_t$의 계산은 다음과 같다.&lt;/p&gt;

\[\begin{align*}
s_t&amp;amp;=\tau(Ws^{t-1})+Ux^t \\
o_t&amp;amp;=softmax(Vs^t) \\
\end{align*}\]

&lt;p&gt;여기서 node 수가 $D$, $J$, $K$인 경우 각 변수의 차원은 아래와 같다.&lt;/p&gt;

\[x\in\mathbb{R}^D, s\in\mathbb{R}^J, o\in\mathbb{R}^K, U\in\mathbb{R}^{J\times D}, W\in\mathbb{R}^{J\times J}, U\in\mathbb{R}^{K\times J}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;long-term-dependency-problem&quot;&gt;Long-term dependency problem&lt;/h2&gt;

&lt;p&gt;hidden state 연산은 다음과 같이 표현할 수 있다.&lt;/p&gt;

\[s^t=\tau(Ux^t+W\tau(Ux^{t-1}+Ws^{t-2}))\]

&lt;p&gt;$s$가 tanh activation function 내에서 중첩되는 것을 볼 수 있다. 이렇게 되면&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;feed forward 시 앞 단에서 입력된 정보가 점점 소실된다: tanh의 output은 $\tau(\cdot)\in(-1,1)$인데, 즉 tanh 연산의 중첩은 1보다 작은 값을 계속해서 곱하는 것과 같다. 이렇게 되면 앞에서 곱해진 값은 점점 작아진다.&lt;/li&gt;
  &lt;li&gt;back-propagation 시 기울기 소실(gradient vanishing) 혹은 폭발(explosion)의 문제가 생길 수 있다: tanh 함수에 의해 기울기가 0에 가깝게 되거나 너무 커지는 경우가 생긴다. 작은 gradient는 더 작아지고, 큰 gradient는 더 커진다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;*Gradient vanishing: back-propagation 시 반영되는 gradient 값이 layer를 지날 수록 소실되는 문제&lt;/p&gt;

&lt;p&gt;*Gradient explosion: gradient가 실제 값보다 증폭되어 loss 계산 시 정답과의 차이가 너무 커져, 업데이트에 과도하게 반영되는 문제&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;다양한-rnn-구조&quot;&gt;다양한 RNN 구조&lt;/h2&gt;

&lt;p&gt;입출력 형태에 따라 다양하게 RNN을 구성할 수 있다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-rnn/rnn_types.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One-to-One: hidden state가 1개인 모형, 기본적인 Neural network 구조&lt;/li&gt;
  &lt;li&gt;One-to-Many: 하나의 입력값을 받아 순차적으로 여러 개의 값(한 sequence)을 생성&lt;/li&gt;
  &lt;li&gt;Many-to-One: 한 sequence를 입력 받아 마지막에 하나의 값을 생성
    &lt;ul&gt;
      &lt;li&gt;e.g. sentence classification&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Many-to-Many: 한 sequence를 입력 받아 latent represenation을 구한 후 이것을 통해 sequence를 출력
    &lt;ul&gt;
      &lt;li&gt;e.g. machine translation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many-to-Many: 한 sequence의 매 token을 입력 받는대로 대응하는 token을 출력하여 한 seqeunce를 생성&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Long Short-term Memory (LSTM)</title>
   <link href="https://alatteaday.github.io/study/2023/05/20/lstm/"/>
   <updated>2023-05-20T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/study/2023/05/20/lstm</id>
   <content type="html">&lt;p&gt;Recurrent neural network (RNN)의 Long-term dependency 문제를 해결하고자 만들어진 프레임워크이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-20-lstm/lstm.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;핵심적인 아이디어는 이전 시점의 state 정보를 이후 state에 얼마나 반영할지를 결정하는 계산을 추가해주는 것이다. 이것을 위해 &lt;strong&gt;forget gate, input gate, output gate&lt;/strong&gt;의 3가지 Gate와 &lt;strong&gt;memory cell&lt;/strong&gt;이 추가 되었다.&lt;/p&gt;

&lt;p&gt;LSTM의 계산 방식과 비교하기 위해 RNN의 계산식을 되짚어보면 다음과 같다.&lt;/p&gt;

\[\begin{align*}
h_t&amp;amp;=\tau(Wh^{t-1}+Ux^t) \\
\hat{y}_t&amp;amp;=softmax(Vh^t) \\
\end{align*}\]

&lt;p&gt;이전 시점($t-1$)의 hidden state와 현재 시점($t$)의 input(둘 다 weighted)을 더하여 tanh를 통과시키면 현재 시점의 hidden state가 된다. 이것에 softmax를 취하면 output이 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lstm-계산식&quot;&gt;LSTM 계산식&lt;/h2&gt;

&lt;p&gt;LSTM은 RNN의 방식에 residual connection 구조에서 착안한 memory 기능을 더하여 long-term dependency 문제를 해결하고자 했다. Memory 기능을 위해 추가된 계산들은 다음과 같다.&lt;/p&gt;

&lt;h3 id=&quot;forget-gate&quot;&gt;Forget gate&lt;/h3&gt;

&lt;p&gt;Forget gate $f$는 이전 시점의 정보를 얼마나 잊을지 결정하는 gate이다.&lt;/p&gt;

\[f_t=\sigma(W_fh_{t-1}+U_fx_t)\]

&lt;p&gt;이전 시점의 hidden state와 현재 시점의 input을 더한 뒤 sigmoid를 취한다. 이것이 이전 시점의 memory cell state에 곱해진다. sigmoid의 특성에 의해 1에 가까울수록 이전 정보가 이후 많이 반영된다.&lt;/p&gt;

&lt;h3 id=&quot;input-gate&quot;&gt;Input gate&lt;/h3&gt;

&lt;p&gt;Input gate $i$는 현재 시점의 input을 다음 시점에 얼마나 반영할지 결정하는 gate이다. 여기서 candidate $\hat(c)$라는 개념이 등장하는데, candidate는 이전 시점의 hidden state와 현재 시점의 input을 고려했을 때 현재의 정보가 어떠한지를 나타내는 cell state의 후보 격인 값이다. 계산 방식이 RNN의 hidden state와 동일하다. input gate 값과 candidate를 곱해 현재의 정보 상 input이 얼마나 반영되면 좋은지를 구하고, 이것을 최종적으로 cell state에 더한다.&lt;/p&gt;

\[\begin{align}
i_t&amp;amp;=\sigma(W_{in}h_{_t-1}+U_{in}x_{t})\\
\hat{C}_t&amp;amp;=\tau(W_{c}h_{t-1}+U_{c}x_t)
\end{align}\]

&lt;h3 id=&quot;memory-cell&quot;&gt;Memory cell&lt;/h3&gt;

&lt;p&gt;Memory cell (cell state)은 세 가지 gate와 함께 LSTM의 구현 목적을 위해 추가된 개념이다. 현재 시점의 cell state는 이전 시점의 cell state 및 현재 시점의 forget gate와 현재 시점의 input gate 및 candidate로 계산한다.&lt;/p&gt;

\[C_t=f_t*C_{t-1}+i_t*\hat{C}_t\]

&lt;p&gt;$*$는 pointwise operation&lt;/p&gt;

&lt;p&gt;이전 정보인 cell state와 현재 input을 얼마나 반영할지가 합해져 현재 시점의 cell state가 구해진다.&lt;/p&gt;

&lt;h3 id=&quot;output-gate&quot;&gt;Output gate&lt;/h3&gt;

&lt;p&gt;Output gate는 memory cell을 현재 시점의 hidden state에 얼마나 반영할지 결정한다.&lt;/p&gt;

\[\begin{align}
o_t&amp;amp;=\sigma(W_oh_{t-1}+U_ox_t) \\
h_t&amp;amp;=o_t\tau(C_t) \\
&amp;amp;=o_t\tau(f_t*C_{t-1}+i_t*\hat{C}_t) \\
\end{align}\]

&lt;p&gt;현재 시점의 hidden state는 이전 시점의 정보와 현재 시점의 input이 반영된 현재 시점의 cell state와 output gate의 결과값과 곱해져 최종 결정된다.&lt;/p&gt;

&lt;h3 id=&quot;output&quot;&gt;Output&lt;/h3&gt;

&lt;p&gt;최종 출력 $\hat{y}_t$은 RNN과 같이 계산된다.&lt;/p&gt;

\[\hat{y}_t=softmax(Vh_t)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lstm의-한계&quot;&gt;LSTM의 한계&lt;/h2&gt;

&lt;p&gt;LSTM은 cell state 도입을 통해 gradient vanishing 문제를 해결하고자 하였다. 하지만 RNN 구조를 기반으로 하고 있는 한 이 문제를 완벽하게 해결하기에 한계가 있다. 오히려 gate를 여러 개 사용하여 계산량이 증가하는 문제가 있다.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Alpaca: A Strong, Replicable Instruction-Following Model</title>
   <link href="https://alatteaday.github.io/papers/2023/05/15/alpaca/"/>
   <updated>2023-05-15T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/05/15/alpaca</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://crfm.stanford.edu/2023/03/13/alpaca.html&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Alpaca aims to support academic research on instruction-following large language models (LLMs), addressing deficiencies like hallucinations, toxicity, and biases.&lt;/li&gt;
  &lt;li&gt;Uses the self-instruct approach to create an instruction-following dataset with text-davinci-003, costing under $500.&lt;/li&gt;
  &lt;li&gt;The LLaMA 7B model is fine-tuned using efficient techniques.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;LLMs trained through instruction-following, such as ChatGPT, have significantly impacted daily life. However, these models still face issues like generating misinformation, toxic content, and exhibiting social biases. To address these problems, academic research is essential. Closed-source models hinder this research, making it difficult to study instruction-following models.&lt;/p&gt;

&lt;p&gt;Alpaca is a model designed for academic research, fine-tuned from the LLaMA 7B model using 52k instruction-following data generated from OpenAI’s text-davinci-003. Commercial use of Alpaca is prohibitied by following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Non-commercial license: LLaMA&lt;/li&gt;
  &lt;li&gt;Data restrictions: Based on text-davinci-003 prohibiting competition with OpenAI&lt;/li&gt;
  &lt;li&gt;Deployment caution: Not designed with adequate safety mesuares for general use.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;training-recipe&quot;&gt;Training Recipe&lt;/h1&gt;

&lt;p&gt;To train a high-quality instruction-following model under an academic budget, two key challenges are addressed:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Strong pre-trained language model: LLaMA models&lt;/li&gt;
  &lt;li&gt;High-quality instruction-following data: Self-instruct method&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;self-instruct-method&quot;&gt;Self-instruct method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Seed set: 175 human-written instruction-following output pairs from self-instruct seed set.&lt;/li&gt;
  &lt;li&gt;Data generation: Prompting text-davinci-003 to generate more instructions using the seed set as examples.&lt;/li&gt;
  &lt;li&gt;Efficiency: Improved the self-instruct method, generating 52k unique instructions and outputs for less than $500 using the OpenAI API.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-15-alpaca/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;fine-tuning-the-model&quot;&gt;Fine-tuning the model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Process: LLaMA models are fine-tuned with the generated instruction-following dataset using fully shared data parallel (FSDP) and mixed precision trianing.&lt;/li&gt;
  &lt;li&gt;Cost and time: Fine-tuning a 7B LLaMA model took 3 hours on eight 80GB A100s, costing less than $100 on most cloud compute providers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;preliminary-evaluation&quot;&gt;Preliminary Evaluation&lt;/h1&gt;

&lt;p&gt;Human evaluation was conducted on inputs from the self-instruct evaluation set. Key findings include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Comparison: Alpaca 7B vs. text-davinci-003&lt;/li&gt;
  &lt;li&gt;Performance: Alpaca wins 90 to 89 comparisons.
    &lt;ul&gt;
      &lt;li&gt;Given Alpaca’s smaller size and limited data, it performed similarly to text-davinci-003.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Generation style: Alpaca’s outputs tend to be similar with text-davinci-003, and reflect the general style of the training dataset.&lt;/li&gt;
  &lt;li&gt;Evaluation limitation: The evaluation data’s limitations should be noted.&lt;/li&gt;
  &lt;li&gt;An interactive demo was released to gather further feedback.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;known-limitiations&quot;&gt;Known Limitiations&lt;/h1&gt;

&lt;p&gt;Alpaca shares common deficiencies with LLMs, such as hallucinations, toxicity, and stereotypes. It struggles particularly with hallucination, sometimes producing well-written misinformation. Despite these issues, Alpaca provides a lightweight model for studying these deficiencies, aiding academic research.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;release&quot;&gt;Release&lt;/h1&gt;

&lt;p&gt;Released assets:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Demo: Interactive demo for evaluation&lt;/li&gt;
  &lt;li&gt;Data: 52k demonstrations used to fine-tune Alpaca&lt;/li&gt;
  &lt;li&gt;Data generation process: Code for generating the data&lt;/li&gt;
  &lt;li&gt;Training code: Fine-tuning code using Hugging Face API&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Future release:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Model weights: Pending guidance from Meta&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The release aims to support academic studies on instruction-following LMs and developing new technique to address the existing deficiencies.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] Llama: Open and efficient foundation language models (2023)</title>
   <link href="https://alatteaday.github.io/papers/2023/05/10/llama/"/>
   <updated>2023-05-10T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/05/10/llama</id>
   <content type="html">&lt;p&gt;Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.” &lt;em&gt;arXiv preprint arXiv:2302.13971&lt;/em&gt; (2023).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;points&quot;&gt;Points&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Efficient inference with smaller models: LLaMA models prioritize inference efficiency by using smaller models trained on large datasets, achieving state-of-the-art (SOTA) performance across benchmarks while being cost-effective during inference.&lt;/li&gt;
  &lt;li&gt;Publicly available data: Unlikely many existing models that rely on proprietary data, LLaMA models are exclusively trained on publicly available datasets, ensuring transparency and compatibility with open-source principles.&lt;/li&gt;
  &lt;li&gt;Broad Benchmark Performance: LLaMA models demonstrate competitive performance on a wide range of tasks, including common sense reasoning, question answering, reading comprehension, etc,.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;Large language models (LLMs) have demonstrated remarkable capabilities in performing new tasks with minimal instruction or examples, thanks to their vast size. However, recent research suggests that smaller models trained on larger datasets can achieve superior performance, highlighting the importance of efficiency during inference rather than training.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;approach&quot;&gt;Approach&lt;/h1&gt;

&lt;p&gt;LLaMA is a series of language models (LMs) designed to optimize performance across various inference budgets, ranging from 7B to 65B parameters, using only publicly available data.&lt;/p&gt;

&lt;h2 id=&quot;pre-training-data&quot;&gt;Pre-training data&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table1.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;The dataset mixture used cover diverse domains and is entirely publicly avaliable, ensuring compatibility with open-source principles:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;English CommonCrawl [67%]: Preprocessed from five CommonCrawl dumps (2017-2020)., filtered for non-English and low-quality content.&lt;/li&gt;
  &lt;li&gt;C4 [15%]: Similarly preprocessed to CommonCrawl, to enhance performance.&lt;/li&gt;
  &lt;li&gt;Github [4.5%]: Filtered for line length and alphanumeric content from Google BigQuery.&lt;/li&gt;
  &lt;li&gt;Wikipedia [4.5%]: Dumps from mid-2022, covering multiple languages.&lt;/li&gt;
  &lt;li&gt;Gutenberg and Books3 [4.5%]: Publicly available books with redundant content removed.&lt;/li&gt;
  &lt;li&gt;ArXiv [2.5%]: Includes scientific data, with non-essential content removed.&lt;/li&gt;
  &lt;li&gt;Stack Exchange [2%]: High-quality Q&amp;amp;A content sorted by score.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;tokenization&quot;&gt;Tokenization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Byte Pair Encoding (BPE) tokenizer used.&lt;/li&gt;
  &lt;li&gt;Splits numbers into digits and decomposes unknown UTF-8 characters.&lt;/li&gt;
  &lt;li&gt;The training dataset contains approximately 1.4T tokens, with minimal repetition (fig 1).
    &lt;p align=&quot;center&quot;&gt;
 &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 30%;&quot; /&gt;
 &lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;LLaMA models are based on transformer architecture with key modifications:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre-normalization [GPT3]: Normalizes the input of each transformer sub-layer, enhancing training stability using RMSNorm.&lt;/li&gt;
  &lt;li&gt;SwiGLU activation function [PaLM]: Uses SwiGLU instead of ReLU, improving performance with a dimension of $2\over3 4d$ instead of $4d$ as in PaLM.&lt;/li&gt;
  &lt;li&gt;Rotary Embeddings [GPTNeo]: Employs Rotary embeddings (RoPE) instead of absolute positional embeddings at each layer of the network.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;optimizer&quot;&gt;Optimizer&lt;/h2&gt;

&lt;p&gt;Trained using the AdamW optimizer with:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\beta_1=0.9, \beta_2=0.95$.&lt;/li&gt;
  &lt;li&gt;Cosine learning rate schedule, ending at 10% of the maximal rate.&lt;/li&gt;
  &lt;li&gt;Weight decay of 0.1 and gradient clipping of 1.0.&lt;/li&gt;
  &lt;li&gt;2,000 warmup-steps, with varying learning rates and batch size with the size of the model (table 2).
    &lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table2.png?raw=true&quot; alt=&quot;table2&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;efficient-implementation&quot;&gt;Efficient implementation&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Causal multi-head attention: Efficient implementation using xformer library to reduce memory and runtime.&lt;/li&gt;
  &lt;li&gt;Activation reductions: Uses checkpointing to recompute activations during the backward pass, especially for computationally expensive layers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;main-results&quot;&gt;Main Results&lt;/h1&gt;

&lt;p&gt;Evaluated on 20 benchmarks for zero-shot and few-shot tasks, compared to non-public models (GPT-3, Gopher, Chinchilla, PaLM) and open-sourced models (OPT, GPT-J, GPT-Neo).&lt;/p&gt;

&lt;h2 id=&quot;common-sense-reasonging&quot;&gt;Common sense reasonging&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table3.png?raw=true&quot; alt=&quot;table3&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: Eight standard benchmarks such as BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA. Theses datasets include Cloze and Winograd style tasks and multiple choice question answering (QA).&lt;/li&gt;
  &lt;li&gt;Results
    &lt;ul&gt;
      &lt;li&gt;LLaMA-65B outperforms Chinchilla 70B and PaLM-540B on most benchmarks except BoolQ.&lt;/li&gt;
      &lt;li&gt;LLaMA-13B outperforms GPT-3 on most benchmarks despite being significantly smaller.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;close-book-question-answering&quot;&gt;Close-book question answering&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table4.png?raw=true&quot; alt=&quot;table4&quot; style=&quot;zoom: 30%;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table5.png?raw=true&quot; alt=&quot;table5&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: Natural Questions and TriviaQA. The models report exact match performance where the models do not have access to documents that contain avidence to answer the question.&lt;/li&gt;
  &lt;li&gt;Results:
    &lt;ul&gt;
      &lt;li&gt;LLaMA-65B achieve state-of-the-art (SOTA) performance in zero-shot and few-shot settings.&lt;/li&gt;
      &lt;li&gt;LLaMA-13B is competitive with GPT-3 and Chinchilla which are larger models.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reading-comprehension&quot;&gt;Reading comprehension&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table6.png?raw=true&quot; alt=&quot;table6&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmark: RACE reading comprehension, collected from English reading comprehension exams in middle and high school Chinese students.&lt;/li&gt;
  &lt;li&gt;Results: LLaMA-65B is competitive with PaLM-540B, and LLaMA-13B outperforms GPT-3.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mathematical-reasoning&quot;&gt;Mathematical reasoning&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table7.png?raw=true&quot; alt=&quot;table7&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: MATH and GSM8k. MATH contains 12K math problems of middle and high school. GSM8k is a set of middle school math problems.&lt;/li&gt;
  &lt;li&gt;Results: LLaMA-65B outperforms Minerva-62B on GSM8k.
    &lt;ul&gt;
      &lt;li&gt;Minerva is a series of PaLM models fine-tuned on 38.5B tokens extracted from ArXiv and Math Web Pages. Both PaLM and LLaMA, however, are not finetuned on math data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code-generation&quot;&gt;Code generation&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table8.png?raw=true&quot; alt=&quot;table8&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Benchmarks: HumanEval and MBPP. The models are evaluated about their ability to write code from a natural language description.&lt;/li&gt;
  &lt;li&gt;Results:
    &lt;ul&gt;
      &lt;li&gt;LLaMA models outperform other models, including LaMDA and PaLM. LLaMA-13B outperforms LaMDA-137B. LLaMA 65B outperforms PaLM-62B.&lt;/li&gt;
      &lt;li&gt;Fine-tuning on code-specific tokens further improves performance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;massive-multitask-language-understanding&quot;&gt;Massive multitask language understanding&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table9.png?raw=true&quot; alt=&quot;table9&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Massive multitask language understanding (MMLU) consists of multiple choice questions covering various domains of knowledge, like humanities, STEM and social sciences.&lt;/li&gt;
  &lt;li&gt;Results: LLaMA-65B underperforms compared to Chinchilla-70B and PaLM-540B, possibly due to limited academic data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evolution-of-performance-during-training&quot;&gt;Evolution of performance during training&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Performance improves steadily, and correlates with the training perplexity of the model.&lt;/li&gt;
  &lt;li&gt;SIQA and WinoGrande are the exceptions: SIQA may not be reliable as performance is varied, and performance doesn’t correlate with training perplexity on WinoGrande.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;instruction-fine-tuning&quot;&gt;Instruction Fine-tuning&lt;/h1&gt;

&lt;p&gt;Fine-tuning improves performance and futher the ability to follow instructions. LLaMA-I is trained on MMLU with instructions and compared with OPT-IML and Flan-PaLM series which fine-tuned with moderate sizes.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table10.png?raw=true&quot; alt=&quot;table10&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLaMA-I with 65B parameter size outperforms existing instruction fine-tuned models, but remains behind GPT ‘code-davinci-002’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;bias-toxicity-and-misinformation&quot;&gt;Bias, Toxicity and Misinformation&lt;/h1&gt;

&lt;p&gt;LLMs have been showed to be biased to content of training data, and to generate toxic content. Evaluated using benchmarks for toxic content generation and stereotypes detection.&lt;/p&gt;

&lt;h2 id=&quot;realtoxicityprompts&quot;&gt;RealToxicityPrompts&lt;/h2&gt;

&lt;p&gt;Indicates how toxic is a model. The toxicity score is automatically evaluated by making a request to PerspectiveAPI, ranging from 0 (non-toxic) to 1 (toxic).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table11.png?raw=true&quot; alt=&quot;table11&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Comparable to other models, with larger models exhibiting more toxicity, especially for “Respectiful” prompts.&lt;/li&gt;
  &lt;li&gt;It can be suggested that the relation between toxicity and model size may only apply within a model family.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;crows-pairs&quot;&gt;CrowS-Pairs&lt;/h2&gt;

&lt;p&gt;Evaluates the biases in a model with 9 categories: gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socioenconomic status.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table12.png?raw=true&quot; alt=&quot;table12&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLaMA shows slight biases, particularly in the religion, age, and gender categories. This may be come from CommonCrawl dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;winogender&quot;&gt;WinoGender&lt;/h2&gt;

&lt;p&gt;Used to investigate the bias of a model on the gender category. It evaluates if the model’s co-reference resolution performance is impacted by the gender of the pronoun.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table13.png?raw=true&quot; alt=&quot;table13&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Performance varies by pronoun type: The models have better performance “their/them/someone” pronouns than for the “her/her/she” and “his/him/he” pronouns.&lt;/li&gt;
  &lt;li&gt;Larger models showing more gender bias: For “gotcha” cases, LLaMA-65B makes more errors, showing that it capture biases on gender.
    &lt;ul&gt;
      &lt;li&gt;“gotcha” cases are in which the pronoun does not match the majority gender of the occupation, and the occupation is the correct answer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;truthfulqa&quot;&gt;TruthfulQA&lt;/h2&gt;

&lt;p&gt;Evaluates the a model’s ability to identify true claims and measures the risk of generating misinformation or false claims. This assesses the truthfulness of a model’s responses.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table14.png?raw=true&quot; alt=&quot;table14&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LLaMA models show better truthfulness compared to GPT-3. However the correct answer rate remains low, indicating a potential for misinformation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;carbon-footprint&quot;&gt;Carbon footprint&lt;/h1&gt;

&lt;p&gt;Details the environmental impact of training and deploying these models.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-05-10-llama/table15.png?raw=true&quot; alt=&quot;table15&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Training language models to follow instructions with human feedback (2022)</title>
   <link href="https://alatteaday.github.io/papers/2023/04/30/rlhf/"/>
   <updated>2023-04-30T00:00:00-05:00</updated>
   <id>https://alatteaday.github.io/papers/2023/04/30/rlhf</id>
   <content type="html">&lt;style&gt;
img {
    display: inline;
}
p {
   margin-top: 1em;
   margin-bottom: 0em;
   margin-left: 0em;
   margin-right: 0em;
}
p.a{
   margin-top: 2.5em;
   margin-bottom: -0.5em;
   margin-left: 0em;
   margin-right: 0em;
}
&lt;/style&gt;

&lt;p&gt;Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” &lt;em&gt;Advances in neural information processing systems&lt;/em&gt; 35 (2022): 27730-27744.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;point&quot;&gt;Point&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Employs &lt;strong&gt;Reinforcement Learning from Human Feedback (RLHF)&lt;/strong&gt; to fine-tune GPT-3 models, aligning them with human intentions while reducing unintended behaviors like hallucinations and toxicity.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;InstructGPT&lt;/strong&gt; models outperforms GPT-3 in truthfulness and reliability, generalizing well to new tasks like non-English and coding instructions.&lt;/li&gt;
  &lt;li&gt;Highlights the need for diverse stakeholder input and suggest combining RLHF with other methods to improve model alignment and safety.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;Language models (LMs) often generate misinsformation, toxic or biased content and this issue cannot be resolved simply by increasing the model size. Understanding user intent is crucial for these models. Fine-tuning with human feedback can align the models with user intentions across various tasks.&lt;/p&gt;

&lt;p&gt;Large language models (LLMs) frequently exhibit uninteded behaviors, such as hallucinations, toxic text generation, failing to follow user instructions. These are influenced by the model’s objective, which typically involves predicting the next token based on web data, differing from the goal of “following the user instructions helpfully and safely”.&lt;/p&gt;

&lt;p&gt;To align LMs, this paper employs &lt;strong&gt;Reinforcement Learning from Human Feedbak (RLHF)&lt;/strong&gt; to fine-tune GPT-3 to follow instructions. Human preferences serve as a reward signal for this fine-tuning process.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;methods-and-experimental-details&quot;&gt;Methods and experimental details&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig2.png?raw=true&quot; alt=&quot;fig2&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;high-level-methology&quot;&gt;High-level methology&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Preparation: Utilize pre-trained language models (GPT-3), prepare a distribution of prompts for alignment, and train human labelers.&lt;/li&gt;
  &lt;li&gt;Collect demonstration data and train a supervised policy: Labelers provide input prompts as desired behavior responses. The model is fine-tuned on this data using supervised learning.&lt;/li&gt;
  &lt;li&gt;Collect comparison data and train a reward model: Labelers compare model outputs and indicate their preferences. A reward model (RM) is trained using these comparisons to predict human-preferred outputs.&lt;/li&gt;
  &lt;li&gt;Optimize a policy aganst the RM using PPO: The RM’s output serves as a scalar reward. The supervised policy (trained GPT-3) is fine-tuned using the PPO algorithm to optimize this reward.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Step 2 and 3 can be iterative: More comparison data is collected on the current best policy, used to train a new RM and subsequently a new policy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;Source of prompts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Consists of text prompts submitted to the OpenAI API, specifically those using an earlier version of InstructGPT models on the Playground interface.&lt;/li&gt;
  &lt;li&gt;The paper does not include data from customers using the API in production.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Deduplication and filtering:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Heuristically deduplicated by checking for prompts that share a long common prefix.&lt;/li&gt;
  &lt;li&gt;The number of prompts is limited to 200 per user ID.&lt;/li&gt;
  &lt;li&gt;Validation and test sets contain no data from users whose data is in the training set.&lt;/li&gt;
  &lt;li&gt;All prompts in the training split were filtered for personally indentifiable information (PII).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Initial source of prompts: Human-written prompts were used as an initial source of instruction to bootstrap the process.&lt;/p&gt;

&lt;p&gt;Datasets for fine-tuning:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SFT dataset: Labelers’ demonstrations (13k prompts, from the API and labeler-written examples).&lt;/li&gt;
  &lt;li&gt;RM dataset: Labeler rankings of model outputs (33k, from the API and labeler-written examples).&lt;/li&gt;
  &lt;li&gt;PPO dataset: Inputs for RLHF fine-tuning. Human labels were not used (31k, only from the API).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Use cases: Most of the use-cases have are generative, rather than classification of prompts submitted to InstructGPT models&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tasks&quot;&gt;Tasks&lt;/h2&gt;

&lt;p&gt;Datasets for training tasks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sources: The datasets are sourced from prompts written by labelers and those submitted to early versions of InstructGPT models via API.&lt;/li&gt;
  &lt;li&gt;Labeler Instructions: Labelers are trained and instructed to write prompts with specific intents or implicit goals in mind to ensure the model aligns with desired behaviors.&lt;/li&gt;
  &lt;li&gt;Language: The datasets are predominately in English (95%). However, the paper also reports the models’ performance in other languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;human-data-collection&quot;&gt;Human data collection&lt;/h2&gt;

&lt;p&gt;Selection of Labelers: A diverse group of labelers was selected to ensure a broad demographic representation. It aims to generate inputs with a wide range of perspectives and to identify potentially harmful outputs.&lt;/p&gt;

&lt;p&gt;Training and Evaluation: Labelers underwent tests designed to measure their performance in labeling according to the set standards. This included their ability to generate diverse prompts and accurately identify harmful content.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;Pre-trained GPT models are utilized as basis. These models are trianed on a broad distribution of Internet data and can be used for various tasks but initially exhibit poorly characterized behavior. The GPT-3 models are then further trained using three different techniques:&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;supervised-fine-tuning-sft&quot;&gt;Supervised fine-tuning (SFT)&lt;/h3&gt;

&lt;p&gt;This method fine-tunes GPT-3 on labeler demonstrations using supervised learning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training details: 16 epochs using a cosine learing rate decay and a residual dropout of 0.2.&lt;/li&gt;
  &lt;li&gt;Model selection: Based on the model’s RM score on the validation set.&lt;/li&gt;
  &lt;li&gt;Finding: Training for more epochs improves both the RM score and human preference ratings, depite some overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reward-modeling-rm&quot;&gt;Reward modeling (RM)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Base model: Starts with a pre-trained SFT model but the final unembedding layer is removed. This layer maps the model’s representations to the vocabulary space for generating output tokens.&lt;/li&gt;
  &lt;li&gt;Input and output: The model takes a prompt and a response are as input and outputs a scalr reward, representing theh quality of the response for the given prompt.&lt;/li&gt;
  &lt;li&gt;Model size: Utilizes 6B reward model (RM) for efficiency. A larger 175B RM was found to be unstable and unsuitable for use as the value function in RL.&lt;/li&gt;
  &lt;li&gt;Data: Uses comparisons between two model outputs for the same input to determine which output is preferred by human labelers.&lt;/li&gt;
  &lt;li&gt;Loss: Trained with cross-entropy loss, using the comparisons as labels. The reward difference reflect the log odds of one response being preferred over the other by a labeler.&lt;/li&gt;
  &lt;li&gt;Speed-up comparison collection: Labelers are presented with $K$ responses to rank for each prompt, where $K$ ranges from 4 to 9. This results in $K(K-1) \over 2$ comparisons for each prompt.&lt;/li&gt;
  &lt;li&gt;Training efficiency and overfitting:
    &lt;ul&gt;
      &lt;li&gt;Comparisons within each labeling task are very correlated. If all comparisons are shuffled into one dataset and processed in a single pass, the model tends to overfit.&lt;/li&gt;
      &lt;li&gt;To address this, the training treats all $K(K-1) \over 2$ comparisons from each prompt as a single batch element, offering several benefits:
        &lt;ul&gt;
          &lt;li&gt;Requires only one forward pass for each set of $K$ responses, instead of $K(K-1) \over 2$ forward passes.&lt;/li&gt;
          &lt;li&gt;Prevents overfitting by avoiding isolated highly correlated comparisons.&lt;/li&gt;
          &lt;li&gt;Improves computational efficiency, and achieves better validation accuracy and log loss.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Loss function:&lt;/p&gt;

\[loss(\theta)=-{1\over \binom{K}{2}} E_{(x,y_w,y_l)~D}[\log(\sigma(r_\theta(x,y_w)-r_\theta(x,y_l)))]\]

    &lt;ul&gt;
      &lt;li&gt;$r_\theta(x,y)$ is the scalar output of the RM for promt $x$ and completion $y$ with parameters $\theta$.&lt;/li&gt;
      &lt;li&gt;$y_w$ is preferred completion out of the pair of $y_w$ and $y_l$.&lt;/li&gt;
      &lt;li&gt;$D$ is the dataset of human comparisons.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning-rl&quot;&gt;Reinforcement learning (RL)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Base model: The SFT model is fine-tuned using Proximal Policy Optimization (PPO) in an environment.&lt;/li&gt;
  &lt;li&gt;Training environment: A bandit environment. It this context, a bandit environment presents a random customer prompt, expects a response, produces a reward determined by the RM, and ends the episode.&lt;/li&gt;
  &lt;li&gt;Input and output: The model takes the prompt and response as input and outputs a reward determined by the RM.&lt;/li&gt;
  &lt;li&gt;KL penalty: A per-token Kullback-Leibler (KL) penalty is added from the SFT model at each token.
    &lt;ul&gt;
      &lt;li&gt;This penalty mitigates over-optimization of the RM and prevents the model from deviating too far from the behavior learned during supervised fine-tuning.&lt;/li&gt;
      &lt;li&gt;The value funciton used in PPO is initialized from the RM.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PPO and PPO-ptx models:
    &lt;ul&gt;
      &lt;li&gt;PPO models: Fine-tuned with PPO.&lt;/li&gt;
      &lt;li&gt;PPO-ptx models: Involve an additional experiment where pre-training gradients are mixed into PPO gradients to address performance regressions on public NLP datasets.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;The objective function for PPO-ptx:&lt;/p&gt;

\[\begin{aligned}
\text{objective}(\phi) = &amp;amp; \ \mathbb{E}_{(x, y) \sim D_{\pi_{\phi}^{RL}}} \left[ r_\theta(x, y) - \beta \log \left( \frac{\pi_\phi^{RL}(y | x)}{\pi^{SFT}(y | x)} \right) \right] \\
&amp;amp; + \gamma \mathbb{E}_{x \sim D_{\text{pretrain}}} \left[ \log(\pi_\phi^{RL}(x)) \right]
\end{aligned}\]

        &lt;p&gt;where:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;$\pi_\phi^{RL}$ is the learned RL policy and $\pi^{SFT}$ is the supervised fine-tuned model.&lt;/li&gt;
          &lt;li&gt;$D_{\pi^{RL}}$ is the distribution of data under the RL policy, and $D_{pretrain}$ is the pre-training distribution.&lt;/li&gt;
          &lt;li&gt;$\beta$ is the KL reward coefficient, controlling the strength of the KL penalty.&lt;/li&gt;
          &lt;li&gt;$\gamma$ is the pre-training loss coefficient, controlling the influence of pre-training gradients. For PPO models $\gamma$ is set to 0.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In this paper, InstructGPT refers to the PPO-ptx models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;The performance of PPO models is compared against several baselines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SFT models: Fine-tuned using supervised learing.&lt;/li&gt;
  &lt;li&gt;GPT-3: The standard GPT-3 model without additional fine-tuning.&lt;/li&gt;
  &lt;li&gt;GPT-3 Prompted: Provided with a few-shot previx to prompt it into an instruction-following mode, where the prefix is prepended to the user-specified instruction.&lt;/li&gt;
  &lt;li&gt;InstructGPT is compared to 175B GPT-3 models fine-tuned on FLAN and T0 datasets. These datasets include various NLP tasks combined with natural language instructions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;The definition of “alignment” to evaluate models is based on their ability to act in accordance with user intentions. The practical evaluation framework checks if the model is helpful, honest and harmless.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Helpfulness: The model should follow instructions and infer intentions from prompts or a patterns.
    &lt;ul&gt;
      &lt;li&gt;Since the intention could be unclear, labeler preference ratings are considered mainly for evaluation.&lt;/li&gt;
      &lt;li&gt;There may be divergence between actual user intentions and labeler interpretations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Honesty: Truthfulness is measured instead of comparing the model’s output to its actual belief.
    &lt;ul&gt;
      &lt;li&gt;Two metrics are used:
        &lt;ul&gt;
          &lt;li&gt;The model’s tendency to fabricate information on closed domain tasks&lt;/li&gt;
          &lt;li&gt;Performance on the TruthfulQA dataset.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Harm: Harmfulness depends on the context in which the model is used, and assessing potential harm requires significatn speculation.
    &lt;ul&gt;
      &lt;li&gt;More specific proxy criteria are used:
        &lt;ul&gt;
          &lt;li&gt;Whether a deployed model could be harmful.&lt;/li&gt;
          &lt;li&gt;Labelers evaluate if an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content.&lt;/li&gt;
          &lt;li&gt;Benchmarks like RealToxicityPrompts and CrowS-pairs are used to measure bias and toxicity.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-on-api-distiribution&quot;&gt;Evaluation on API distiribution&lt;/h3&gt;

&lt;p&gt;When using prompts from the API for evaluting human preference ratings, only prompts not included in training are selected.&lt;/p&gt;

&lt;p&gt;Since prompts for InsturctGPT models are not suitable for the GPT-3 baselines, prompts submitted to the GPT-3 API are also used for evaluation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The GPT-3 prompts are not in an instruction-following style.&lt;/li&gt;
  &lt;li&gt;The 175B SFT model is chosen as the baseline due to its average performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each model is evaluated based on how often its outputs are preferred, and labelers judge the overall quality of each response on a 1-7 Likert scale.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-on-public-nlp-datasets&quot;&gt;Evaluation on public NLP datasets&lt;/h3&gt;

&lt;p&gt;Two types of public datasets are used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Safety evaluation: Focuses on truthfulness, toxicity, and bias. Includes evaluations of toxicity using the RealToxicityPrompts dataset.&lt;/li&gt;
  &lt;li&gt;Zero-shot performance: Assesses performance on traditional NLP tasks such as question anwering (QA), reading comprehension, and summarization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The experimental results are organized into three parts: results on the API prompt distribution, results on public NLP datasets, and qualitative results.&lt;/p&gt;

&lt;h2 id=&quot;results-on-the-api-distribution&quot;&gt;Results on the API distribution&lt;/h2&gt;

&lt;h3 id=&quot;1-labelers-significantly-prefer-instructgpt-outputs-over-outputs-from-gpt-3&quot;&gt;1. Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig1.png?raw=true&quot; alt=&quot;fig1&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;175B InstructGPT outputs are preferred to GPT-3 outputs around 85% of the time and around 71% compared to few-shot GPT-3.&lt;/li&gt;
  &lt;li&gt;The preference order is GPT-3 &amp;lt; GPT-3 Prompted &amp;lt; SFT &amp;lt; PPO.&lt;/li&gt;
  &lt;li&gt;Adding updates on the pre-training mix during PPO does not lead to significant changes in labeler preference.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;a&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig3.png?raw=true&quot; alt=&quot;fig3&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This preference trend remains consistent when evaluating models on prompts submitted to GPT-3 models on the API, though PPO-ptx models perform slightly worse at larger sizes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;a&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig4.png?raw=true&quot; alt=&quot;fig4&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;InstructGPT outputs are rated favorably on more concrete axes: They follow constraints and instruction better and hallucinate less.&lt;/li&gt;
  &lt;li&gt;This suggests that InstructGPT models are more reliable and easier to control than GPT-3.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-instructgpt-models-generalize-to-the-preferences-of-held-out-labelers-that-did-not-produce-any-training-data&quot;&gt;2. InstructGPT models generalize to the preferences of “held-out” labelers that did not produce any training data.&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;InstructGPT models’ outputs are rated better than GPT-3 baselines by held-out labelers, indicating InstructGPT models are not simiply overfitting to the preferences of training labelers.&lt;/li&gt;
  &lt;li&gt;RMs also demonstrate generlization capabilties with cross-validation results: 69.6% accuracy in predicting the preferences of held-out labelers, which is slightly lower than 72.4% accuracy in the predicting preferences within the training set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-public-nlp-datasets-are-not-reflective-of-how-the-lms-are-used&quot;&gt;3. Public NLP datasets are not reflective of how the LMs are used.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig5.png?raw=true&quot; alt=&quot;fig5&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When comparing InstructGPT to 175B GPT-3 baseline fine-tuned on FLAN and T0, these models perform better than GPT-3 with a good prompt but worse than the SFT baseline. This suggests the datasets are not sufficiently diverse to improve API prompt distribution.&lt;/li&gt;
  &lt;li&gt;InstructGPT may outperform FLAN and T0 because:
    &lt;ul&gt;
      &lt;li&gt;Public NLP datasets are desinged to capture typical tasks that are easy to evaluate (e.g., classification, QA). However, open-ended generation and brainstorming constitute most (57%) of tasks the API users want.&lt;/li&gt;
      &lt;li&gt;Public NLP datasets may lack the high diversity of inputs that real-world users are interested in.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results-on-public-nlp-datasets&quot;&gt;Results on public NLP datasets&lt;/h2&gt;

&lt;h3 id=&quot;1-instructgpt-models-show-improvements-in-truthfulness-over-gpt-3&quot;&gt;1. InstructGPT models show improvements in truthfulness over GPT-3.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig6.png?raw=true&quot; alt=&quot;fig6&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PPO models demonstrate significant improvements on the TruthfulQA dataset.&lt;/li&gt;
  &lt;li&gt;The 1.3B PPO-ptx model performs slightly worse than GPT-3 of the same size.&lt;/li&gt;
  &lt;li&gt;Training with an “Instruction+QA” prompt helps the model avoid generating false information.
    &lt;ul&gt;
      &lt;li&gt;Instruction+QA: Instructs the model to respond with “I have no comment” when it’s uncertain of the correct answer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-instructgpt-shows-small-improvements-in-toxicity-over-gpt-3-but-not-bias&quot;&gt;2. InstructGPT shows small improvements in toxicity over GPT-3, but not bias.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig7.png?raw=true&quot; alt=&quot;fig7&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Toxicity: Evaluated using the RealToxicityPrompts benchmark.
    &lt;ul&gt;
      &lt;li&gt;Evaluation method: Toxicity scores are obtained through the Perspective API with model samples and labelers rate the samples.&lt;/li&gt;
      &lt;li&gt;InstructGPT outputs are less toxic than those of GPT-3 when instructed to generate respectful outputs. Without any prompt, the models are similar, and InstructGPT can be more toxic when prompted to produce toxic content.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bias: Evaluated using the Winogender and CrowS-Pairs benchmarks.
    &lt;ul&gt;
      &lt;li&gt;Evaluation method: Calculates the relative probabilities of producing sentences in each pair and the entropy of the associated binary probability distributions.
        &lt;ul&gt;
          &lt;li&gt;Unbiased models will show no preference, thus having maximum entropy.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;InstructGPT and GPT-3 show similar levels of bias. The PPO-ptx model shows higher bias when instructed to act respectfully, with unclear patterns.&lt;/li&gt;
      &lt;li&gt;Instructed models tend to be more certain of their outputs, regardlessly with stereotypes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-modifying-rlhf-fine-tuning-procedures-can-minimize-performance-regressions-on-public-nlp-datasets&quot;&gt;3. Modifying RLHF fine-tuning procedures can minimize performance regressions on public NLP datasets.&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Alignment tax: PPO model experience a decrease in performance on public NLP datasets, referred to as “alignment tax”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;width: 100%;&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig28.png?raw=true&quot; alt=&quot;fig28&quot; style=&quot;width: 49%; vertical-align:text-top;&quot; /&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig29.png?raw=true&quot; alt=&quot;fig29&quot; style=&quot;width: 49%; vertical-align:text-top;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mitigation strategies: Mixing pre-training updates to the PPO fine-tuning (PPO-ptx) reduces performance regressions across all datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;a&quot; align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig33.png?raw=true&quot; alt=&quot;fig33&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PPO-ptx performs better than merely increasing the KL coefficient. Changing the KL model from the PPO initialization to GPT-3 yields similar improvements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;qualitative-results&quot;&gt;Qualitative results&lt;/h2&gt;

&lt;h3 id=&quot;1-instructgpt-models-show-promising-generlization-to-instructions-outside-of-the-rlhf-fine-tuning-distribution&quot;&gt;1. InstructGPT models show promising generlization to instructions outside of the RLHF fine-tuning distribution.&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;InstructGPT models can follow non-English instructions, and perform coding tasks, despite limited training data in these formats.&lt;/li&gt;
  &lt;li&gt;Alignment methods can generalize to produce desired behaviors on inputs not directly supervised.&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig8.png?raw=true&quot; alt=&quot;fig8&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;175B PPO-ptx model can answer questions about code and non-English instructions, but often responds in English to questions in other languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-instructgpt-still-makes-simple-mistakes&quot;&gt;2. InstructGPT still makes simple mistakes.&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2023-04-30-rlhf/fig9.png?raw=true&quot; alt=&quot;fig9&quot; style=&quot;zoom: 35%;&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model sometimes incorrectly assumes a false premise in an instruction is true.&lt;/li&gt;
  &lt;li&gt;It can overly hedge even when the answer is clear.&lt;/li&gt;
  &lt;li&gt;It struggles with generating responses when there’re multiple or challenging constraints in an instruction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;h2 id=&quot;implications-for-alignment-research&quot;&gt;Implications for alignment research&lt;/h2&gt;

&lt;p&gt;Improving the alignment of current AI systems provides a clear empirical feedback loop, esssential for refining alignment techniques.&lt;/p&gt;

&lt;p&gt;Moreover, RLHF is an important building block for aligning superhuman systems, especially for tasks difficult to evaluate.&lt;/p&gt;

&lt;p&gt;General lessons for alignment research:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The cost of increasing model alignment is modest relative to pre-training&lt;/strong&gt;: The significant costs lie in data collection and computation. With RLHF, larger LMs become more helpful, suggesting investing in aligning existing LMs is more efficient than training new, larger models.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;There is evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in&lt;/strong&gt;: E.g., non-English and code tasks. This is important as creating supervised models for each task is expensive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The proposed fine-tuning can mitigate most of the performance degradations&lt;/strong&gt;: Low alignment tax techniques are needed for future AI systems capable of understanding human intents, and RLHF is effective in this regard.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Alignment techniques are validated in the real world&lt;/strong&gt;: This work grounds alignment research in real-world applications, providing valuable insights for AI systems used by actual users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;who-are-we-aligning-to&quot;&gt;Who are we aligning to?&lt;/h2&gt;

&lt;p&gt;Factors influencing the fine-tuning data and key sources of alignment preferences:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Labelers’ preferences: The models are aligned to the preferences of hired labelers who generate the training data. They are mostly English speakers, with around 73% agreement among them.&lt;/li&gt;
  &lt;li&gt;Researchers’ preferences: Researchers design the study, write instructions, and guide labelers on edge cases, thereby influencing the alignment. More research is needed to understand the impact of different instructions and interfaces on the collected data and model behavior.&lt;/li&gt;
  &lt;li&gt;Customer prompts: Training data includes prompts from OpenAI customers using the API. There is potential misalignment between customer goals and end-user well-being.&lt;/li&gt;
  &lt;li&gt;Customer representation: The customers are not representative of all potential or current LM users. The initial user base was biased towards OpenAI’s networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Challenges and future directions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Designing a fair and transparent alignment process is complex.&lt;/li&gt;
  &lt;li&gt;This paper demonstrates that the alignment method can work for a specific human reference group but doesn’t claim these group preferences are ideal.&lt;/li&gt;
  &lt;li&gt;Multiple stakeholders need consideration, including model trainers, developers, end-users, and the broader impacted population.&lt;/li&gt;
  &lt;li&gt;Aligning a system to everyone’s preferences simultaneously is impossible, and not all trade-offs will be universally endorsed.&lt;/li&gt;
  &lt;li&gt;One potential approach is to train models for different group preferences so that it can reflect diverse values. However, this may still impact broader society, raising decisions about prioritizing preferences.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Methodology:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Contractor influence: InstructGPT is influenced by the human feedback from about 40 contractors.
    &lt;ul&gt;
      &lt;li&gt;Contractors’ identity, beliefs, cultural backgrounds, and personal history may affect their judgments.&lt;/li&gt;
      &lt;li&gt;They were selected based on their performance with sensitive prompts and labeling tasks.&lt;/li&gt;
      &lt;li&gt;The small team size allowed for better communication but is not representative of the broader population will use the models.&lt;/li&gt;
      &lt;li&gt;They are mostly  English-speaking, and the data is almost entirely in English.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data collection improvements: Most comparisons are labeled by only one contractor to reduce costs.
    &lt;ul&gt;
      &lt;li&gt;Multiple labelings could help identify disagreement areas, indicating where a single model may not align with all labelers.&lt;/li&gt;
      &lt;li&gt;Averaging labeler preferences for disagreements might not be ideal, especially for minority groups, whose preferences should be weighted more heavily.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Imcomplete alignment and safety: InstructGPT is not fully aligned or safe.
    &lt;ul&gt;
      &lt;li&gt;It still generates toxic or biased outputs, misinformations, and sexual or violent content.&lt;/li&gt;
      &lt;li&gt;It sometimes fails to generate reasonable outputs for certain inputs.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Following potentially harmful instructions: InstructGPT often follows instructions even if it could lead to real-world harm.
    &lt;ul&gt;
      &lt;li&gt;It produces more toxic outputs than GPT-3 when instructed to be maximally biased.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>[Paper] Deep learning for image super-resolution: A survey (2020)</title>
   <link href="https://alatteaday.github.io/papers/2020/12/22/srsurvey/"/>
   <updated>2020-12-22T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2020/12/22/srsurvey</id>
   <content type="html">&lt;p&gt;Wang, Zhihao, Jian Chen, and Steven CH Hoi. “Deep learning for image super-resolution: A survey.” &lt;em&gt;IEEE transactions on pattern analysis and machine intelligence&lt;/em&gt; 43.10 (2020): 3365-3387.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9044873?casa_token=ZvibT-s3inQAAAAA:7z3uDjyf2cDsJhnY-NLadsaG1exlVS3qQAPck6JXaj6awV7I5Gcc8XXbjjw5uugCWXfE6tXJNB4&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Super-resolution (SR) is the process of enhancing the resolution of images, transforming low-resolution (LR) images to high-resolution (HR) images.&lt;/li&gt;
  &lt;li&gt;SR is an ill-posed problem due to the existence of multiple HR images for a single LR image.&lt;/li&gt;
  &lt;li&gt;Deep learning has significantly advanced SR, with approaches like CNNs (SRCNN) and GANs (SRGAN).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;problem-setting-and-terminology&quot;&gt;Problem Setting and Terminology&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Problem Definition: Developing a super-resolution model to approximate HR images from LR inputs.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Image Quality Assessment (IQA): Methods include subjective human perception and objective computational techniques, classified into full-reference, reduced-reference, and no-reference methods.&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;supervised-super-resolution&quot;&gt;Supervised Super-Resolution&lt;/h1&gt;

&lt;h2 id=&quot;sr-framework&quot;&gt;SR Framework&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-Upsampling Framework: Uses traditional upsampling followed by deep neural networks (e.g., SRCNN).&lt;/li&gt;
  &lt;li&gt;Post-Upsampling Framework: Employs end-to-end deep learning models for upsampling.&lt;/li&gt;
  &lt;li&gt;Progressive Upsampling Framework: Utilizes cascades of CNNs for step-by-step refinement of images.&lt;/li&gt;
  &lt;li&gt;Iterative Up-and-Down Sampling: Incorporates methods like DBPN and SRFBN for capturing LR-HR dependencies.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;upsampling-methods&quot;&gt;Upsampling Methods&lt;/h2&gt;

&lt;h3 id=&quot;interpolation-based&quot;&gt;Interpolation-Based&lt;/h3&gt;

&lt;p&gt;Includes nearest-neighbor, bilinear, and bicubic interpolation. These are traditional techniques used to resize images before the advent of deep learning-based methods.&lt;/p&gt;

&lt;h3 id=&quot;learning-based&quot;&gt;Learning-Based&lt;/h3&gt;

&lt;p&gt;Utilizes transposed convolution layers and sub-pixel layers for end-to-end learning.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Transposed Convolution Layer (Deconvolution Layer)&lt;/strong&gt;: Predicts the possible input based on feature maps sized like the convolution output for resolution, expanding the image by inserting zeros and performing convolution.
    &lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/deconv.jpg?raw=true&quot; alt=&quot;deconv&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This method enlarges the image size while maintaining a connectivity pattern, but it can cause uneven overlapping on each axis, leading to checkerboard-like artifacts that can affect SR performance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sub-Pixel Layer (Pixelshuffle)&lt;/strong&gt;: Generates plurality of channels by convolution and the reshape them.
    &lt;p align=&quot;center&quot;&gt;
   &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/subpixel.png?raw=true&quot; alt=&quot;subpixel&quot; style=&quot;zoom: 70%;&quot; /&gt;
&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Given an input size $(h \times w \times c)$, it generates $s^2$ times channels, where $s$ is a scaling factor. The output size becomes $(h \times w \times s^2c)$, which is then reshaped(shuffled) to $(sh \times sw \times c)$.&lt;/li&gt;
      &lt;li&gt;This method maintains a larger receptive field than the transposed convolution layer, providing more contextual and realistic details. However, the distribution of the receptive field can be uneven, leading to artifacts near the boundaries of different blocks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;network-design&quot;&gt;Network Design&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-22-srsurvey/networks.png?raw=true&quot; alt=&quot;networks&quot; style=&quot;zoom: 100%;&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;residual-learning&quot;&gt;Residual Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Simplifies learning by focusing on the residuals between LR and HR images instead of learning a direct mapping. This approach reduces the complexity of the transformation task.&lt;/li&gt;
  &lt;li&gt;By learning only the difference (residuals) between the input and the target image, the model can focus on fine details, resulting in better performance and faster convergence.&lt;/li&gt;
  &lt;li&gt;Example: The ResNet architecture uses residual blocks to enhance the ability of very deep networks to learn effectively without vanishing gradients.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recursive-learning&quot;&gt;Recursive Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Repeatedly applies the same modules to capture higher-level features while maintaining a manageable number of parameters.&lt;/li&gt;
  &lt;li&gt;Allows the network to refine features iteratively, leading to more detailed and accurate image reconstructions.&lt;/li&gt;
  &lt;li&gt;Example: Deep Recursive Convolutional Network (DRCN) utilizes a single convolutional layer applied multiple times to expand the receptive field without increasing the number of parameters significantly.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-path-learning&quot;&gt;Multi-Path Learning&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Local Multi-Path Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Extracts features through multiple parallel paths which then get fused to provide better modeling capabilities. This approach helps in capturing different aspects of the image simultaneously.&lt;/li&gt;
      &lt;li&gt;Different paths can focus on various scales or types of features, which are then combined to improve the overall representation.&lt;/li&gt;
      &lt;li&gt;Example: Multi-scale Residual Network (MSRN) uses multiple convolutional layers with different kernel sizes to capture multi-scale features.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scale-Specific Multi-Path Learning&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Involves having separate paths for different scaling factors within a single network, allowing the network to handle multiple scales more effectively.&lt;/li&gt;
      &lt;li&gt;Example: MDSR (Multi-Scale Deep Super-Resolution) shares most network parameters but has scale-specific layers to handle different upscaling factors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dense-connections&quot;&gt;Dense Connections&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Enhances gradient flow and feature reuse by connecting each layer to every other layer in a feed-forward fashion. This ensures that gradients can flow directly to earlier layers, improving learning efficiency.&lt;/li&gt;
  &lt;li&gt;Promotes feature reuse, leading to more efficient and compact networks.&lt;/li&gt;
  &lt;li&gt;Example: DenseNet connects each layer to every other layer, facilitating better feature propagation and reducing the risk of gradient vanishing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;group-convolution&quot;&gt;Group Convolution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Splits the input channels into groups and performs convolutions within each group. This reduces the computational complexity and number of parameters.&lt;/li&gt;
  &lt;li&gt;Often used in lightweight models to balance performance and efficiency.&lt;/li&gt;
  &lt;li&gt;Example: Xception and MobileNet architectures use depthwise separable convolutions, a type of group convolution, to reduce the number of parameters and computation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pyramid-pooling&quot;&gt;Pyramid Pooling&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Uses pooling operations at multiple scales to capture both global and local context information. This helps in understanding the image at different resolutions.&lt;/li&gt;
  &lt;li&gt;Example: PSPNet (Pyramid Scene Parsing Network) uses pyramid pooling to aggregate contextual information from different scales, which is then combined to enhance the feature representation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention-mechanisms&quot;&gt;Attention Mechanisms&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Channel Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Focuses on the interdependencies between feature channels. It assigns different weights to different channels, enhancing important features and suppressing less useful ones.&lt;/li&gt;
      &lt;li&gt;Example: Squeeze-and-Excitation Networks (SENet) uses a squeeze operation to aggregate feature maps across spatial dimensions, followed by an excitation operation that recalibrates channel-wise feature responses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spatial Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Focuses on the spatial location of important features. It assigns weights to different spatial regions, allowing the model to focus on relevant areas of the image.&lt;/li&gt;
      &lt;li&gt;Example: Convolutional Block Attention Module (CBAM) combines channel and spatial attention to improve representation by focusing on meaningful parts of the image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-Local Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Captures long-range dependencies between distant pixels. This is particularly useful for super-resolution tasks where global context is important.&lt;/li&gt;
      &lt;li&gt;Example: Non-local Neural Networks use a self-attention mechanism to compute relationships between all pairs of positions in the feature map, allowing the model to capture global context and dependencies.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Combined Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Some networks combine multiple types of attention mechanisms to leverage the strengths of each. For instance, combining channel and spatial attention can provide a more comprehensive attention mechanism.&lt;/li&gt;
      &lt;li&gt;Example: The Residual Channel Attention Network (RCAN) uses channel attention modules within a residual network structure to enhance the network’s ability to capture important features for image super-resolution&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;learning-strategies&quot;&gt;Learning Strategies&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Loss Functions: Early methods used pixel-wise L2 loss, while newer approaches incorporate more complex losses like content loss, adversarial loss, and perceptual loss to improve the quality of the reconstructed images.&lt;/li&gt;
  &lt;li&gt;Training Techniques: Techniques such as curriculum learning, multi-supervision, and progressive learning are used to enhance the training process and improve model performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-super-resolution&quot;&gt;Unsupervised Super-Resolution&lt;/h1&gt;

&lt;p&gt;Unsupervised methods do not rely on paired LR-HR datasets. Instead, they use generative models and adversarial training to learn the mapping from LR to HR images. Techniques include CycleGAN, which learns the transformation by mapping LR images to HR images and vice versa.&lt;/p&gt;

&lt;h1 id=&quot;domain-specific-super-resolution&quot;&gt;Domain-Specific Super-Resolution&lt;/h1&gt;

&lt;p&gt;Domain-specific methods focus on specific applications such as face SR, text SR, and medical image SR. These methods leverage domain knowledge to improve the quality of SR in specific contexts.&lt;/p&gt;

&lt;h1 id=&quot;benchmark-datasets-and-performance-evaluation&quot;&gt;Benchmark Datasets and Performance Evaluation&lt;/h1&gt;

&lt;p&gt;Several benchmark datasets are used for evaluating SR models, including Set5, Set14, BSD100, and Urban100. Common evaluation metrics include Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM).&lt;/p&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;While PSNR is widely used, it does not always correlate well with human perception of image quality. SSIM addresses this by considering luminance, contrast, and structure.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;PSNR is one of the most popular reconstruction quality measurements of lossy transformation. In the context of SR, it’s defined via the maximum pixel value ($L$) and the mean squared error (MSE) between the images.&lt;/p&gt;

\[PSNR=10\cdot\log_{10}\big({L^2\over{1\over N}\sum_{i=1}^N(I(i)-\hat{I}(i))^2}\big)\]

    &lt;ul&gt;
      &lt;li&gt;$I(i)$ and $\hat{I}(i)$ represent the pixel values of the original and reconstructed images, respectively. and $N$ is the total number of pixels.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SSIM measures the structural similarity between images based on independent comparisons in terms of luminance, contrast, and structures. It considers the human visual system (HVS) is highly adapted to extract image structures.&lt;/p&gt;

\[SSIM(I,\hat{I})={(2\mu_I\mu_{\hat{I}}+C_1)(2\sigma_{I\hat{I}}+C_2)\over(\mu_I^2+\mu_{\hat{I}}^2+C_1)(\sigma_I^2+\sigma_\hat{I}^2+C_2)}\]

    &lt;ul&gt;
      &lt;li&gt;$\mu_I$ and $\mu_\hat{I}$ are the mean pixel values of the original and reconstructed images, respectively. $\sigma_I^2$ and $\sigma_\hat{I}^2$ are the variances, and $\sigma_{I\hat{I}}$ is the covariance of $I$ and $\hat{I}$. $C_1$ and $C_2$ are constants to stabilize the division when the denominators are small.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;challenges-and-future-directions&quot;&gt;Challenges and Future Directions&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Scalability: Developing SR models that can handle varying scales and resolutions efficiently.&lt;/li&gt;
  &lt;li&gt;Real-World Applications: Enhancing SR models to perform well on real-world images with diverse degradation.&lt;/li&gt;
  &lt;li&gt;Efficiency: Reducing computational complexity and memory usage while maintaining high performance.&lt;/li&gt;
  &lt;li&gt;Generality: Creating SR models that generalize well across different types of images and domains.&lt;/li&gt;
  &lt;li&gt;Perceptual Quality: Improving the perceptual quality of SR images, ensuring that they are visually appealing and free from artifacts.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;The survey paper provides an in-depth review of deep learning-based super-resolution techniques, categorizing them into supervised, unsupervised, and domain-specific methods. It discusses various network architectures, upsampling techniques, and learning strategies, highlighting the advancements and challenges in the field. The paper also covers benchmark datasets and performance evaluation metrics, providing a comprehensive overview of the current state of image super-resolution research.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>[Paper] SinGAN: Learning a Generative Model from a Single Natural Image (2024)</title>
   <link href="https://alatteaday.github.io/papers/2020/12/11/singan/"/>
   <updated>2020-12-11T00:00:00-06:00</updated>
   <id>https://alatteaday.github.io/papers/2020/12/11/singan</id>
   <content type="html">&lt;p&gt;Shaham, Tamar Rott, Tali Dekel, and Tomer Michaeli. “Singan: Learning a generative model from a single natural image.” &lt;em&gt;Proceedings of the IEEE/CVF international conference on computer vision&lt;/em&gt;. 2019.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/html/Shaham_SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image_ICCV_2019_paper.html&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;GAN은 현실감 있고 질 좋은 이미지를 생성하는 성공적인 결과를 보여주었다. 그러나 이 장점은 모델이 특정한 클래스 내 데이터를 학습하여 해당 클래스에 속하는 이미지를 생성하는 것에 국한된다는 한계를 동반한다. 여러 개의 클래스를 갖는 다양한 데이터에서 어떠한 분포를 찾아내는 것은 여전히 어려운 문제이다. 이를 해결하기 위해 생성할 때 다른 input signal을 추가하여 조절하거나 모델을 특정한 task에 맞게 학습시키는 것이 요구된다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 기존의 한계점을 벗어나기 위해 “Unconditional generation learned from a single natural image” 를 제안한다. 단일 이미지(single image) 내부 패턴의 통계량만을 가지고도 충분히 생성 모델을 학습시킬 수 있다는 것이 이 논문의 아이디어이다. 이미지 하나에서 충분히 복잡한 구조와 질감을 얻어낼 수 있기 때문이다. 이런 방식으로 GAN 모델을 학습할 수 있다면 같은 클래스에 속하는 여러 개의 이미지 데이터에 의존할 필요가 없다.&lt;/p&gt;

&lt;p&gt;Single image를 다루는 기존의 모델들은 이미지를 생성할 때 저마다의 한계를 갖는다. InGAN은 최초로 Single image를 다룬 GAN 기반의 모델이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/ingan.png?raw=true&quot; alt=&quot;dddd&quot; style=&quot;zoom: 40%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;핵심적인 아이디어는 Generator가 입력 이미지인 $x$를 가지고 생성된 이미지인 $y$를 “retarget”하게 한다는 것이다. 그러나 InGAN은 입력 이미지에 conditional하다는 한계를 갖는다. 입력 이미지가 있어야 이미지를 생성할 수 있다. unconditional한 single image GAN 모델의 경우 texture generation에 관하여만 존재했다[3, 4, 5]. 이 모델들은 texture에 국한된 이미지만 생성할 수 있을 뿐, 보통 이미지라고 인식될 만큼의 자연스러운 결과물의 생성이 불가하다.&lt;/p&gt;

&lt;p&gt;Image manipulation task를 다루는 최근의 생성 모델들은 대부분 GAN을 기반으로 한다. 관련 task는 teractive image editing, sketch2image, image-to-image translation, super resolution 등이다. 그런데 기존 모델들은 특정한 클래스의 데이터를 학습하게 되어 있어 다양한 task를 유연하게 해결하지 못하는 등의 한계가 있다.&lt;/p&gt;

&lt;p&gt;이 논문에서 제안하는 SinGAN 모델은 기존의 한계점을 해결할 수 있다. SinGAN은 unconditional하게 입력 이미지 없이 noise만으로 이미지를 생성한다. 그러면서도 기존의 unconditional texture generation 모델과 달리 자연스러운 이미지를 생성할 수 있다. 나아가 특정한 클래스의 데이터의 공통적인 특성을 학습하는 것에 주력하지 않고, 하나의 이미지를 가지고 scale을 변화시키며 내부적인 특성을 학습한다. 이는 모델이 한정되지 않은 다양한 task를 수행하면서도 좋은 성능을 가지게끔 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;p&gt;SinGAN은 하나의 입력 이미지의 내재된 통계량을 가지고 unconditional하게 이미지를 생성하도록 만들어진 모델이다. 학습 방식의 핵심적인 부분은 이미지의 scale을 여러 단계를 거칠 때마다 변화시키며 이미지의 특성을 파악하게끔 한다는 것이다. 아래는 모델의 전체적 구조를 나타낸 것이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig4.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;모델의 구조는 전반적으로 데이터 scale이 위로 올라갈수록 정교해지는 pyramid 형태를 갖는다. Generator $G: {G_0, …, G_N}$는 입력 이미지인 $x:{x_0, …, x_N}$로 학습된다. $x_n$은 값 $r_n (r&amp;gt;1)$에 따라 원본 이미지를 downsampling한 것으로, $x_N$은 제일 coarse한 scale을 갖는다. 각 $G$는 이전 단계에서 생성된 이미지 $\tilde{x}$와 해당 단계의 scale에 맞는 noise $z$를 입력 받는다. 그리고 대응되는 Discriminator $D :{D_0, …, D_N}$를 속이는 방향으로 이미지를 생성하며 학습한다. D는 원본 이미지 $x_n$를 $G$가 생성한 $\tilde{x}_n$와 비교하여 판별해내는 방향으로 학습된다. 이 때 $x_n$과 $\tilde{x}_n$ 이미지 전체를 기준으로 비교하는 것이 아니라 이미지의 일부분을 두고 비교하는데, 마치 이미지 위에 겹쳐져 비교할 부분을 가리키는 것을 patch라고 한다. 이 patch size는 pyramid 단계에서 올라갈수록 작아진다.&lt;/p&gt;

&lt;p&gt;Scale이 제일 coarse한 단계의 $G_N$은 white gaussian noise $z_N$만을 입력으로 받아 이미지를 생성한다.&lt;/p&gt;

&lt;p&gt;\[
\tilde{x}_N=G_N(z_N)
\]&lt;/p&gt;

&lt;p&gt;맨 처음 단계의 patch size는 보통 원본 이미지 높이의 절반 정도가 된다. 따라서 $\tilde{x}_N$은 이미지의 대략적인 배치와 구조를 나타내게 된다. 이후 단계가 올라갈수록 이전 단계에서 표현되지 못한 디테일들을 가지는 이미지가 생성된다. 이것을 위해 $G_n$의 입력으로 $z_n$과 함께 이전 단계에서 생성된 이미지를 upsampling한 이미지가 주어진다.&lt;/p&gt;

&lt;p&gt;\[
\tilde{x}_n=G_n(z_n, (\tilde{x}_{n+1})\uparrow^r), n&amp;lt;N
\]&lt;/p&gt;

&lt;p&gt;각 단계에서 $G_n$의 내부적 구조는 5개의 Conv-block으로 이뤄져 있다. 아래의 그림과 같다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig5.png?raw=true&quot; style=&quot;zoom: 40%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;\[
\tilde{x}_n=(\tilde{x}_{n+1})\uparrow^r+\psi_n(z_n, (\tilde{x}_{n+1})\uparrow^r)
\]&lt;/p&gt;

&lt;p&gt;$z_n$을 $(\tilde{x}_{n+1})\uparrow^r$ 에 더하는 데, 이것을 Convolutional layer 이전에 하여 $G$가 noise를 누락하지 못하게 한다. 종종 randomness를 조건으로 다루는 GAN 연구에서 발생하는 문제점을 이렇게 해결하였다. 또한 데이터를 5개의 Conv-block을 통과시킨 후 $(\tilde{x}_{n+1})↑^r$을 한 번 더 더해주는 residual learning 방식을 사용한다. 각 Conv layer는 Conv(3X3), BatchNorm, LeakyReLU로 구성되어 있다. 오로지 Conv layer만 사용한 점은 test 시 noise의 차원을 변경하여 다양한 크기의 이미지를 생성할 수 있다는 이점을 주기도 한다. 한편 $D_n$의 구조는 $G_n$의 5-Conv net과 동일하다.&lt;/p&gt;

&lt;p&gt;각 단계의 Loss function은 Adversarial Loss와 Reconstruction Loss으로 이뤄져 있다.&lt;/p&gt;

&lt;p&gt;\[
min_{G_n}max_{D_n}L_{adv}(G_n, D_n)+{\alpha}L_{rec}(G_n)
\]&lt;/p&gt;

&lt;p&gt;Adversarial loss는 xn과 $\tilde{x}_n$의 분포 차이를 작게 하기 위해 사용된다. Reconstruction loss는 이미지를 생성할 때 필요한 원본 이미지의 중요한 특징 정보들을 보존하게끔 하기 위해 사용된다.&lt;/p&gt;

&lt;p&gt;Adversarial loss로는 WGAN-GP Loss[6]를 사용했다. WGAN-GP는 Wasserstein GAN(WGAN)의 weight clipping의 문제점을 해결하기 위한 방법으로 gradient penalty를 도입한 모델이다. GAN은 원본 이미지와 $G$에 의해 생성된 이미지 분포의 차이로써 Jensen-Shannon divergence를 사용한다. $G$는 이것을 최소화 하도록 학습된다. 그런데 이 과정이 반복되다보면 $D$가 포화되면서 gradient vanishing 문제가 발생한다. WGAN은 이 문제를 해결하기 위해 분포 간 차이의 척도로서 Wasserstein-1 distance를 사용한 것이다. 또한 $D$-해당 논문에서는 ‘the critic’-는 1-Lipschitz Function으로, 미분 계수가 거의 모든 곳에서 1을 넘지 않는다. 이 Lipschitz constraint를 강화하기 위해 $D$의 weight를 콤팩트 공간에 가둬두는 weight clipping이 더불어 제시되었다. 이것으로 gradient vanishing 문제가 발생하는 것을 방지한다.&lt;/p&gt;

&lt;p&gt;WGAN-GP는 weight clipping이 모델 최적화를 어렵게 한다는 것을 보이면서, 이 문제를 해결하기 위해 만들어졌다. Gradient penalty는 weight clipping 대신 $D$의 Lipschitz constraint를 강화하기 위한 장치로서 제시되었다. 1-Lipschitz function인 D의 미분 계수를 입력값에 따라 직접적으로 제한하는 방식이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/loss.png?raw=true&quot; style=&quot;zoom: 43%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SinGAN에서는 학습 안정성이 높은 WGAN-GP를 사용하였다. 또한 loss function을 patch 몇 개가 아닌 이미지 전체에 걸쳐 적용하였는데, 이것으로 모델이 boundary condition을 학습하게끔 했다.&lt;/p&gt;

&lt;p&gt;Reconstruction loss는 원본 이미지를 생성하는 noise map의 존재를 가정하기 위함이다. $\tilde{x}_n^{rec}$은 $n$번째 단계에서 noise map ${z_N^{rec}, z_{N-1}^{rec}, …, z_0^{rec}}={z^*, 0, …, 0}$을 가지고 생성된 이미지이다. $z^*$은 학습 내내 고정되는 noise map이다.&lt;/p&gt;

\[\begin{aligned}
L_{rec} &amp;amp;= \lVert G_n(0, (\tilde{x}\_{n+1}^{rec}\uparrow^r))-x_n \rVert^2, \ n&amp;lt;N \\
L_{rec} &amp;amp;= \lVert G_N(z^*)-x_N \rVert^2, \ n&amp;lt;N
\end{aligned}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;result&quot;&gt;Result&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig6.png?raw=true&quot; style=&quot;zoom: 50%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SinGAN은 Single image를 학습하여 이미지를 자연스럽고도 다양하게 생성하였다. 원본 이미지의 전반적인 배치와 패턴 구조를 보존하여 현실감 있는 결과물을 만들어 냈다. 그림자, 물에 반사되는 모습 등이 자연스럽게 표현되었다. 그러면서도 patch의 새로운 조합을 생성하여 원본 이미지와 완전히 다른 이미지를 만들거나, 학습한 이미지보다 더 높은 화질을 갖는 이미지를 생성하기도 했다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/fig8.png?raw=true&quot; style=&quot;zoom: 45%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Scale이 pyramid 형태인 모델 구조는 좋은 결과를 보였다. 학습 시 설정하는 scale 수에 따라 모델이 이미지를 생성하는 데에 이미지의 특성을 반영하는 정도가 결정되는 것을 볼 수 있었다. Scale 수가 작으면 coarse한 단계의 patch가 작아지므로 집약적인 디테일을 학습한다. Scale 수가 증가할 수록 patch가 커져 이미지의 전반적인 배치나 특성을 보존하도록 학습한다. 이것을 시험하기 위해 scale을 지정하여 원본 이미지를 얼마나 변화시켜 생성할 것인지 결정하였다. n=N일 때 얼룩말의 모습은 n=N-1, n=N-2일 때보다 부자연스럽다.&lt;/p&gt;

&lt;p&gt;모델이 생성한 이미지가 얼마나 자연스러운지를 평가하기 위해 두 가지 metric이 사용되었다. 첫 번째 방법은 user study이다. 고용된 사람들에게 두 가지 질문을 했다. 하나는 원본 이미지와 생성된 이미지를 한 번에 1초 간 보여준 뒤 어떤 것이 가짜인지 묻는 paired case이다. 다른 하나는 하나의 이미지를 1초 간 보여준 뒤 그것이 가짜인지 아닌지를 묻는 unpaired case이다. 50개의 가짜 이미지를 랜덤하게 제공했다. 한편 coarsest scale을 N과 N-1로 달리하여 이미지를 준비하기도 했다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table1.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;결과적으로 비교 기준이 없는 unpaired case에서, coarsest scale에 관해서는 전반적 구조가 더 보존되는 N-1일 때 사람들이 더 헛갈려 했다. 그러나 N일 때에도 40% 이상의 혼동율로, 분간할 수 없는 수준을 가리키는 50%에 근접하였다. SinGAN이 생성한 이미지가 사람이 보기에 꽤 자연스럽다는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;두 번째 방법은 Single Image Frechet Inception Distance(SIFID) metric이다. Frechet Inception Distance(FID)는 원본 이미지와 생성된 이미지 feature 분포의 편차를 측정하는 것이다. SIFID는 원본 이미지의 통계량을 얼마나 보존하였는지를 측정하기 위해 이 논문에서 제안한 것이다. SIFID는 FID가 이미지 당 하나의 vector를 사용하는 것과 달리, 이미지 내 위치 당 하나의 vector를 사용하여 원본 이미지와 그것으로부터 생성된 이미지 feature 간의 통계량 차이를 비교하는 기준이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table2.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Coarsest scale에 관해서는 user study의 경우와 같이 N-1일 때가 결과가 더 좋았다. 한편 user study와 달리 paired case의 결과가 더 좋았는데, SIFID는 원본과 생성 이미지를 비교하여 계산되기 때문이다.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/alatteaday/alatteaday.github.io/blob/gh-pages/post_images/2020-12-11-singan/table3.png?raw=true&quot; style=&quot;zoom: 30%;&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;SinGAN은 이미지 생성 뿐만아니라 다양한 image manipulation task에 사용될 수 있다. 모델 구조를 바꾸거나 fine-tuning을 적용하지 않고도 super resolution, editing, paint-to-image 등의 여러 task를 수행한다. 단 입력 이미지와 같은 특징 분포를 갖는 이미지만을 생성할 수 있다. n번째 scale 단계에서 down sampled된 이미지를 넣어 해당 이미지의 patch들의 분포에 맞게 학습해 나가는 방식이다. 성능 또한 각 task를 목적으로 만들어진 모델들에 비해 성능 또한 떨어지지 않는다. 예를 들어 super resolution의 경우 SRGAN, EDSR, DIP, ZSSR과 SinGAN을 비교했는데, single data 기반 SOTA 모델인 DIP, ZSSR 및 dataset 기반 모델인 EDSR보다 성능이 좋았고, SRGAN과는 거의 근접한 성능을 보였다.&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;SinGAN은 Single image로 GAN을 학습하여 자연스러운 이미지를 생성하고, 다양한 이미지의 변용이 가능한 장점이 있다. 우선 SinGAN은 입력 데이터 하나만으로도 학습이 가능하다. 기존의 모델들은 특정 클래스에 속하는 이미지를 생성하는 모델을 학습시키기 위해 해당 클래스의 많은 이미지 데이터가 필요했다. SinGAN은 이 점에서 많은 데이터를 구할 필요가 없어 학습이 용이하다. 그럼에도 SinGAN은 사람이 보기에도 자연스러운 이미지를 생성해낼 수 있다. 기존의 Single image 기반 GAN 모델은 texture 이미지 생성에만 국한되었다. 나아가 이미지의 전반적인 패턴과 특징을 유지하면서도 물체의 배치 등을 변화시켜 입력 이미지와 다른 다양한 이미지를 생성할 수 있다.&lt;/p&gt;

&lt;p&gt;많은 image manipulation task를 다루는 모델들은 특정한 task를 수행하기 위한 목적으로 만들어졌다. 그러나 SinGAN은 모델 구조 수정이나 tuning 등의 추가 작업을 하지 않으면서도 다양한 task를 해결할 수 있다. Scale을 변화시켜 학습하고, 테스트 시 적절한 단계에 이미지를 입력함으로써 super resolution, editing, paint-to-image, single image animation 등을 수행한다.&lt;/p&gt;

&lt;p&gt;다만 user study에서 볼 수 있듯 결과물이 실제 이미지와 비교하였을 때 가짜인지 구분이 되는 정도이다. 더 자연스러운 이미지를 생성해내는 성능을 위해 발전할 여지가 있다.&lt;/p&gt;
</content>
 </entry>
 

</feed>
